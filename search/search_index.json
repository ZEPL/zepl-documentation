{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Load your data Load data from S3 using Apache Spark Interpreters Apache Spark - scala, pyspark, sparksql Python - Python2, Python3 JDBC - MySql, Postgresql, Oracle Config - inline configuration Accessing Data Data Sources Data Source Integration Loading Additional Libraries Load dependency libraries from a maven repository into Apache Spark Install Python packages using Pip Load Python libraries via the conda interpreter Secure JDBC Connections Whitelist IP addresses Working with External Clusters Amazon EMR Importing Existing Notebooks Import Jupyter or Zeppelin notebooks External notebooks: Apache Zeppelin Github Amazon S3 Import a notebook file Exporting Notebooks from Zepl Export into Apache Zeppelin notebook format Sharing Notebooks with your Team Sharing notebooks Publishing your Notebooks Publishing a notebook Exploring public notebooks Managing Notebook Versions How to manage notebook versions","title":"Welcome"},{"location":"#load-your-data","text":"Load data from S3 using Apache Spark","title":"Load your data"},{"location":"#interpreters","text":"Apache Spark - scala, pyspark, sparksql Python - Python2, Python3 JDBC - MySql, Postgresql, Oracle Config - inline configuration","title":"Interpreters"},{"location":"#accessing-data","text":"Data Sources Data Source Integration","title":"Accessing Data"},{"location":"#loading-additional-libraries","text":"Load dependency libraries from a maven repository into Apache Spark Install Python packages using Pip Load Python libraries via the conda interpreter","title":"Loading Additional Libraries"},{"location":"#secure-jdbc-connections","text":"Whitelist IP addresses","title":"Secure JDBC Connections"},{"location":"#working-with-external-clusters","text":"Amazon EMR","title":"Working with External Clusters"},{"location":"#importing-existing-notebooks","text":"Import Jupyter or Zeppelin notebooks External notebooks: Apache Zeppelin Github Amazon S3 Import a notebook file","title":"Importing Existing Notebooks"},{"location":"#exporting-notebooks-from-zepl","text":"Export into Apache Zeppelin notebook format","title":"Exporting Notebooks from Zepl"},{"location":"#sharing-notebooks-with-your-team","text":"Sharing notebooks","title":"Sharing Notebooks with your Team"},{"location":"#publishing-your-notebooks","text":"Publishing a notebook Exploring public notebooks","title":"Publishing your Notebooks"},{"location":"#managing-notebook-versions","text":"How to manage notebook versions","title":"Managing Notebook Versions"},{"location":"faq/","text":"FAQ If you don't see your question in this FAQ please send us an email at support@zepl.com . Is Zepl the enterprise version of Apache Zeppelin? No. Though the people behind Zepl are also the creators of Apache Zeppelin, Zepl is a separate data science analytics platform expanding upon what we started doing with Apache Zeppelin. Zeppelin was created because we wanted something that could quickly and easily plug into various back-ends doing everything from data sourcing to visualization. But it was created for the individual in a localized environment. But analytics is not a one person job. It's iterative and usually goes through multiple rounds of back and forth between different people. Zepl is built this in mind. In addition to the broad spectrum data science facilities and gorgeous charting you have in Zeppelin, Zepl provides full enterprise collaboration features such as sharing, fine-grained, role-based access control, team management, comments, notifications, and much more. Zepl is built for more than just analytics. It's a collaborative analytics platform for the modern age. Does Zepl work with Jupyter notebooks? Yes. You can import or sync Jupyter notebooks stored in S3 or Github into Zepl. Once in Zepl, in addition to being able to view the notebook from any browser, you can clone the notebook, make edits, and execute the notebook directly within Zepl. Note: Currently only Jupyter v4.x is supported Can I use Zepl and Apache Zeppelin together? Yes. You can connect multiple Zeppelin instances to Zepl. Simply click on the Create Space button from the main page, toggle on External Repository and select Zeppelin from the dropdown menu that appears. All the notebooks in your Zeppelin instance will be sync-ed and available to view in the new Zepl Space . At this point you can clone, edit, execute and share the notebooks. Can I use Zepl and Jupyter together? Yes. You can continue to use Jupyter in your local environment and sync and/or import the notebooks as desired. You can also export notebooks from Zepl to be worked on from you local Jupyter or Zeppelin instance. How long are notebooks saved in Zepl? The notebooks are saved until you delete them or delete your account. Can I save my notebooks in a different repository/storage of my choice? All notebooks created in Zepl are stored in Zepl. You can export the notebooks to your preferred storage if you like. Can I connect Zepl to more than one Apache Zeppelin instance? Yes. You can connect to as many Apache Zeppelin instances as you like. You simply need to make sure that each Zeppelin instance is setup with its own unique token which is provided when you create a Zeppelin instance Space in Zepl. How can I use Zepl with Apache Zeppelin running on EMR? Zepl enables close integration with Apache Zeppelin so you can setup Zeppelin in EMR and use Zepl not only for sync-ing and backing up your notebooks, but you can leverage Zepl as the authentication portal for Zeppelin. Details on how you can set this up can be found here . Can Zepl run privately in an Amazon VPC? Yes. Enterprise deployments currently only run in Amazon VPC's though other cloud solutions will be supported in the near future. Can you create multiple organizations in Zepl? Yes. The administrator has access to create one or more organizations in their Zepl account. Can I share my notebooks with people who are not users of Zepl? Yes. You can directly share notebooks with non-registered users by simply entering their email address in the notebook sharing window. Another option is to publish your notebook. Publishing a notebook in Zepl creates a public facing URL for the notebook which you can send to anyone you like. What level of security does Zepl offer both for notebooks and users? We take security very seriously at Zepl. Besides all transmissions being over SSL, all notebooks are encrypted with the encryption keys in a separate infrastructure. I have Jupyter and Zeppelin notebooks stored in Github. Can I import them to Zepl? Yes. Zepl supports both Zeppelin and Jupyter (v4.x) notebooks. Simply click on the Create Space button from the main page, toggle on External Repository and select GitHub from the dropdown menu that appears. Then enter your GitHub credentials. All the notebooks in your GitHub repo will be sync-ed and available to view in the new Zepl Space . At this point you can clone, edit, execute and share the notebooks. If I set my notebook repository to my private S3 or Github account when creating a new Space via an external repository, will my notebooks be saved in Zepl as well? Yes. When you sync your notebooks from S3 or Github, copies of your notebooks are also saved in Zepl. In addition, if you make changes to your notebooks and save them in your private S3/Github repo, you can update these notebooks manually by clicking the Synchronize repository link in the top middle of the page. Otherwise Zepl sync's them behind the scenes every 10 minutes. Can I read/import data from S3 to analyze in Zepl? Yes. Through Python and Scala you can programmatically access data from S3 or other publicly available data sources. What are the different roles available in Zepl? What rights does each have? There are two levels of roles available in Zepl: Within Spaces you can give members of that Space different roles: i. Member - only has access to notebooks within the Space ii. Collaborator - the above plus can add/delete notebooks within the Space iii. Manager - the above plus can add/remove members in the Space When inviting team members to Organizations you can give each team member one of the following roles: i. Member - only a participant ii. Manager - has the ability to add/delete other members and Spaces within the Organization even if not the owner iii. Admin - all of the manager rights as well as the ability to add/delete both notebooks and interpreter clusters even if not the owner What does it mean for a notebook to be public (e.g. publicly accessible for people having the link)? Public notebooks are notebooks that are published through Zepl and become accessible publicly through the generated URL. You can make the notebook private again by toggling the Published control off in the Publish dialog. What are the notebooks in the Explore showcases and how does one manage them? Notebooks in the Explore section are notebooks that were aggregated by us and some of our members. They include notebooks publicly published by various users including data scientists, professors, engineers, etc. and cover a wide range of topics from How-Tos to research papers. Please take the time to Explore . Which versions of Python, Scala and Spark does Zepl support? Currently Zepl supports: Python v2.7 & v3.5 Spark v2.1.0 Scala v2.11 R v3.3.2 How can I load external Python libraries? You can load external Python libraries by using either of the followings commands in a notebook paragraph: %python.conda install <libname> %python !pip install <libname>","title":"FAQ"},{"location":"faq/#faq","text":"If you don't see your question in this FAQ please send us an email at support@zepl.com .","title":"FAQ"},{"location":"faq/#is-zepl-the-enterprise-version-of-apache-zeppelin","text":"No. Though the people behind Zepl are also the creators of Apache Zeppelin, Zepl is a separate data science analytics platform expanding upon what we started doing with Apache Zeppelin. Zeppelin was created because we wanted something that could quickly and easily plug into various back-ends doing everything from data sourcing to visualization. But it was created for the individual in a localized environment. But analytics is not a one person job. It's iterative and usually goes through multiple rounds of back and forth between different people. Zepl is built this in mind. In addition to the broad spectrum data science facilities and gorgeous charting you have in Zeppelin, Zepl provides full enterprise collaboration features such as sharing, fine-grained, role-based access control, team management, comments, notifications, and much more. Zepl is built for more than just analytics. It's a collaborative analytics platform for the modern age.","title":"Is Zepl the enterprise version of Apache Zeppelin?"},{"location":"faq/#does-zepl-work-with-jupyter-notebooks","text":"Yes. You can import or sync Jupyter notebooks stored in S3 or Github into Zepl. Once in Zepl, in addition to being able to view the notebook from any browser, you can clone the notebook, make edits, and execute the notebook directly within Zepl. Note: Currently only Jupyter v4.x is supported","title":"Does Zepl work with Jupyter notebooks?"},{"location":"faq/#can-i-use-zepl-and-apache-zeppelin-together","text":"Yes. You can connect multiple Zeppelin instances to Zepl. Simply click on the Create Space button from the main page, toggle on External Repository and select Zeppelin from the dropdown menu that appears. All the notebooks in your Zeppelin instance will be sync-ed and available to view in the new Zepl Space . At this point you can clone, edit, execute and share the notebooks.","title":"Can I use Zepl and Apache Zeppelin together?"},{"location":"faq/#can-i-use-zepl-and-jupyter-together","text":"Yes. You can continue to use Jupyter in your local environment and sync and/or import the notebooks as desired. You can also export notebooks from Zepl to be worked on from you local Jupyter or Zeppelin instance.","title":"Can I use Zepl and Jupyter together?"},{"location":"faq/#how-long-are-notebooks-saved-in-zepl","text":"The notebooks are saved until you delete them or delete your account.","title":"How long are notebooks saved in Zepl?"},{"location":"faq/#can-i-save-my-notebooks-in-a-different-repositorystorage-of-my-choice","text":"All notebooks created in Zepl are stored in Zepl. You can export the notebooks to your preferred storage if you like.","title":"Can I save my notebooks in a different repository/storage of my choice?"},{"location":"faq/#can-i-connect-zepl-to-more-than-one-apache-zeppelin-instance","text":"Yes. You can connect to as many Apache Zeppelin instances as you like. You simply need to make sure that each Zeppelin instance is setup with its own unique token which is provided when you create a Zeppelin instance Space in Zepl.","title":"Can I connect Zepl to more than one Apache Zeppelin instance?"},{"location":"faq/#how-can-i-use-zepl-with-apache-zeppelin-running-on-emr","text":"Zepl enables close integration with Apache Zeppelin so you can setup Zeppelin in EMR and use Zepl not only for sync-ing and backing up your notebooks, but you can leverage Zepl as the authentication portal for Zeppelin. Details on how you can set this up can be found here .","title":"How can I use Zepl with Apache Zeppelin running on EMR?"},{"location":"faq/#can-zepl-run-privately-in-an-amazon-vpc","text":"Yes. Enterprise deployments currently only run in Amazon VPC's though other cloud solutions will be supported in the near future.","title":"Can Zepl run privately in an Amazon VPC?"},{"location":"faq/#can-you-create-multiple-organizations-in-zepl","text":"Yes. The administrator has access to create one or more organizations in their Zepl account.","title":"Can you create multiple organizations in Zepl?"},{"location":"faq/#can-i-share-my-notebooks-with-people-who-are-not-users-of-zepl","text":"Yes. You can directly share notebooks with non-registered users by simply entering their email address in the notebook sharing window. Another option is to publish your notebook. Publishing a notebook in Zepl creates a public facing URL for the notebook which you can send to anyone you like.","title":"Can I share my notebooks with people who are not users of Zepl?"},{"location":"faq/#what-level-of-security-does-zepl-offer-both-for-notebooks-and-users","text":"We take security very seriously at Zepl. Besides all transmissions being over SSL, all notebooks are encrypted with the encryption keys in a separate infrastructure.","title":"What level of security does Zepl offer both for notebooks and users?"},{"location":"faq/#i-have-jupyter-and-zeppelin-notebooks-stored-in-github-can-i-import-them-to-zepl","text":"Yes. Zepl supports both Zeppelin and Jupyter (v4.x) notebooks. Simply click on the Create Space button from the main page, toggle on External Repository and select GitHub from the dropdown menu that appears. Then enter your GitHub credentials. All the notebooks in your GitHub repo will be sync-ed and available to view in the new Zepl Space . At this point you can clone, edit, execute and share the notebooks.","title":"I have Jupyter and Zeppelin notebooks stored in Github. Can I import them to Zepl?"},{"location":"faq/#if-i-set-my-notebook-repository-to-my-private-s3-or-github-account-when-creating-a-new-space-via-an-external-repository-will-my-notebooks-be-saved-in-zepl-as-well","text":"Yes. When you sync your notebooks from S3 or Github, copies of your notebooks are also saved in Zepl. In addition, if you make changes to your notebooks and save them in your private S3/Github repo, you can update these notebooks manually by clicking the Synchronize repository link in the top middle of the page. Otherwise Zepl sync's them behind the scenes every 10 minutes.","title":"If I set my notebook repository to my private S3 or Github account when creating a new Space via an external repository, will my notebooks be saved in Zepl as well?"},{"location":"faq/#can-i-readimport-data-from-s3-to-analyze-in-zepl","text":"Yes. Through Python and Scala you can programmatically access data from S3 or other publicly available data sources.","title":"Can I read/import data from S3 to analyze in Zepl?"},{"location":"faq/#what-are-the-different-roles-available-in-zepl-what-rights-does-each-have","text":"There are two levels of roles available in Zepl: Within Spaces you can give members of that Space different roles: i. Member - only has access to notebooks within the Space ii. Collaborator - the above plus can add/delete notebooks within the Space iii. Manager - the above plus can add/remove members in the Space When inviting team members to Organizations you can give each team member one of the following roles: i. Member - only a participant ii. Manager - has the ability to add/delete other members and Spaces within the Organization even if not the owner iii. Admin - all of the manager rights as well as the ability to add/delete both notebooks and interpreter clusters even if not the owner","title":"What are the different roles available in Zepl? What rights does each have?"},{"location":"faq/#what-does-it-mean-for-a-notebook-to-be-public-eg-publicly-accessible-for-people-having-the-link","text":"Public notebooks are notebooks that are published through Zepl and become accessible publicly through the generated URL. You can make the notebook private again by toggling the Published control off in the Publish dialog.","title":"What does it mean for a notebook to be public (e.g. publicly accessible for people having the link)?"},{"location":"faq/#what-are-the-notebooks-in-the-explore-showcases-and-how-does-one-manage-them","text":"Notebooks in the Explore section are notebooks that were aggregated by us and some of our members. They include notebooks publicly published by various users including data scientists, professors, engineers, etc. and cover a wide range of topics from How-Tos to research papers. Please take the time to Explore .","title":"What are the notebooks in the Explore showcases and how does one manage them?"},{"location":"faq/#which-versions-of-python-scala-and-spark-does-zepl-support","text":"Currently Zepl supports: Python v2.7 & v3.5 Spark v2.1.0 Scala v2.11 R v3.3.2","title":"Which versions of Python, Scala and Spark does Zepl support?"},{"location":"faq/#how-can-i-load-external-python-libraries","text":"You can load external Python libraries by using either of the followings commands in a notebook paragraph: %python.conda install <libname> %python !pip install <libname>","title":"How can I load external Python libraries?"},{"location":"support/","text":"Support TECHNICAL If you have any technical questions or experience any unexpected problems please contact us at support@zepl.com PRICING Please visit the Zepl Pricing Guide or contact us directly at info@zepl.com LEGAL Please visit our Terms of Service page.","title":"Support"},{"location":"support/#support","text":"TECHNICAL If you have any technical questions or experience any unexpected problems please contact us at support@zepl.com PRICING Please visit the Zepl Pricing Guide or contact us directly at info@zepl.com LEGAL Please visit our Terms of Service page.","title":"Support"},{"location":"guide/accessing_data/","text":"Uploading and Downloading Data This section describes how to upload data from your own filesystem into Zepl and analyze it using Spark, Python, Scala, R and other interpreters within Zepl. You can also download and delete the uploaded data. Uploading Data If you have data files on your local machine that you want to analyze with Zepl you can upload the file by clicking the right menu bar in your notebook and choosing the Upload file button. You can also simply drag and drop the relevant file into the sidebar. You can upload multiple files which are only accessible through the given notebook. To access the same file in different notebooks the file will need to be uploaded to each notebook separately. Note: Currently we allow up to 100MB in total. Accessing Data Once the file is uploaded to the notebook, you can access the file by the following URL (where <file-name> is the name of the file): http://zdata/<file-name> Here are some examples you can use: Scala %spark import org.apache.spark.SparkFiles sc.addFile(\"http://zdata/bank.csv\") val sparkDF = spark.read.format(\"csv\") .option(\"delimiter\", \";\") .option(\"header\", \"true\") .option(\"inferSchema\", \"true\") .load(SparkFiles.get(\"bank.csv\")) PySpark %spark.pyspark from pyspark import SparkFiles sc.addFile('http://zdata/bank.csv') sparkDF = spark.read.format('csv').options(delimiter=';', header='true', inferSchema='true').load(SparkFiles.get('bank.csv')) SparkR %spark.r spark.addFile(\"http://zdata/bank.csv\") sparkDF <- read.df(path = spark.getSparkFiles(\"bank.csv\"), source = \"csv\", delimiter = \";\", header = \"true\", inferSchema = \"true\") If you want to read your data into Python directly, you can also read your data using pandas . For example: Python %python import pandas as pd pandas_df = pd.read_csv('http://zdata/bank.csv', sep=';', header='infer') Downloading Data If the data volume is small enough, you can also load this data directly into the container. You can use %python !wget http://zdata/<file-name> to download data to the container. Once the file is downloaded to the container, you can access the downloaded file as a local file. For example, you can use the following code to access the data after downloading a file named bank.csv to the container: %spark val sparkDF = spark.read.format(\"csv\") .option(\"delimiter\", \";\") .option(\"header\", \"true\") .option(\"inferSchema\", \"true\") .load(\"bank.csv\") Editing Data You cannot edit data directly within Zepl but you can overwrite the data file by uploading a file with the same name. Warning: Overwritten data cannot be recovered. Deleting Data To delete data, click the red \"x\" button next to the data file in the Files tab on the notebook page. Warning: Deleted data cannot be recovered.","title":"Uploading Data"},{"location":"guide/accessing_data/#uploading-and-downloading-data","text":"This section describes how to upload data from your own filesystem into Zepl and analyze it using Spark, Python, Scala, R and other interpreters within Zepl. You can also download and delete the uploaded data.","title":"Uploading and Downloading Data"},{"location":"guide/accessing_data/#uploading-data","text":"If you have data files on your local machine that you want to analyze with Zepl you can upload the file by clicking the right menu bar in your notebook and choosing the Upload file button. You can also simply drag and drop the relevant file into the sidebar. You can upload multiple files which are only accessible through the given notebook. To access the same file in different notebooks the file will need to be uploaded to each notebook separately. Note: Currently we allow up to 100MB in total.","title":"Uploading Data"},{"location":"guide/accessing_data/#accessing-data","text":"Once the file is uploaded to the notebook, you can access the file by the following URL (where <file-name> is the name of the file): http://zdata/<file-name> Here are some examples you can use: Scala %spark import org.apache.spark.SparkFiles sc.addFile(\"http://zdata/bank.csv\") val sparkDF = spark.read.format(\"csv\") .option(\"delimiter\", \";\") .option(\"header\", \"true\") .option(\"inferSchema\", \"true\") .load(SparkFiles.get(\"bank.csv\")) PySpark %spark.pyspark from pyspark import SparkFiles sc.addFile('http://zdata/bank.csv') sparkDF = spark.read.format('csv').options(delimiter=';', header='true', inferSchema='true').load(SparkFiles.get('bank.csv')) SparkR %spark.r spark.addFile(\"http://zdata/bank.csv\") sparkDF <- read.df(path = spark.getSparkFiles(\"bank.csv\"), source = \"csv\", delimiter = \";\", header = \"true\", inferSchema = \"true\") If you want to read your data into Python directly, you can also read your data using pandas . For example: Python %python import pandas as pd pandas_df = pd.read_csv('http://zdata/bank.csv', sep=';', header='infer')","title":"Accessing Data"},{"location":"guide/accessing_data/#downloading-data","text":"If the data volume is small enough, you can also load this data directly into the container. You can use %python !wget http://zdata/<file-name> to download data to the container. Once the file is downloaded to the container, you can access the downloaded file as a local file. For example, you can use the following code to access the data after downloading a file named bank.csv to the container: %spark val sparkDF = spark.read.format(\"csv\") .option(\"delimiter\", \";\") .option(\"header\", \"true\") .option(\"inferSchema\", \"true\") .load(\"bank.csv\")","title":"Downloading Data"},{"location":"guide/accessing_data/#editing-data","text":"You cannot edit data directly within Zepl but you can overwrite the data file by uploading a file with the same name. Warning: Overwritten data cannot be recovered.","title":"Editing Data"},{"location":"guide/accessing_data/#deleting-data","text":"To delete data, click the red \"x\" button next to the data file in the Files tab on the notebook page. Warning: Deleted data cannot be recovered.","title":"Deleting Data"},{"location":"guide/autocompletion_notebooks/","text":"Autocompletion in Zepl Activating and Using Autocompletion autocomplete shortcut: ctrl + . or tab Zepl supports spark, SQL, and Python autocompletion. You'll first need to run a paragraph in the notebook you're working on to activate the feature. Then just use the ctrl + . or tab hotkey to see possible completions pop up as shown below: If you forget to run a paragraph in the notebook to activate the feature you'll get a warning like the following: Notice that the status bar at the top right of the page says Stopped . Running a paragraph or the whole notebook will start the container allowing for the use of autocompletion and the status will say Started . Supported Interpreter Types Here is the list of interpreters and their specific autocompletion supported features. Spark Interpreter Symbol Autocompletion Supported Type %spark supported SparkInterpreter: provides a Scala environment %pyspark supported PySparkInterpreter: provides a PySpark environment %ipyspark supported IPySparkInterpreter: provides an iPython Spark environment %dep supported DepInterpreter: loads dependency libraries into the Spark environment %sql X SparkSqlInterpreter - provides a SparkSQL environment %r X SparkRInterpreter - provides an R environment Python Interpreter Symbol Autocompletion Supported Type %python supported PythonInterpreter: provides a Python environment %ipython supported IPythonInterpreter: provides an iPython environment %conda X PythonCondaInterpreter: provides a Conda environment %sql X PythonInterpreterPandasSql: provides a pandasql environment JDBC Interpreter Symbol Autocompletion Supported Type %sql supported JDBC interpreter with drivers for popular databases","title":"Autocompletion"},{"location":"guide/autocompletion_notebooks/#autocompletion-in-zepl","text":"","title":"Autocompletion in Zepl"},{"location":"guide/autocompletion_notebooks/#activating-and-using-autocompletion","text":"autocomplete shortcut: ctrl + . or tab Zepl supports spark, SQL, and Python autocompletion. You'll first need to run a paragraph in the notebook you're working on to activate the feature. Then just use the ctrl + . or tab hotkey to see possible completions pop up as shown below: If you forget to run a paragraph in the notebook to activate the feature you'll get a warning like the following: Notice that the status bar at the top right of the page says Stopped . Running a paragraph or the whole notebook will start the container allowing for the use of autocompletion and the status will say Started .","title":"Activating and Using Autocompletion"},{"location":"guide/autocompletion_notebooks/#supported-interpreter-types","text":"Here is the list of interpreters and their specific autocompletion supported features.","title":"Supported Interpreter Types"},{"location":"guide/autocompletion_notebooks/#spark-interpreter","text":"Symbol Autocompletion Supported Type %spark supported SparkInterpreter: provides a Scala environment %pyspark supported PySparkInterpreter: provides a PySpark environment %ipyspark supported IPySparkInterpreter: provides an iPython Spark environment %dep supported DepInterpreter: loads dependency libraries into the Spark environment %sql X SparkSqlInterpreter - provides a SparkSQL environment %r X SparkRInterpreter - provides an R environment","title":"Spark Interpreter"},{"location":"guide/autocompletion_notebooks/#python-interpreter","text":"Symbol Autocompletion Supported Type %python supported PythonInterpreter: provides a Python environment %ipython supported IPythonInterpreter: provides an iPython environment %conda X PythonCondaInterpreter: provides a Conda environment %sql X PythonInterpreterPandasSql: provides a pandasql environment","title":"Python Interpreter"},{"location":"guide/autocompletion_notebooks/#jdbc-interpreter","text":"Symbol Autocompletion Supported Type %sql supported JDBC interpreter with drivers for popular databases","title":"JDBC Interpreter"},{"location":"guide/exploring_notebooks/","text":"Exploring Public Notebooks Zepl Explore provides a communal space for anyone to share their Zepl notebooks with the world. Show off what you've done with Zepl and see what others have accomplished. You can both add any published notebook using it's publish URL or search for notebooks with keywords in the text field near the top of the page. Contribute and come back often to see what's new! Note: Anyone can view the notebooks, even non-Zepl users. Showcases? Zepl has a number of showcases available to categorize submitted notebooks.","title":"Explore Public Notebooks"},{"location":"guide/exploring_notebooks/#exploring-public-notebooks","text":"Zepl Explore provides a communal space for anyone to share their Zepl notebooks with the world. Show off what you've done with Zepl and see what others have accomplished. You can both add any published notebook using it's publish URL or search for notebooks with keywords in the text field near the top of the page. Contribute and come back often to see what's new! Note: Anyone can view the notebooks, even non-Zepl users.","title":"Exploring Public Notebooks"},{"location":"guide/exploring_notebooks/#showcases","text":"Zepl has a number of showcases available to categorize submitted notebooks.","title":"Showcases?"},{"location":"guide/export_notebook/","text":"Exporting Zepl Notebooks Zepl supports exporting its native notebooks to the Apache Zeppelin format for use in your Zeppelin instances or for storage in an external repository. Export to the Apache Zeppelin Notebook Format To export your notebook use the Export to Zeppelin menu item in the \"...\" menu at the top right of the notebook page as shown below: Then select the version you'd like to export to and the file will be automatically downloaded to your local filesystem. Note: We currently support export to versions v0.7.x and v0.8.x in JSON format and v0.9.x in the .zpln file format. Importing into an Apache Zeppelin Instance Use the Import note menu on the home page of your Apache Zeppelin instance as shown below: Apache Zeppelin File Size Limitations If you encounter issues due to your downloaded notebook's file size being too large for your Apache Zeppelin instance (the default is 1MB) you can increase the limit by changing the zeppelin.websocket.max.text.message.size property in ZEPPELIN_HOME/conf/zeppelin-site.xml . The example below sets the maximum limit to 50MB: <property> <name>zeppelin.websocket.max.text.message.size</name> <value>52428800</value> <description>Size in characters of the maximum text message to be received by websocket. Defaults to 1024000</description> </property> Alternatively you can set the ZEPPELIN_WEBSOCKET_MAX_TEXT_MESSAGE_SIZE environment variable in ZEPPELIN_HOME/conf/zeppelin-env.sh . For more information on configuration please refer to the official Apache Zeppelin documentation .","title":"Export Notebook"},{"location":"guide/export_notebook/#exporting-zepl-notebooks","text":"Zepl supports exporting its native notebooks to the Apache Zeppelin format for use in your Zeppelin instances or for storage in an external repository.","title":"Exporting Zepl Notebooks"},{"location":"guide/export_notebook/#export-to-the-apache-zeppelin-notebook-format","text":"To export your notebook use the Export to Zeppelin menu item in the \"...\" menu at the top right of the notebook page as shown below: Then select the version you'd like to export to and the file will be automatically downloaded to your local filesystem. Note: We currently support export to versions v0.7.x and v0.8.x in JSON format and v0.9.x in the .zpln file format.","title":"Export to the Apache Zeppelin Notebook Format"},{"location":"guide/export_notebook/#importing-into-an-apache-zeppelin-instance","text":"Use the Import note menu on the home page of your Apache Zeppelin instance as shown below:","title":"Importing into an Apache Zeppelin Instance"},{"location":"guide/export_notebook/#apache-zeppelin-file-size-limitations","text":"If you encounter issues due to your downloaded notebook's file size being too large for your Apache Zeppelin instance (the default is 1MB) you can increase the limit by changing the zeppelin.websocket.max.text.message.size property in ZEPPELIN_HOME/conf/zeppelin-site.xml . The example below sets the maximum limit to 50MB: <property> <name>zeppelin.websocket.max.text.message.size</name> <value>52428800</value> <description>Size in characters of the maximum text message to be received by websocket. Defaults to 1024000</description> </property> Alternatively you can set the ZEPPELIN_WEBSOCKET_MAX_TEXT_MESSAGE_SIZE environment variable in ZEPPELIN_HOME/conf/zeppelin-env.sh . For more information on configuration please refer to the official Apache Zeppelin documentation .","title":"Apache Zeppelin File Size Limitations"},{"location":"guide/feature_versioning/","text":"Notebook Versioning Notebook versions are created automatically when paragraphs are executed, code is modified or results change. Versions can also be named for easier identification and to act as a tagging system. Naming a version can also be used to create saved versions manually. And, of course, it's possible to rollback the notebook to any of the previous versions. Creating a Version of a Notebook Manually To create a manual version of a notebook, click the Name current version item in the \"...\" dropdown menu on the top right side of the notebook page. Then enter the name you want in the Name this version dialog window that pops up and click Submit . Examining the Version History You can have a look at the list of saved versions by clicking the Version history item in the \"...\" dropdown menu on the top right side of the notebook page as shown below: Renaming Existing Versions Existing versions can be renamed by clicking the kebab menu at the upper right of each version entry area and selecting Rename as shown here: Automatically Created Versions When a paragraph is executed or code is modified in a paragraph, Zepl creates notebook versions automatically (at short time intervals). Of course, we can rename the automatically created versions too. Rollbacking Back To preview the content of a previous version, click its name in the version history list. A preview will be shown for verification with a Close Preview button appearing in the top right. Click that button if you'd like to cancel the operation and keep the current contents of the notebook. Otherwise click the Rollback button to proceed. The notebook will now roll back and refresh to the version you selected as shown below:","title":"Notebook Versioning"},{"location":"guide/feature_versioning/#notebook-versioning","text":"Notebook versions are created automatically when paragraphs are executed, code is modified or results change. Versions can also be named for easier identification and to act as a tagging system. Naming a version can also be used to create saved versions manually. And, of course, it's possible to rollback the notebook to any of the previous versions.","title":"Notebook Versioning"},{"location":"guide/feature_versioning/#creating-a-version-of-a-notebook-manually","text":"To create a manual version of a notebook, click the Name current version item in the \"...\" dropdown menu on the top right side of the notebook page. Then enter the name you want in the Name this version dialog window that pops up and click Submit .","title":"Creating a Version of a Notebook Manually"},{"location":"guide/feature_versioning/#examining-the-version-history","text":"You can have a look at the list of saved versions by clicking the Version history item in the \"...\" dropdown menu on the top right side of the notebook page as shown below:","title":"Examining the Version History"},{"location":"guide/feature_versioning/#renaming-existing-versions","text":"Existing versions can be renamed by clicking the kebab menu at the upper right of each version entry area and selecting Rename as shown here:","title":"Renaming Existing Versions"},{"location":"guide/feature_versioning/#automatically-created-versions","text":"When a paragraph is executed or code is modified in a paragraph, Zepl creates notebook versions automatically (at short time intervals). Of course, we can rename the automatically created versions too.","title":"Automatically Created Versions"},{"location":"guide/feature_versioning/#rollbacking-back","text":"To preview the content of a previous version, click its name in the version history list. A preview will be shown for verification with a Close Preview button appearing in the top right. Click that button if you'd like to cancel the operation and keep the current contents of the notebook. Otherwise click the Rollback button to proceed. The notebook will now roll back and refresh to the version you selected as shown below:","title":"Rollbacking Back"},{"location":"guide/github_integration/","text":"GitHub Integration GitHub is a popular online repository data scientists use to store their notebooks whether they be Zeppelin or Jupyter. Zepl supports integration of both notebook types from GitHub via Spaces as described below. Note that the files in GitHub must be in JSON (Apache Zeppelin) or ipynb (Jupyter) format. Note: GitHub synchronization is unidirectional from GitHub to Zepl so edits in Zepl do not get updated in GitHub . Also the notebooks from GitHub are read-only in Zepl and need to be cloned to modify. Creating a GitHub Repository Space Click the New Space button in the main page to create a new Space and fill in the name and description fields. Then check External Repository and select GitHub from the dropdown menu. Connecting to a GitHub Repository In order for Zepl to connect to your GitHub repository you will need a GitHub repo access scope token as pictured below. Once GitHub is selected in the dropdown menu the following fields will need to be filled in: GitHub Auth Token : copy the GitHub access token that you generated in the previous section GitHub Repository URL : the URL of your GitHub repository Branch to Use : the branch name of the repo (if no branch name is set, Zepl will use the master branch by default) Base Path (optional): Zepl will recursively search this directory path for notebook files (if this field is not set Zepl will search from the repository's root path) Below is an example of what the dialog window might look like: You can now click Apply to create your GitHub Space . Zepl will first test the connection to the GitHub repository and if successful will redirect you to the newly created Space with all notebooks imported from the repository. The GitHub space will automatically synchronize with the GitHub repository every 12 hours and will add new notebooks, update any modified notebooks, and remove any deleted notebooks. You can also manually re-trigger the synchronization. You can now enjoy the power of Zepl with your notebooks stored in GitHub . If you need help with GitHub tokens you can reference the GitHub settings and token generation GitHub documentation .","title":"Github"},{"location":"guide/github_integration/#github-integration","text":"GitHub is a popular online repository data scientists use to store their notebooks whether they be Zeppelin or Jupyter. Zepl supports integration of both notebook types from GitHub via Spaces as described below. Note that the files in GitHub must be in JSON (Apache Zeppelin) or ipynb (Jupyter) format. Note: GitHub synchronization is unidirectional from GitHub to Zepl so edits in Zepl do not get updated in GitHub . Also the notebooks from GitHub are read-only in Zepl and need to be cloned to modify.","title":"GitHub Integration"},{"location":"guide/github_integration/#creating-a-github-repository-space","text":"Click the New Space button in the main page to create a new Space and fill in the name and description fields. Then check External Repository and select GitHub from the dropdown menu.","title":"Creating a GitHub Repository Space"},{"location":"guide/github_integration/#connecting-to-a-github-repository","text":"In order for Zepl to connect to your GitHub repository you will need a GitHub repo access scope token as pictured below. Once GitHub is selected in the dropdown menu the following fields will need to be filled in: GitHub Auth Token : copy the GitHub access token that you generated in the previous section GitHub Repository URL : the URL of your GitHub repository Branch to Use : the branch name of the repo (if no branch name is set, Zepl will use the master branch by default) Base Path (optional): Zepl will recursively search this directory path for notebook files (if this field is not set Zepl will search from the repository's root path) Below is an example of what the dialog window might look like: You can now click Apply to create your GitHub Space . Zepl will first test the connection to the GitHub repository and if successful will redirect you to the newly created Space with all notebooks imported from the repository. The GitHub space will automatically synchronize with the GitHub repository every 12 hours and will add new notebooks, update any modified notebooks, and remove any deleted notebooks. You can also manually re-trigger the synchronization. You can now enjoy the power of Zepl with your notebooks stored in GitHub . If you need help with GitHub tokens you can reference the GitHub settings and token generation GitHub documentation .","title":"Connecting to a GitHub Repository"},{"location":"guide/import_notebook/","text":"Importing Notebooks into Zepl Zepl allows you to leverage the functionality of an enterprise platform for versioning, collaborating with and sharing both your Zeppelin and Jupyter notebooks by importing them into your Spaces . This can be done in 2 ways: uploading notebooks directly to Zepl fetching notebooks from a publicly hosted location To get started click the Import Notebook option from either the \"kebab\" menu in the Spaces main page or from the menu bar inside of a Space . Note: Currently only Jupyter 4.x or greater and Zeppelin up to v0.8.x are supported. Uploading Notebooks Click Select File to upload a new file from your local filesystem. Note: The file size limit is 10 MB. The notebook name will be auto-filled but you can rename it as you like. Importing from a URL Click Fetch from URL to upload a new file from a publicly accessible host and enter the URL in the Link to your notebook field. Once the upload is completed you will be redirected to the notebook. Juno Look and Feel For those who are familiar with Jupyter notebooks and prefer a similar notebook format, Zepl includes the Juno view. If you import a Jupyter notebook the Juno view will be set automatically but you can change it once you open the notebook as demonstrated in the image below.","title":"Import Notebook"},{"location":"guide/import_notebook/#importing-notebooks-into-zepl","text":"Zepl allows you to leverage the functionality of an enterprise platform for versioning, collaborating with and sharing both your Zeppelin and Jupyter notebooks by importing them into your Spaces . This can be done in 2 ways: uploading notebooks directly to Zepl fetching notebooks from a publicly hosted location To get started click the Import Notebook option from either the \"kebab\" menu in the Spaces main page or from the menu bar inside of a Space . Note: Currently only Jupyter 4.x or greater and Zeppelin up to v0.8.x are supported.","title":"Importing Notebooks into Zepl"},{"location":"guide/import_notebook/#uploading-notebooks","text":"Click Select File to upload a new file from your local filesystem. Note: The file size limit is 10 MB. The notebook name will be auto-filled but you can rename it as you like.","title":"Uploading Notebooks"},{"location":"guide/import_notebook/#importing-from-a-url","text":"Click Fetch from URL to upload a new file from a publicly accessible host and enter the URL in the Link to your notebook field. Once the upload is completed you will be redirected to the notebook.","title":"Importing from a URL"},{"location":"guide/import_notebook/#juno-look-and-feel","text":"For those who are familiar with Jupyter notebooks and prefer a similar notebook format, Zepl includes the Juno view. If you import a Jupyter notebook the Juno view will be set automatically but you can change it once you open the notebook as demonstrated in the image below.","title":"Juno Look and Feel"},{"location":"guide/resource_mgmt/","text":"Resources in Zepl A Resource must be attached to a Zepl Notebook before it can be executed. Resources in Zepl determine which interpreters will be available and which type of physical resource (e.g CPU, RAM, etc.) will be allocated to execute the notebook. Zepl Resources include some of the most popular data science packages and libraries: Spark 2.2, 2.3 R packages ggplot2 knitr bayesian_first_aid Python 2.7, 3.6 pre installed python libraries You can run pip list on the python interpreter to see the full list of installed python packages. Zepl also supports pip and conda if additional libraries are required. Attaching a Resource to a Notebook You can select a resource when you create or clone a notebook. You can also switch a notebook's attached resource after the notebook has been created through the notebook's Settings menu in the top right of the window. Changing the attached resource will cause any running containers to be shutdown. Resource Management Note: only an organization's admin can create and manage a resource Click Resources from the top right area of the main page. Creating & Updating Resources Zepl provides a Simple Workload as the default resource. Additional resources can be created to customize the capabilities and capacity of attached notebooks. To create a new resource click the New Resource button. The following form should appear. Give the new resource a name and description. Configuration Resource type : select the container size for the running notebook (each resource type consumes hour units at different rates) Idle shutdown time : the resource for a notebook will shutdown automatically if the container is idle for the set amount of time Max concurrent running notebooks : the maximum number of notebooks which can be attached to this resource at the same time Processing time limit : a limit to the total time that this resource can run across all notebooks - it's useful in preventing over-consumption of resources and capping the associated fees ( this is currently ignored ) Set as default resource : sets resource to be selected by default when you create (clone) a notebook Image type : defines which interpreters and tools are provided in the container in which the notebook will run You can edit the resource's settings by clicking on its name or on the Change Setting item in the \"...\" menu to the right of the resource entry in the list on the Resource settings page. Note: You cannot change the container type or the image type after the resource is created. Create a new resource to do so. Deleting Resources After a resource is deleted, all containers for that resource will be shutdown and any notebooks that have the deleted resource attached will be set to the default resource. If you're trying to delete the default resource, the next available resource in the resource list will be set as the next default resource. Resource Permissions Users share resources within an organization. You can make use of resource permissions to control the usage of these resources by members of the organization. After the introduction of resource permissions users can attach resources to notebooks and run them. Note: View only and Edit permissions set while sharing notebooks can only be used to restrict editing of notebooks and not restrict their execution. There are three different permissions presets which can be applied to users or user groups such as team groups (members/managers) or Space user groups (members/collaborators/managers) Allow Attach, Detach Resource on a notebook Allow Start, Stop, Execute Resource Allow Modify Resource Setting Please note that these permissions do not depend on each other or have precedence over each other. You will have to be explicit on permissions. A user who doesn't have attach permission can still have edit permission. Also giving permission to a specific user group doesn't give permission to a higher user group. For example, giving edit permission to users of a Space doesn't give edit permission to managers of that space. Creating a Resource Permission Initially only resource admins can configure permissions for resources after which users given edit permission would be able to do this. To add permissions go to a resource's edit page and click the Add permissions button. Fill in the name of a user, Space or Team . Check the desired permissions and click Submit . Editing or Deleting Resource Permissions You can use the \"...\" menu on the right of the permissions entry in the list to edit or delete specific resource permissions. Creating Generic Permissions To give permission to all users of a certain type, such as all users or all space collaborators or all team managers, type all in the search box. Be aware that permissions operate as follows: specific user rules will overwrite all-user rules specific team rules will overwrite all-team rules specific space rules will overwrite all-space rules","title":"Resources"},{"location":"guide/resource_mgmt/#resources-in-zepl","text":"A Resource must be attached to a Zepl Notebook before it can be executed. Resources in Zepl determine which interpreters will be available and which type of physical resource (e.g CPU, RAM, etc.) will be allocated to execute the notebook. Zepl Resources include some of the most popular data science packages and libraries: Spark 2.2, 2.3 R packages ggplot2 knitr bayesian_first_aid Python 2.7, 3.6 pre installed python libraries You can run pip list on the python interpreter to see the full list of installed python packages. Zepl also supports pip and conda if additional libraries are required.","title":"Resources in Zepl"},{"location":"guide/resource_mgmt/#attaching-a-resource-to-a-notebook","text":"You can select a resource when you create or clone a notebook. You can also switch a notebook's attached resource after the notebook has been created through the notebook's Settings menu in the top right of the window. Changing the attached resource will cause any running containers to be shutdown.","title":"Attaching a Resource to a Notebook"},{"location":"guide/resource_mgmt/#resource-management","text":"Note: only an organization's admin can create and manage a resource Click Resources from the top right area of the main page.","title":"Resource Management"},{"location":"guide/resource_mgmt/#creating-updating-resources","text":"Zepl provides a Simple Workload as the default resource. Additional resources can be created to customize the capabilities and capacity of attached notebooks. To create a new resource click the New Resource button. The following form should appear. Give the new resource a name and description.","title":"Creating &amp; Updating Resources"},{"location":"guide/resource_mgmt/#configuration","text":"Resource type : select the container size for the running notebook (each resource type consumes hour units at different rates) Idle shutdown time : the resource for a notebook will shutdown automatically if the container is idle for the set amount of time Max concurrent running notebooks : the maximum number of notebooks which can be attached to this resource at the same time Processing time limit : a limit to the total time that this resource can run across all notebooks - it's useful in preventing over-consumption of resources and capping the associated fees ( this is currently ignored ) Set as default resource : sets resource to be selected by default when you create (clone) a notebook Image type : defines which interpreters and tools are provided in the container in which the notebook will run You can edit the resource's settings by clicking on its name or on the Change Setting item in the \"...\" menu to the right of the resource entry in the list on the Resource settings page. Note: You cannot change the container type or the image type after the resource is created. Create a new resource to do so.","title":"Configuration"},{"location":"guide/resource_mgmt/#deleting-resources","text":"After a resource is deleted, all containers for that resource will be shutdown and any notebooks that have the deleted resource attached will be set to the default resource. If you're trying to delete the default resource, the next available resource in the resource list will be set as the next default resource.","title":"Deleting Resources"},{"location":"guide/resource_mgmt/#resource-permissions","text":"Users share resources within an organization. You can make use of resource permissions to control the usage of these resources by members of the organization. After the introduction of resource permissions users can attach resources to notebooks and run them. Note: View only and Edit permissions set while sharing notebooks can only be used to restrict editing of notebooks and not restrict their execution. There are three different permissions presets which can be applied to users or user groups such as team groups (members/managers) or Space user groups (members/collaborators/managers) Allow Attach, Detach Resource on a notebook Allow Start, Stop, Execute Resource Allow Modify Resource Setting Please note that these permissions do not depend on each other or have precedence over each other. You will have to be explicit on permissions. A user who doesn't have attach permission can still have edit permission. Also giving permission to a specific user group doesn't give permission to a higher user group. For example, giving edit permission to users of a Space doesn't give edit permission to managers of that space.","title":"Resource Permissions"},{"location":"guide/resource_mgmt/#creating-a-resource-permission","text":"Initially only resource admins can configure permissions for resources after which users given edit permission would be able to do this. To add permissions go to a resource's edit page and click the Add permissions button. Fill in the name of a user, Space or Team . Check the desired permissions and click Submit .","title":"Creating a Resource Permission"},{"location":"guide/resource_mgmt/#editing-or-deleting-resource-permissions","text":"You can use the \"...\" menu on the right of the permissions entry in the list to edit or delete specific resource permissions.","title":"Editing or Deleting Resource Permissions"},{"location":"guide/resource_mgmt/#creating-generic-permissions","text":"To give permission to all users of a certain type, such as all users or all space collaborators or all team managers, type all in the search box. Be aware that permissions operate as follows: specific user rules will overwrite all-user rules specific team rules will overwrite all-team rules specific space rules will overwrite all-space rules","title":"Creating Generic Permissions"},{"location":"guide/s3_integration/","text":"S3 Integration S3 is a popular online data storage service offered by AWS and used by many data scientists to store their notebooks whether they be Zeppelin or Jupyter. Zepl supports integration of both notebook types via Spaces as described below. Note that the files in S3 must be in JSON (Apache Zeppelin) or ipynb (Jupyter) format. Note: S3 synchronization is unidirectional from S3 to Zepl so edits in Zepl do not get updated in S3 . Also the notebooks from S3 are read-only in Zepl and need to be cloned to modify. In this section, we will explain how you can create an Amazon Web Service S3 Space in Zepl and seamlessly synchronize your notebooks from your own S3 bucket . Creating an AWS S3 Repository Space Click the New Space button in the main page to create a new Space and fill in the name and description fields. Then check External Repository and select S3 from the dropdown menu. In order for Zepl to connect to your S3 repository, you will need to enter your Access key Private key , Bucket and Region . If you are unaware of how to find and retrieve this information, you can refer to the AWS documentation . A completed form might look like the following image. If the synchronization is successful, the notebooks in your S3 bucket should be automatically added to your Zepl S3 Space. The S3 space will automatically synchronize with the S3 repository every 12 hours and will add new notebooks, update any modified notebooks, and remove any deleted notebooks. You can also manually re-trigger the synchronization.","title":"Amazon S3"},{"location":"guide/s3_integration/#s3-integration","text":"S3 is a popular online data storage service offered by AWS and used by many data scientists to store their notebooks whether they be Zeppelin or Jupyter. Zepl supports integration of both notebook types via Spaces as described below. Note that the files in S3 must be in JSON (Apache Zeppelin) or ipynb (Jupyter) format. Note: S3 synchronization is unidirectional from S3 to Zepl so edits in Zepl do not get updated in S3 . Also the notebooks from S3 are read-only in Zepl and need to be cloned to modify. In this section, we will explain how you can create an Amazon Web Service S3 Space in Zepl and seamlessly synchronize your notebooks from your own S3 bucket .","title":"S3 Integration"},{"location":"guide/s3_integration/#creating-an-aws-s3-repository-space","text":"Click the New Space button in the main page to create a new Space and fill in the name and description fields. Then check External Repository and select S3 from the dropdown menu. In order for Zepl to connect to your S3 repository, you will need to enter your Access key Private key , Bucket and Region . If you are unaware of how to find and retrieve this information, you can refer to the AWS documentation . A completed form might look like the following image. If the synchronization is successful, the notebooks in your S3 bucket should be automatically added to your Zepl S3 Space. The S3 space will automatically synchronize with the S3 repository every 12 hours and will add new notebooks, update any modified notebooks, and remove any deleted notebooks. You can also manually re-trigger the synchronization.","title":"Creating an AWS S3 Repository Space"},{"location":"guide/sharing_notebooks/","text":"Spaces and Notebooks Sharing In this section we will explain how you can create a new Space and share notebooks with other members and teams. What is a Space ? A Zepl Space is simply a collection of notebooks. Both members and teams in an Organization can be invited to a Space and can be granted permission to share and collaborate on the notebooks within. Creating a new Space Click the New Space button on the main page to create a new Space . Give your Space a name and a short description. Zepl offers the ability to import notebooks from several external sources when creating a Space if desired. To do so check the External Repositories checkbox which will display the current options available: Apache Zeppelin notebooks, notebooks stored in Github and in AWS S3 . Note that the synchronized notebooks are read-only and in order to run/execute them you'll need to clone them first. Sharing Spaces After creating a new Space you can share it with other members or teams or with the entire Organization . Members of a Space are able to access all notebooks that belong to the Space . Sharing Data Notebooks Zepl provides fine-grained access control for sharing notebooks with others as shown in the following image. Permissions There are three preset permission policies to choose from: * Full Read Access: allows members to view and comment on the notebook * Full Collaborate Access: Full Read Access plus the ability to edit, schedule, version and run the notebook * Full Manage Access: Full Collaboration Access plus the ability to publish, share, move and delete the notebook You can choose from one of these or grant your own custom permissions by making the appropriate selections in the dialog window. Note that in addition to the Run notebook permission option, a member must have the Allow Start, Stop, Execute Resource permission granted in the Resource permissions area for the notebook's assigned resource in order to run the notebook. A quick link to the Resource permissions settings page appears next to the Run option for convenience. Version This is currently hard-coded to the latest version of the notebook. Format Zepl supports several different visual styles: default: grey background notebook theme simple: white background notebook theme with smaller paragraph padding report: minimizes the number of icons and shows them only on mouse-over Juno: Jupyter -like styling Publishing Notebooks to the Web You can both share and publish notebooks from the Spaces main page using the \"...\" menu to the right of the notebook name. Publishing a notebook provides a URL for use elsewhere and will automatically render the notebook public. Anyone who has access to this URL will be able to view the notebook in read-only mode. To make the notebook private again simply re-open the Publish dialog and toggle the control back to the UNPUBLISHED state. Unpublished notebooks are only accessible to Zepl members who have been granted access rights via the sharing mechansim described above.","title":"Sharing Notebooks"},{"location":"guide/sharing_notebooks/#spaces-and-notebooks-sharing","text":"In this section we will explain how you can create a new Space and share notebooks with other members and teams.","title":"Spaces and Notebooks Sharing"},{"location":"guide/sharing_notebooks/#what-is-a-space","text":"A Zepl Space is simply a collection of notebooks. Both members and teams in an Organization can be invited to a Space and can be granted permission to share and collaborate on the notebooks within.","title":"What is a Space?"},{"location":"guide/sharing_notebooks/#creating-a-new-space","text":"Click the New Space button on the main page to create a new Space . Give your Space a name and a short description. Zepl offers the ability to import notebooks from several external sources when creating a Space if desired. To do so check the External Repositories checkbox which will display the current options available: Apache Zeppelin notebooks, notebooks stored in Github and in AWS S3 . Note that the synchronized notebooks are read-only and in order to run/execute them you'll need to clone them first.","title":"Creating a new Space"},{"location":"guide/sharing_notebooks/#sharing-spaces","text":"After creating a new Space you can share it with other members or teams or with the entire Organization . Members of a Space are able to access all notebooks that belong to the Space .","title":"Sharing Spaces"},{"location":"guide/sharing_notebooks/#sharing-data-notebooks","text":"Zepl provides fine-grained access control for sharing notebooks with others as shown in the following image. Permissions There are three preset permission policies to choose from: * Full Read Access: allows members to view and comment on the notebook * Full Collaborate Access: Full Read Access plus the ability to edit, schedule, version and run the notebook * Full Manage Access: Full Collaboration Access plus the ability to publish, share, move and delete the notebook You can choose from one of these or grant your own custom permissions by making the appropriate selections in the dialog window. Note that in addition to the Run notebook permission option, a member must have the Allow Start, Stop, Execute Resource permission granted in the Resource permissions area for the notebook's assigned resource in order to run the notebook. A quick link to the Resource permissions settings page appears next to the Run option for convenience. Version This is currently hard-coded to the latest version of the notebook. Format Zepl supports several different visual styles: default: grey background notebook theme simple: white background notebook theme with smaller paragraph padding report: minimizes the number of icons and shows them only on mouse-over Juno: Jupyter -like styling","title":"Sharing Data Notebooks"},{"location":"guide/sharing_notebooks/#publishing-notebooks-to-the-web","text":"You can both share and publish notebooks from the Spaces main page using the \"...\" menu to the right of the notebook name. Publishing a notebook provides a URL for use elsewhere and will automatically render the notebook public. Anyone who has access to this URL will be able to view the notebook in read-only mode. To make the notebook private again simply re-open the Publish dialog and toggle the control back to the UNPUBLISHED state. Unpublished notebooks are only accessible to Zepl members who have been granted access rights via the sharing mechansim described above.","title":"Publishing Notebooks to the Web"},{"location":"guide/zeppelin_integration/","text":"Apache Zeppelin Notebook Integration Apache Zeppelin is a popular open-source data science notebook platform which can leverage Spark for big data analysis and produce beautiful charts and graphs for displaying insights. Zepl, being of the same lineage for SaaS and Enterprise users, naturally integrates with Zeppelin notebooks. This is done via Zepl Spaces . Create a Zeppelin Instance Space Click the New Space button in the main page to create a new Space and fill in the name and description fields. Then check External Repository and select Zeppelin from the dropdown menu. Once the Space is created, Zepl will produce a unique token which you can use to connect Zepl to your Apache Zeppelin instance. You can edit the new Space 's settings by using the Settings dropdown menu item in the \"...\" menu to the right of the Space 's block. Connecting Zepl with Apache Zeppelin Setting the Apache Zeppelin environment variables Preparation for Synchronization In order to synchronize your Zeppelin instance to your Zepl account you'll need to set three environment variables in your ZEPPELIN_HOME/conf/zeppelin-env.sh file for Linux/macOS or in your ZEPPELIN_HOME/conf/zeppelin-env.cmd file for Windows: ZEPPELIN_NOTEBOOK_STORAGE, ZEPPELINHUB_API_ADDRESS and ZEPPELINHUB_API_TOKEN. If you don't already have this file you can create it by copying the ZEPPELIN_HOME/conf/zeppelin-env.sh.template or ZEPPELIN_HOME/conf/zeppelin-env.cmd.template file as follows (replace sh with cmd for Windows): $ cd ZEPPELIN_HOME/conf $ cp zeppelin-env.sh.template zeppelin-env.sh Now copy your generated token from your new Zepl Space . Then edit the zeppelin-env.sh or zeppelin-env.cmd file to add the environment variables as follows according to the Zeppelin version in use: Zeppelin-0.6.X for Linux / macOS export ZEPPELIN_NOTEBOOK_STORAGE=\"org.apache.zeppelin.notebook.repo.VFSNotebookRepo, org.apache.zeppelin.notebook.repo.zeppelinhub.ZeppelinHubRepo\" export ZEPPELINHUB_API_ADDRESS=\"ADDRESS_OF_ZEPPELINHUB_SERVICE\" (e.g. https://www.zepl.com) export ZEPPELINHUB_API_TOKEN=\"YOUR_TOKEN_STRING\" Zeppelin-0.6.X for Windows set ZEPPELIN_NOTEBOOK_STORAGE=org.apache.zeppelin.notebook.repo.VFSNotebookRepo, org.apache.zeppelin.notebook.repo.zeppelinhub.ZeppelinHubRepo set ZEPPELINHUB_API_ADDRESS=https://www.zepl.com set ZEPPELINHUB_API_TOKEN=YOUR_TOKEN_STRING Zeppelin-0.7.x single user for Linux / macOS export ZEPPELIN_NOTEBOOK_STORAGE=\"org.apache.zeppelin.notebook.repo.GitNotebookRepo, org.apache.zeppelin.notebook.repo.zeppelinhub.ZeppelinHubRepo\" export ZEPPELINHUB_API_ADDRESS=\"ADDRESS_OF_ZEPPELINHUB_SERVICE\" (e.g. https://www.zepl.com) export ZEPPELINHUB_API_TOKEN=\"YOUR_TOKEN_STRING\" Zeppelin-0.7.x single user for Windows set ZEPPELIN_NOTEBOOK_STORAGE=org.apache.zeppelin.notebook.repo.GitNotebookRepo, org.apache.zeppelin.notebook.repo.zeppelinhub.ZeppelinHubRepo set ZEPPELINHUB_API_ADDRESS=https://www.zepl.com set ZEPPELINHUB_API_TOKEN=YOUR_TOKEN_STRING Zeppelin-0.7.x multiple users on Zeppelin server for Linux / macOS export ZEPPELIN_NOTEBOOK_STORAGE=\"org.apache.zeppelin.notebook.repo.GitNotebookRepo, org.apache.zeppelin.notebook.repo.zeppelinhub.ZeppelinHubRepo\" export ZEPPELINHUB_API_ADDRESS=\"ADDRESS_OF_ZEPPELINHUB_SERVICE\" (e.g. https://www.zepl.com) In this multi-user scenario you don't need your token as it will be automatically recognized from your new Zeppelin Space . In order to login with your Zepl credentials, you will need to complete the steps in the next section . You may also find up-to-date instructions on integrating the latest Apache Zeppelin master branch with Zepl on the Apache Zeppelin documentation website . Login to Apache Zeppelin 0.7.x with Zepl credentials Starting with the 0.7.x release, Apache Zeppelin has added more multi-user support functionality so we made it possible to login to Zeppelin with your Zepl credentials in case multiple users are using the same Zeppelin server. For the best integration experience we recommend that you use the latest release of Zeppelin (currently 0.8.1 ). After you set the environment variables mentioned in the previous section , you'll need to modify your conf/shiro.ini security configuration file. If you don't have this file, you can copy it from ZEPPELIN_HOME/conf/shiro.ini.template as follows: $ cd ZEPPELIN_HOME/conf $ cp shiro.ini.template shiro.ini Then modify the section under ### A sample for configuring ZeppelinHub Realm as shown below: #### A sample for configuring ZeppelinHub Realm zeppelinHubRealm = org.apache.zeppelin.realm.ZeppelinHubRealm ### Url of ZeppelinHub zeppelinHubRealm.zeppelinhubUrl = service_url securityManager.realms = $zeppelinHubRealm Note that instead of service_url you should specify the URL of your Zepl service (e.g. https://www.zepl.com). You can now login to Zeppelin using your Zepl account credentials. Start the Apache Zeppelin daemon Finally, start (or restart) Apache Zeppelin. $ cd ZEPPELIN_HOME $ ./bin/zeppelin-daemon.sh start (or restart) Then return to Zepl and the indicator shown below should indicate that Zeppelin is properly connected.","title":"Apache Zeppelin"},{"location":"guide/zeppelin_integration/#apache-zeppelin-notebook-integration","text":"Apache Zeppelin is a popular open-source data science notebook platform which can leverage Spark for big data analysis and produce beautiful charts and graphs for displaying insights. Zepl, being of the same lineage for SaaS and Enterprise users, naturally integrates with Zeppelin notebooks. This is done via Zepl Spaces .","title":"Apache Zeppelin Notebook Integration"},{"location":"guide/zeppelin_integration/#create-a-zeppelin-instance-space","text":"Click the New Space button in the main page to create a new Space and fill in the name and description fields. Then check External Repository and select Zeppelin from the dropdown menu. Once the Space is created, Zepl will produce a unique token which you can use to connect Zepl to your Apache Zeppelin instance. You can edit the new Space 's settings by using the Settings dropdown menu item in the \"...\" menu to the right of the Space 's block.","title":"Create a Zeppelin Instance Space"},{"location":"guide/zeppelin_integration/#connecting-zepl-with-apache-zeppelin","text":"","title":"Connecting Zepl with Apache Zeppelin"},{"location":"guide/zeppelin_integration/#setting-the-apache-zeppelin-environment-variables","text":"","title":"Setting the Apache Zeppelin environment variables"},{"location":"guide/zeppelin_integration/#preparation-for-synchronization","text":"In order to synchronize your Zeppelin instance to your Zepl account you'll need to set three environment variables in your ZEPPELIN_HOME/conf/zeppelin-env.sh file for Linux/macOS or in your ZEPPELIN_HOME/conf/zeppelin-env.cmd file for Windows: ZEPPELIN_NOTEBOOK_STORAGE, ZEPPELINHUB_API_ADDRESS and ZEPPELINHUB_API_TOKEN. If you don't already have this file you can create it by copying the ZEPPELIN_HOME/conf/zeppelin-env.sh.template or ZEPPELIN_HOME/conf/zeppelin-env.cmd.template file as follows (replace sh with cmd for Windows): $ cd ZEPPELIN_HOME/conf $ cp zeppelin-env.sh.template zeppelin-env.sh Now copy your generated token from your new Zepl Space . Then edit the zeppelin-env.sh or zeppelin-env.cmd file to add the environment variables as follows according to the Zeppelin version in use:","title":"Preparation for Synchronization"},{"location":"guide/zeppelin_integration/#zeppelin-06x-for-linux-macos","text":"export ZEPPELIN_NOTEBOOK_STORAGE=\"org.apache.zeppelin.notebook.repo.VFSNotebookRepo, org.apache.zeppelin.notebook.repo.zeppelinhub.ZeppelinHubRepo\" export ZEPPELINHUB_API_ADDRESS=\"ADDRESS_OF_ZEPPELINHUB_SERVICE\" (e.g. https://www.zepl.com) export ZEPPELINHUB_API_TOKEN=\"YOUR_TOKEN_STRING\"","title":"Zeppelin-0.6.X for Linux / macOS"},{"location":"guide/zeppelin_integration/#zeppelin-06x-for-windows","text":"set ZEPPELIN_NOTEBOOK_STORAGE=org.apache.zeppelin.notebook.repo.VFSNotebookRepo, org.apache.zeppelin.notebook.repo.zeppelinhub.ZeppelinHubRepo set ZEPPELINHUB_API_ADDRESS=https://www.zepl.com set ZEPPELINHUB_API_TOKEN=YOUR_TOKEN_STRING","title":"Zeppelin-0.6.X for Windows"},{"location":"guide/zeppelin_integration/#zeppelin-07x-single-user-for-linux-macos","text":"export ZEPPELIN_NOTEBOOK_STORAGE=\"org.apache.zeppelin.notebook.repo.GitNotebookRepo, org.apache.zeppelin.notebook.repo.zeppelinhub.ZeppelinHubRepo\" export ZEPPELINHUB_API_ADDRESS=\"ADDRESS_OF_ZEPPELINHUB_SERVICE\" (e.g. https://www.zepl.com) export ZEPPELINHUB_API_TOKEN=\"YOUR_TOKEN_STRING\"","title":"Zeppelin-0.7.x single user for Linux / macOS"},{"location":"guide/zeppelin_integration/#zeppelin-07x-single-user-for-windows","text":"set ZEPPELIN_NOTEBOOK_STORAGE=org.apache.zeppelin.notebook.repo.GitNotebookRepo, org.apache.zeppelin.notebook.repo.zeppelinhub.ZeppelinHubRepo set ZEPPELINHUB_API_ADDRESS=https://www.zepl.com set ZEPPELINHUB_API_TOKEN=YOUR_TOKEN_STRING","title":"Zeppelin-0.7.x single user for Windows"},{"location":"guide/zeppelin_integration/#zeppelin-07x-multiple-users-on-zeppelin-server-for-linux-macos","text":"export ZEPPELIN_NOTEBOOK_STORAGE=\"org.apache.zeppelin.notebook.repo.GitNotebookRepo, org.apache.zeppelin.notebook.repo.zeppelinhub.ZeppelinHubRepo\" export ZEPPELINHUB_API_ADDRESS=\"ADDRESS_OF_ZEPPELINHUB_SERVICE\" (e.g. https://www.zepl.com) In this multi-user scenario you don't need your token as it will be automatically recognized from your new Zeppelin Space . In order to login with your Zepl credentials, you will need to complete the steps in the next section . You may also find up-to-date instructions on integrating the latest Apache Zeppelin master branch with Zepl on the Apache Zeppelin documentation website .","title":"Zeppelin-0.7.x multiple users on Zeppelin server for Linux / macOS"},{"location":"guide/zeppelin_integration/#login-to-apache-zeppelin-07x-with-zepl-credentials","text":"Starting with the 0.7.x release, Apache Zeppelin has added more multi-user support functionality so we made it possible to login to Zeppelin with your Zepl credentials in case multiple users are using the same Zeppelin server. For the best integration experience we recommend that you use the latest release of Zeppelin (currently 0.8.1 ). After you set the environment variables mentioned in the previous section , you'll need to modify your conf/shiro.ini security configuration file. If you don't have this file, you can copy it from ZEPPELIN_HOME/conf/shiro.ini.template as follows: $ cd ZEPPELIN_HOME/conf $ cp shiro.ini.template shiro.ini Then modify the section under ### A sample for configuring ZeppelinHub Realm as shown below: #### A sample for configuring ZeppelinHub Realm zeppelinHubRealm = org.apache.zeppelin.realm.ZeppelinHubRealm ### Url of ZeppelinHub zeppelinHubRealm.zeppelinhubUrl = service_url securityManager.realms = $zeppelinHubRealm Note that instead of service_url you should specify the URL of your Zepl service (e.g. https://www.zepl.com). You can now login to Zeppelin using your Zepl account credentials.","title":"Login to Apache Zeppelin 0.7.x with Zepl credentials"},{"location":"guide/zeppelin_integration/#start-the-apache-zeppelin-daemon","text":"Finally, start (or restart) Apache Zeppelin. $ cd ZEPPELIN_HOME $ ./bin/zeppelin-daemon.sh start (or restart) Then return to Zepl and the indicator shown below should indicate that Zeppelin is properly connected.","title":"Start the Apache Zeppelin daemon"},{"location":"guide/accesscontrols/organization_permissions/","text":"Organization Permissions Permission Model Permissions in Zepl are organized by centrally managed Security Policies. These policies can be highly customized with granular and specific permissions around actions in the product. After understanding the product level permissions available to you in the policies, you can assign team members to use those policies and gain access to those permissions. Each team member can be assigned multiple Security Policies, and their permissions to perform actions in Zepl are the sum of all the permissions granted to them by the Security Policies. Zepl provides several convenient Security Policies for you to use out of the box, and it's easy for you to build policies customized for the particular needs of your enterprise. View Security Policies Security Policies are located under Settings. Not everyone can view and edit security policies for an organization - you need the appropriate permission to do so. This permission is granted by default to \"Organization Owners\" and \"Security Administrators\". Edit Policy Permissions When looking at the list of security policies, click on any security policy to see what permissions are assigned to that policy. To edit what permissions are associated with a policy, you simply need to click on the check mark next to a policy. To make your edits permanent changes to the policy, simply click Save. Create New Policy Creating a new policy is very similar to viewing and editing an existing one. Click \"New Policy\" above the list of current security policies to start the process. Give the policy a name, select the permissions you want associated with the policy, and click \"Save\" to finish creating the policy. Assigning Policies to Users To assign users to policies, navigate to the Members page under Settings. You can assign policies to an individual team member by clicking on their name to access their profile page. Select any policies you would like to assign to the user on the members profile page and click save. Assigning policies to users requires a permission of \"Assign and unassign security policies\". This permission is granted by default to \"Organization Owners\" and \"Security Administrators\". Whenever you invite a user to join your organization, you can also assign policies to this user at the same time if you have the permission to assign users to policies. Default Policies Zepl comes with several out-of-the-box policies that you can use to grant permissions to organization members. These policies can be changed to customize your needs. Default Permissions The following permissions are available for you to include in a policy:","title":"Organization Permissions"},{"location":"guide/accesscontrols/organization_permissions/#organization-permissions","text":"","title":"Organization Permissions"},{"location":"guide/accesscontrols/organization_permissions/#permission-model","text":"Permissions in Zepl are organized by centrally managed Security Policies. These policies can be highly customized with granular and specific permissions around actions in the product. After understanding the product level permissions available to you in the policies, you can assign team members to use those policies and gain access to those permissions. Each team member can be assigned multiple Security Policies, and their permissions to perform actions in Zepl are the sum of all the permissions granted to them by the Security Policies. Zepl provides several convenient Security Policies for you to use out of the box, and it's easy for you to build policies customized for the particular needs of your enterprise.","title":"Permission Model"},{"location":"guide/accesscontrols/organization_permissions/#view-security-policies","text":"Security Policies are located under Settings. Not everyone can view and edit security policies for an organization - you need the appropriate permission to do so. This permission is granted by default to \"Organization Owners\" and \"Security Administrators\".","title":"View Security Policies"},{"location":"guide/accesscontrols/organization_permissions/#edit-policy-permissions","text":"When looking at the list of security policies, click on any security policy to see what permissions are assigned to that policy. To edit what permissions are associated with a policy, you simply need to click on the check mark next to a policy. To make your edits permanent changes to the policy, simply click Save.","title":"Edit Policy Permissions"},{"location":"guide/accesscontrols/organization_permissions/#create-new-policy","text":"Creating a new policy is very similar to viewing and editing an existing one. Click \"New Policy\" above the list of current security policies to start the process. Give the policy a name, select the permissions you want associated with the policy, and click \"Save\" to finish creating the policy.","title":"Create New Policy"},{"location":"guide/accesscontrols/organization_permissions/#assigning-policies-to-users","text":"To assign users to policies, navigate to the Members page under Settings. You can assign policies to an individual team member by clicking on their name to access their profile page. Select any policies you would like to assign to the user on the members profile page and click save. Assigning policies to users requires a permission of \"Assign and unassign security policies\". This permission is granted by default to \"Organization Owners\" and \"Security Administrators\". Whenever you invite a user to join your organization, you can also assign policies to this user at the same time if you have the permission to assign users to policies.","title":"Assigning Policies to Users"},{"location":"guide/accesscontrols/organization_permissions/#default-policies","text":"Zepl comes with several out-of-the-box policies that you can use to grant permissions to organization members. These policies can be changed to customize your needs.","title":"Default Policies"},{"location":"guide/accesscontrols/organization_permissions/#default-permissions","text":"The following permissions are available for you to include in a policy:","title":"Default Permissions"},{"location":"guide/authentication/authentication_providers/","text":"Configure an authentication provider in Zepl Zepl understands that your data, and the data science that is used to mine insights from it, is extremely valuable and sensitive. To ensure that only trusted parties can access Zepl resources, you can configure Zepl to use the identity provider platform of your choice. Doing so is a two step configuration: extending access from your identity provider to Zepl, and configuring Zepl Single Sign-on to use that authentication provider. Today, we support the following protocols for logging in to Zepl: Username / Password Google CAS OpenID Connect Okta Azure AD Auth0 SAML Okta Azure AD (coming soon) Auth0 Don't see your provider? Don't worry - we are constantly adding more. Contact us and let us know how we can help you. Please note that today, Zepl does not support importing groups & roles from these identity providers. To use SSO with your Zepl account, your SSO email must match the email of your Zepl account. If you use these identity providers to access your data, you will need to go through an additional step in Zepl to connect to your data with your provider - this documentation section strictly covers configuring Zepl to use your identity provider of choice to log in to the product. Username / Password When you first sign up for Zepl, you created a username and password - this is the default authentication method for Zepl organizations. When you have set Username / Password authentication for your organization, team members can reset their passwords with the \"Forgot your password\". Many users find this to be sufficient for their security needs, especially if they are a small team. Google Enable your users to authenticate with their existing Google accounts. There are only two fields required for this configuration in Zepl: Client ID and Client Secret . Google's Developer Portal Setup OAuth Consent: Select OAuth consent screen Select Application Type Internal: Only users with a Google Account in your organization can grant access to the scopes requested by this app External: Your app will be available to any user with Google Account Click \"Create\" Application Name: Zepl Application logo (right click image to save as): Authorized domains: Your Company Domain Example: zepl.com Click \"Save\" Setup OAuth 2.0 Client ID and Secret: Select Credentials > Create Project Select \"+ Create Credentials\" > OAuth client ID Application type: Web application Name: Zepl Authorized JavaScript origins: Nothing required Authorized redirect URIs: https://app.zepl.com/api/v2/authenticator.identity.zepl/callback/googleprovider Click \"Create\" Copy the Client ID and Client Secret to use in the next section In Zepl's Authentication Settings: Select Settings > Authentication > Google Client ID: Paste the value copied from the previous section Example: 1007962201111-p95t6jaaq720idooa8hq6dpm8qqgwe3s.apps.googleusercontent.com Client Secret: Paste the value copied from the previous section Example: GcP61ILeWLk6pveVh8ohQe3t OpenID Connect Most popular SSO providers support authentication with OpenID Connect. Throughout this SSO side configuration, you will need need to provide your SSO provider a redirect URL: https://app.zepl.com/api/v2/authenticator.identity.zepl/callback/openidprovider . As you do so, keep track of the 3 key fields required in Zepl: OpenID URL , Client ID , and Client Secret . We provide OpenID configuration steps below for several of the top authentication providers, but please note these steps may change and look different in your environment. Okta Contents Supported Features Requirements Notes Configuration Steps Supported Features Zepl OIDC supports: Service Provider (SP)-Initiated Authentication and Identity Provider (IdP)-Initiated Authentication (SSO) flows Requirements Note: The email address for a specific user's Zepl account, MUST match the email value in that user's Okta account. Notes Zepl does not currently support importing groups & roles from identity providers. Known Issues/Troubleshooting N/A In Okta's Admin Portal: Login to the Okta Integration Network: https://www.okta.com/integrations/ Search for 'Zepl OpenID Connect' and then select it. Click 'Add' Select 'Done' Choose 'Sign On' Find the Client ID and Client Secret . Please copy these or keep for reference in the next section. Enter the \u2018Organization ID\u2019 for your Zepl app instance. This 9 character field can be found by looking at the Zepl app URL (https://app.zepl.com/{Organization ID}). Then Save. In Zepl's Authentication Settings: Select Settings > Authentication > OpenID OpenID URL: https://{Your Okta Account Name}.okta.com Note: This should NOT contain '-admin' or a trailing slash character (/) at the end of the url. Be sure to include, https:// . Client ID: Paste from the okta application you created in the previous section. Example: 4o199js5phG2Cv3204p6 Client Secret: Paste from the okta application you created in the previous section. Example: 0S-djdCveAac3aGz0CtNBOOwndoHL3I1MWbVv7el Select Save & Activate Now, logout, log back in, and you should be redirected to Okta Azure AD Azure's Admin Portal : Select Azure Active Directory > App Registrations > New Registration Enter this value for the redirect URL: https://app.zepl.com/api/v2/authenticator.identity.zepl/callback/azureadprovider Register > Select your new app Copy Application (client) ID below Copy Directory (tenant) ID below Creating a Client Secret: Select Azure Active Directory > App Registrations > Select the app you just created Select Certificates & secrets > + New client secret Copy the secret value below Note: Once you navigate away from this page you can no longer copy this value. In Zepl's Authentication Settings: Select Settings > Authentication > OpenID OpenID URL: https://login.microsoftonline.com/{Your Directory (tenant) ID}/v2.0 . Replace \"{Your Directory (tenant) ID}\" with the Directory (tenant) ID from above. Note: This should NOT contain a slash character (/) at the end of the url. Be sure to include, https:// Client ID: Copy the Application (client) ID from the previous section. Example: d610128e-9dda-4575-9d2s-b5e75a1e457w Client Secret: Copy the client secret from the previous section. Example: VUq1jKjSsI9HaI1MtaSD3@oZFYfU/I_ Select Save & Activate Now, logout, log back in, and you should be redirected to the Azure authentication page Auth0 Auth0 Admin Portal : Select Applications > Create Application Name: \"Zepl\" Type: \"Regular Web Application Select Create In the Application screen, navigate to the \"Settings\" tab: Please copy the Domain , Client ID , and Client Secret or keep for reference in the next section Set Application Logo: https://zepl-logo.s3-us-west-1.amazonaws.com/logo_254_256.png Set the Application Login URI: https://app.zepl.com/api/v2/authenticator.identity.zepl/callback/auth0provider Set the Allowed Callback URLs: https://app.zepl.com/api/v2/authenticator.identity.zepl/callback/auth0provider Select \"Save Changes\" at the bottom of the screen In Zepl's Authentication Settings: Select Settings > Authentication > Auth0 Auth0 URL: https://{Auth0 Application Domain}/ . Paste the Domain value from the Application you created in the previous section. Note: This MUST contain a trailing slash character (/) at the end of the url. Be sure to include, https:// . Client ID: Paste from the application you created in the previous section. Example: d6KWU7RMqQi6LfTlnm6ZywIQSCOrFZCQ Client Secret: Paste from the application you created in the previous section. Example: axfS4VpeH3ydkjERD9k6JvZTAZm_Bz23pI58JmRP0UrRdPDr351ESsjkBUIs321sE Select Save & Activate Now, logout, log back in, and you should be redirected to Auth0 SAML Most popular SSO providers support authentication with SAML. Throughout this configuration, you will need need to provide your SSO provider a redirect URL: https://app.zepl.com/api/v2/authenticator.identity.zepl/callback/samlprovider . As you do so, keep track of the 2 key fields required in Zepl: SAML Meta Data URL and Email Attribute Mapping . We provide SAML configuration steps below for several of the top authentication providers, but please note these steps may change and look different in your environment. Okta Note: The email address for a specific user's Zepl account, MUST match the email value in that user's Okta account. In Okta's Admin Portal: You must be an admin on Okta, go to Admin dashboard Select Classic UI : In the top left of the admin console menu bar, make sure that the view is set to Classic UI , NOT <> Developer Console . Create a new SAML application: Applications tab > Add Application > Create New App Platform: Web Sign on Method: SAML 2.0 General Settings Application Name: Zepl Application logo (right click image to save as): Configure SAML 2.0 configurations Single Sign On URL: https://app.zepl.com/api/v2/authenticator.identity.zepl/callback/samlprovider . Be sure to check the \"Use this for Recipient URL and Destination URL\" Audience URI (SP Entity ID): https://app.zepl.com Attribute Statements: Name: urn:oasis:names:tc:SAML:1.1:nameid-format:emailAddress Value: user.email (Select input from Okta's dropdown) Feedback Configurations Are you a customer or partner?: I'm an Okta customer adding an internal app Finish Identity Provider Metadata: Select the Sign On tab > Right click the Identity Provider metadata link > Copy Link (see image below). You will use this as the SAML Meta Data URL in Zepl later. Example: https://{Your Okta Account}/app/{Okta Specific ID}/sso/saml/metadata Note: We are not interested in the content in this link. we only want to to copy the link itself. This will be used in Zepl as the SAML Meta Data URL. Assignments: Make sure that your Okta user is assigned to this new application. All user emails in Okat must match the email of the corresponding Zepl user account. In Zepl's Authentication Settings: Select Settings > Authentication > SAML SAML Meta Data URL: Past the Identity Provider Metadata URL from Okta (step 9 above) Example: https://{Your Okta Account}/app/{Okta Specific ID}/sso/saml/metadata Email Attribute Mapping: Copy this from the Attribute Statement Name in step 7 above, urn:oasis:names:tc:SAML:1.1:nameid-format:emailAddress Note that these are customizable, but should be the same on both sides. Auth0 Auth0 Admin Portal : Select Applications > Create Application > Regular Web Application Set Settings > Allowed Callback URLs to: https://app.zepl.com/api/v2/authenticator.identity.zepl/callback/samlprovider Select Addons > Select SAML2 Web App SAML2 Web App Settings Application Callback URL: https://app.zepl.com/api/v2/authenticator.identity.zepl/callback/samlprovider Select Enable SAML2 Web App Usage Identity Provider Metadata: Right click the Download and Select Copy Link Note: We are not interested in the content in this link. we only want to to copy the link itself. This will be used in Zepl as the SAML Meta Data URL. Navigate to Users & Roles > Users Make sure that your Auth0 user email corresponds to the email used in you Zepl account In Zepl's Authentication Settings: Select Settings > Authentication > SAML SAML Meta Data URL: Past the URL copied from Identity Provider Metadata download link in step 6 above Example: https://{Auth0 Application Domain}.auth0.com/samlp/metadata/9vtrqlAArViP2Us7s7d8W4w0O3ZT2PxH?_ga=2.180000053.1200000568.2984604343-352716394.1592942775 Email Attribute Mapping: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress CAS Note: This documentation is coming soon to a Zepl docs page near you!","title":"Authentication Providers"},{"location":"guide/authentication/authentication_providers/#configure-an-authentication-provider-in-zepl","text":"Zepl understands that your data, and the data science that is used to mine insights from it, is extremely valuable and sensitive. To ensure that only trusted parties can access Zepl resources, you can configure Zepl to use the identity provider platform of your choice. Doing so is a two step configuration: extending access from your identity provider to Zepl, and configuring Zepl Single Sign-on to use that authentication provider. Today, we support the following protocols for logging in to Zepl: Username / Password Google CAS OpenID Connect Okta Azure AD Auth0 SAML Okta Azure AD (coming soon) Auth0 Don't see your provider? Don't worry - we are constantly adding more. Contact us and let us know how we can help you. Please note that today, Zepl does not support importing groups & roles from these identity providers. To use SSO with your Zepl account, your SSO email must match the email of your Zepl account. If you use these identity providers to access your data, you will need to go through an additional step in Zepl to connect to your data with your provider - this documentation section strictly covers configuring Zepl to use your identity provider of choice to log in to the product.","title":"Configure an authentication provider in Zepl"},{"location":"guide/authentication/authentication_providers/#username-password","text":"When you first sign up for Zepl, you created a username and password - this is the default authentication method for Zepl organizations. When you have set Username / Password authentication for your organization, team members can reset their passwords with the \"Forgot your password\". Many users find this to be sufficient for their security needs, especially if they are a small team.","title":"Username / Password "},{"location":"guide/authentication/authentication_providers/#google","text":"Enable your users to authenticate with their existing Google accounts. There are only two fields required for this configuration in Zepl: Client ID and Client Secret .","title":"Google "},{"location":"guide/authentication/authentication_providers/#googles-developer-portal","text":"","title":"Google's Developer Portal"},{"location":"guide/authentication/authentication_providers/#setup-oauth-consent","text":"Select OAuth consent screen Select Application Type Internal: Only users with a Google Account in your organization can grant access to the scopes requested by this app External: Your app will be available to any user with Google Account Click \"Create\" Application Name: Zepl Application logo (right click image to save as): Authorized domains: Your Company Domain Example: zepl.com Click \"Save\"","title":"Setup OAuth Consent:"},{"location":"guide/authentication/authentication_providers/#setup-oauth-20-client-id-and-secret","text":"Select Credentials > Create Project Select \"+ Create Credentials\" > OAuth client ID Application type: Web application Name: Zepl Authorized JavaScript origins: Nothing required Authorized redirect URIs: https://app.zepl.com/api/v2/authenticator.identity.zepl/callback/googleprovider Click \"Create\" Copy the Client ID and Client Secret to use in the next section","title":"Setup OAuth 2.0 Client ID and Secret:"},{"location":"guide/authentication/authentication_providers/#in-zepls-authentication-settings","text":"Select Settings > Authentication > Google Client ID: Paste the value copied from the previous section Example: 1007962201111-p95t6jaaq720idooa8hq6dpm8qqgwe3s.apps.googleusercontent.com Client Secret: Paste the value copied from the previous section Example: GcP61ILeWLk6pveVh8ohQe3t","title":"In Zepl's Authentication Settings:"},{"location":"guide/authentication/authentication_providers/#openid-connect","text":"Most popular SSO providers support authentication with OpenID Connect. Throughout this SSO side configuration, you will need need to provide your SSO provider a redirect URL: https://app.zepl.com/api/v2/authenticator.identity.zepl/callback/openidprovider . As you do so, keep track of the 3 key fields required in Zepl: OpenID URL , Client ID , and Client Secret . We provide OpenID configuration steps below for several of the top authentication providers, but please note these steps may change and look different in your environment.","title":"OpenID Connect "},{"location":"guide/authentication/authentication_providers/#okta","text":"","title":"Okta "},{"location":"guide/authentication/authentication_providers/#contents","text":"Supported Features Requirements Notes Configuration Steps","title":"Contents"},{"location":"guide/authentication/authentication_providers/#supported-features","text":"Zepl OIDC supports: Service Provider (SP)-Initiated Authentication and Identity Provider (IdP)-Initiated Authentication (SSO) flows","title":"Supported Features"},{"location":"guide/authentication/authentication_providers/#requirements","text":"Note: The email address for a specific user's Zepl account, MUST match the email value in that user's Okta account.","title":"Requirements"},{"location":"guide/authentication/authentication_providers/#notes","text":"Zepl does not currently support importing groups & roles from identity providers.","title":"Notes"},{"location":"guide/authentication/authentication_providers/#known-issuestroubleshooting","text":"N/A","title":"Known Issues/Troubleshooting"},{"location":"guide/authentication/authentication_providers/#in-oktas-admin-portal","text":"Login to the Okta Integration Network: https://www.okta.com/integrations/ Search for 'Zepl OpenID Connect' and then select it. Click 'Add' Select 'Done' Choose 'Sign On' Find the Client ID and Client Secret . Please copy these or keep for reference in the next section. Enter the \u2018Organization ID\u2019 for your Zepl app instance. This 9 character field can be found by looking at the Zepl app URL (https://app.zepl.com/{Organization ID}). Then Save.","title":"In Okta's Admin Portal:"},{"location":"guide/authentication/authentication_providers/#in-zepls-authentication-settings_1","text":"Select Settings > Authentication > OpenID OpenID URL: https://{Your Okta Account Name}.okta.com Note: This should NOT contain '-admin' or a trailing slash character (/) at the end of the url. Be sure to include, https:// . Client ID: Paste from the okta application you created in the previous section. Example: 4o199js5phG2Cv3204p6 Client Secret: Paste from the okta application you created in the previous section. Example: 0S-djdCveAac3aGz0CtNBOOwndoHL3I1MWbVv7el Select Save & Activate Now, logout, log back in, and you should be redirected to Okta","title":"In Zepl's Authentication Settings:"},{"location":"guide/authentication/authentication_providers/#azure-ad","text":"","title":"Azure AD "},{"location":"guide/authentication/authentication_providers/#azures-admin-portal","text":"Select Azure Active Directory > App Registrations > New Registration Enter this value for the redirect URL: https://app.zepl.com/api/v2/authenticator.identity.zepl/callback/azureadprovider Register > Select your new app Copy Application (client) ID below Copy Directory (tenant) ID below Creating a Client Secret: Select Azure Active Directory > App Registrations > Select the app you just created Select Certificates & secrets > + New client secret Copy the secret value below Note: Once you navigate away from this page you can no longer copy this value.","title":"Azure's Admin Portal:"},{"location":"guide/authentication/authentication_providers/#in-zepls-authentication-settings_2","text":"Select Settings > Authentication > OpenID OpenID URL: https://login.microsoftonline.com/{Your Directory (tenant) ID}/v2.0 . Replace \"{Your Directory (tenant) ID}\" with the Directory (tenant) ID from above. Note: This should NOT contain a slash character (/) at the end of the url. Be sure to include, https:// Client ID: Copy the Application (client) ID from the previous section. Example: d610128e-9dda-4575-9d2s-b5e75a1e457w Client Secret: Copy the client secret from the previous section. Example: VUq1jKjSsI9HaI1MtaSD3@oZFYfU/I_ Select Save & Activate Now, logout, log back in, and you should be redirected to the Azure authentication page","title":"In Zepl's Authentication Settings:"},{"location":"guide/authentication/authentication_providers/#auth0","text":"","title":"Auth0 "},{"location":"guide/authentication/authentication_providers/#auth0-admin-portal","text":"Select Applications > Create Application Name: \"Zepl\" Type: \"Regular Web Application Select Create In the Application screen, navigate to the \"Settings\" tab: Please copy the Domain , Client ID , and Client Secret or keep for reference in the next section Set Application Logo: https://zepl-logo.s3-us-west-1.amazonaws.com/logo_254_256.png Set the Application Login URI: https://app.zepl.com/api/v2/authenticator.identity.zepl/callback/auth0provider Set the Allowed Callback URLs: https://app.zepl.com/api/v2/authenticator.identity.zepl/callback/auth0provider Select \"Save Changes\" at the bottom of the screen","title":"Auth0 Admin Portal:"},{"location":"guide/authentication/authentication_providers/#in-zepls-authentication-settings_3","text":"Select Settings > Authentication > Auth0 Auth0 URL: https://{Auth0 Application Domain}/ . Paste the Domain value from the Application you created in the previous section. Note: This MUST contain a trailing slash character (/) at the end of the url. Be sure to include, https:// . Client ID: Paste from the application you created in the previous section. Example: d6KWU7RMqQi6LfTlnm6ZywIQSCOrFZCQ Client Secret: Paste from the application you created in the previous section. Example: axfS4VpeH3ydkjERD9k6JvZTAZm_Bz23pI58JmRP0UrRdPDr351ESsjkBUIs321sE Select Save & Activate Now, logout, log back in, and you should be redirected to Auth0","title":"In Zepl's Authentication Settings:"},{"location":"guide/authentication/authentication_providers/#saml","text":"Most popular SSO providers support authentication with SAML. Throughout this configuration, you will need need to provide your SSO provider a redirect URL: https://app.zepl.com/api/v2/authenticator.identity.zepl/callback/samlprovider . As you do so, keep track of the 2 key fields required in Zepl: SAML Meta Data URL and Email Attribute Mapping . We provide SAML configuration steps below for several of the top authentication providers, but please note these steps may change and look different in your environment.","title":"SAML "},{"location":"guide/authentication/authentication_providers/#okta_1","text":"Note: The email address for a specific user's Zepl account, MUST match the email value in that user's Okta account.","title":"Okta "},{"location":"guide/authentication/authentication_providers/#in-oktas-admin-portal_1","text":"You must be an admin on Okta, go to Admin dashboard Select Classic UI : In the top left of the admin console menu bar, make sure that the view is set to Classic UI , NOT <> Developer Console . Create a new SAML application: Applications tab > Add Application > Create New App Platform: Web Sign on Method: SAML 2.0 General Settings Application Name: Zepl Application logo (right click image to save as): Configure SAML 2.0 configurations Single Sign On URL: https://app.zepl.com/api/v2/authenticator.identity.zepl/callback/samlprovider . Be sure to check the \"Use this for Recipient URL and Destination URL\" Audience URI (SP Entity ID): https://app.zepl.com Attribute Statements: Name: urn:oasis:names:tc:SAML:1.1:nameid-format:emailAddress Value: user.email (Select input from Okta's dropdown) Feedback Configurations Are you a customer or partner?: I'm an Okta customer adding an internal app Finish Identity Provider Metadata: Select the Sign On tab > Right click the Identity Provider metadata link > Copy Link (see image below). You will use this as the SAML Meta Data URL in Zepl later. Example: https://{Your Okta Account}/app/{Okta Specific ID}/sso/saml/metadata Note: We are not interested in the content in this link. we only want to to copy the link itself. This will be used in Zepl as the SAML Meta Data URL. Assignments: Make sure that your Okta user is assigned to this new application. All user emails in Okat must match the email of the corresponding Zepl user account.","title":"In Okta's Admin Portal:"},{"location":"guide/authentication/authentication_providers/#in-zepls-authentication-settings_4","text":"Select Settings > Authentication > SAML SAML Meta Data URL: Past the Identity Provider Metadata URL from Okta (step 9 above) Example: https://{Your Okta Account}/app/{Okta Specific ID}/sso/saml/metadata Email Attribute Mapping: Copy this from the Attribute Statement Name in step 7 above, urn:oasis:names:tc:SAML:1.1:nameid-format:emailAddress Note that these are customizable, but should be the same on both sides.","title":"In Zepl's Authentication Settings:"},{"location":"guide/authentication/authentication_providers/#auth0_1","text":"","title":"Auth0 "},{"location":"guide/authentication/authentication_providers/#auth0-admin-portal_1","text":"Select Applications > Create Application > Regular Web Application Set Settings > Allowed Callback URLs to: https://app.zepl.com/api/v2/authenticator.identity.zepl/callback/samlprovider Select Addons > Select SAML2 Web App SAML2 Web App Settings Application Callback URL: https://app.zepl.com/api/v2/authenticator.identity.zepl/callback/samlprovider Select Enable SAML2 Web App Usage Identity Provider Metadata: Right click the Download and Select Copy Link Note: We are not interested in the content in this link. we only want to to copy the link itself. This will be used in Zepl as the SAML Meta Data URL. Navigate to Users & Roles > Users Make sure that your Auth0 user email corresponds to the email used in you Zepl account","title":"Auth0 Admin Portal:"},{"location":"guide/authentication/authentication_providers/#in-zepls-authentication-settings_5","text":"Select Settings > Authentication > SAML SAML Meta Data URL: Past the URL copied from Identity Provider Metadata download link in step 6 above Example: https://{Auth0 Application Domain}.auth0.com/samlp/metadata/9vtrqlAArViP2Us7s7d8W4w0O3ZT2PxH?_ga=2.180000053.1200000568.2984604343-352716394.1592942775 Email Attribute Mapping: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress","title":"In Zepl's Authentication Settings:"},{"location":"guide/authentication/authentication_providers/#cas","text":"Note: This documentation is coming soon to a Zepl docs page near you!","title":"CAS "},{"location":"guide/authentication/zepl_account/","text":"Sign up for Zepl To sign up for Zepl, you must go through our registration process to create an organization for your team and a user account for yourself or be invited to join a teammate\u2019s organization. To start the registration process and create your own organization and user account, visit app.zepl.com/register . To join a colleagues organization, they must send you first an invitation from the members section in settings. You will receive an email inviting you to join their organization - click join and go through the account creation process to do so. You can also sign up for Zepl through Snowflake partner connect. This allows you to connect Snowflake and Zepl with just a few clicks from your existing Snowflake account. See more instructions on how to do that here . Login Page Zepl SaaS has a common login page for you to log int the product . However, you should know that every Zepl organization has a unique login page URL which you can bookmark. Otherwise, Zepl uses browser cookies to keep track of what organizations you have logged into in the past to quickly redirect you to your organization\u2019s login page. There, you simply enter your username and credentials to log in, using the authentication provider determined by your organization administrator. Most users are only part of this one organization and will only see one account when they get to the shared login page. Users that are part of more than one organization will see all the organizations they have logged into with their current browser. Find My Organizations If you arrive on the login page and don\u2019t see any organizations to log in to, you may need to find the login page for your organization. The most likely reason for this is that your cookies have recently been deleted, so Zepl doesn\u2019t know what login page to redirect you to. You can find your organizations by providing your email at on the Find My Organizations page. Reset Password If you have trouble logging in or have forgotten your password, you can reset it on the Forgot Your Password page. If you still have trouble logging in after resetting your password, please contact Zepl Support.","title":"Zepl Account"},{"location":"guide/authentication/zepl_account/#sign-up-for-zepl","text":"To sign up for Zepl, you must go through our registration process to create an organization for your team and a user account for yourself or be invited to join a teammate\u2019s organization. To start the registration process and create your own organization and user account, visit app.zepl.com/register . To join a colleagues organization, they must send you first an invitation from the members section in settings. You will receive an email inviting you to join their organization - click join and go through the account creation process to do so. You can also sign up for Zepl through Snowflake partner connect. This allows you to connect Snowflake and Zepl with just a few clicks from your existing Snowflake account. See more instructions on how to do that here .","title":"Sign up for Zepl"},{"location":"guide/authentication/zepl_account/#login-page","text":"Zepl SaaS has a common login page for you to log int the product . However, you should know that every Zepl organization has a unique login page URL which you can bookmark. Otherwise, Zepl uses browser cookies to keep track of what organizations you have logged into in the past to quickly redirect you to your organization\u2019s login page. There, you simply enter your username and credentials to log in, using the authentication provider determined by your organization administrator. Most users are only part of this one organization and will only see one account when they get to the shared login page. Users that are part of more than one organization will see all the organizations they have logged into with their current browser.","title":"Login Page"},{"location":"guide/authentication/zepl_account/#find-my-organizations","text":"If you arrive on the login page and don\u2019t see any organizations to log in to, you may need to find the login page for your organization. The most likely reason for this is that your cookies have recently been deleted, so Zepl doesn\u2019t know what login page to redirect you to. You can find your organizations by providing your email at on the Find My Organizations page.","title":"Find My Organizations"},{"location":"guide/authentication/zepl_account/#reset-password","text":"If you have trouble logging in or have forgotten your password, you can reset it on the Forgot Your Password page. If you still have trouble logging in after resetting your password, please contact Zepl Support.","title":"Reset Password"},{"location":"guide/datasource/datasource/","text":"Data Sources Zepl Data Sources enable you to securely connect to the database of your choice with your personal credentials. It allows users a secure way to save JDBC connection configurations and to use those configurations in the notebook. Each user is able to add their own credentials to a datasource, making it easy to maintain a library of secure data connections that everyone on the team can use. Establishing a Data Source Connection Click the Data Source icon in the upper right side to open the data source panel and click the Add New button. Select a Data Source type. Zepl currently integrates with Snowflake, S3, MySQL, PostgreSQL, Cassandra, and SAP HANA - we are constantly adding more, so let us know if something you were hoping to use is not yet available! Provide a name and a description for your datasource. The name should be unique within your organization since the name will be used as a data source identifier when you reference it in notebooks. Provide connection string information for your datasource so that Zepl can establish a JDBC connection to your Data Source. This connection string information is shared with everyone in your organization. The fields that are required vary for each Data Source integration. Add your personal user credentials to the Data Source. Every user hoping to run a notebook which uses that Data Source must add their own set of user credentials to the Data Source in order to be able to use it - credentials are private for each user and not shared across the organization. The required user credential fields will vary for each integration but are typically username and password. Click \u201cTest Connection\u201d to validate that you can properly connect to the data source. Then, click \u201cAdd\u201d to save your Data Source. You have successfully created a Data Source! The datasource will attach to the notebook it was created in. To use the Data Source in any other notebook, simply add the Data Source to the notebook by clicking the \u201c+\u201d arrow next to its name. Adding Credentials to An Existing Data Source Data Sources connections are shared with all members of an organization. However, each user must go through a one-step configuration to securely add their own credentials (or a shared set of credentials) to the data source. After adding a Data Source to the Notebook, click Test all connections to see if your connection is fully configured. Any data source that has not been properly configured will cause an error and you can add your credentials to it by clicking edit. Managing Data Sources You can manage an Organization's data sources on the Data Sources page in the resources section of the website. Everyone in the organization has access to this page and can add, update or delete data sources. To use data sources created by other members simply enter your own data source account credentials. The My credentials column in the list indicates whether you have set your credentials or not and neighboring columns indicate how many members have set their own credentials and how many notebooks the data source is attached to. Exploring Data from a Data Source Once you create the Data Source you can explore and interact with it in the notebook via Python, Spark, SQL, or R. The first step is adding the Data Source to a notebook. The Data Source name is then used in a code snippet to establish the connection. Below we use the Zepl_DS Data Source as an example. Python Example %python import pandas as pd # establish connection to Snowflake conn = z.getDatasource('Zepl_DS') # execute query res = conn.execute('SELECT * FROM ITEM LIMIT 1000') # convert datasource into pandas dataframe df = pd.DataFrame(res) df.columns = [col[0] for col in conn.description] # print dataframe as table z.show(df) Spark Example %spark // establish connection to Snowflake and read query result as spark dataframe val df = z.getDatasource(\"Zepl_DS\") .asInstanceOf[org.apache.spark.sql.DataFrameReader] .option(\"query\", \"SELECT * FROM ITEM LIMIT 100\") .load() // print dataframe as table z.show(df) R example %r # conn is an DBIConnection object returned by dbConnect() from DPLYR library conn <- z.getDatasource(\"Zepl_DS\") # use dbGetQuery() from DPLYR library to run query apps <- dbGetQuery(conn$con, \"select * from trip_master limit 200\") # z.show() works for the results returned from dbGetQuery() z.show(apps) SQL Example %datasource.Zepl_DS SELECT * FROM ITEM LIMIT 1000","title":"Data Sources"},{"location":"guide/datasource/datasource/#data-sources","text":"Zepl Data Sources enable you to securely connect to the database of your choice with your personal credentials. It allows users a secure way to save JDBC connection configurations and to use those configurations in the notebook. Each user is able to add their own credentials to a datasource, making it easy to maintain a library of secure data connections that everyone on the team can use.","title":"Data Sources"},{"location":"guide/datasource/datasource/#establishing-a-data-source-connection","text":"Click the Data Source icon in the upper right side to open the data source panel and click the Add New button. Select a Data Source type. Zepl currently integrates with Snowflake, S3, MySQL, PostgreSQL, Cassandra, and SAP HANA - we are constantly adding more, so let us know if something you were hoping to use is not yet available! Provide a name and a description for your datasource. The name should be unique within your organization since the name will be used as a data source identifier when you reference it in notebooks. Provide connection string information for your datasource so that Zepl can establish a JDBC connection to your Data Source. This connection string information is shared with everyone in your organization. The fields that are required vary for each Data Source integration. Add your personal user credentials to the Data Source. Every user hoping to run a notebook which uses that Data Source must add their own set of user credentials to the Data Source in order to be able to use it - credentials are private for each user and not shared across the organization. The required user credential fields will vary for each integration but are typically username and password. Click \u201cTest Connection\u201d to validate that you can properly connect to the data source. Then, click \u201cAdd\u201d to save your Data Source. You have successfully created a Data Source! The datasource will attach to the notebook it was created in. To use the Data Source in any other notebook, simply add the Data Source to the notebook by clicking the \u201c+\u201d arrow next to its name.","title":"Establishing a Data Source Connection"},{"location":"guide/datasource/datasource/#adding-credentials-to-an-existing-data-source","text":"Data Sources connections are shared with all members of an organization. However, each user must go through a one-step configuration to securely add their own credentials (or a shared set of credentials) to the data source. After adding a Data Source to the Notebook, click Test all connections to see if your connection is fully configured. Any data source that has not been properly configured will cause an error and you can add your credentials to it by clicking edit.","title":"Adding Credentials to An Existing Data Source"},{"location":"guide/datasource/datasource/#managing-data-sources","text":"You can manage an Organization's data sources on the Data Sources page in the resources section of the website. Everyone in the organization has access to this page and can add, update or delete data sources. To use data sources created by other members simply enter your own data source account credentials. The My credentials column in the list indicates whether you have set your credentials or not and neighboring columns indicate how many members have set their own credentials and how many notebooks the data source is attached to.","title":"Managing Data Sources"},{"location":"guide/datasource/datasource/#exploring-data-from-a-data-source","text":"Once you create the Data Source you can explore and interact with it in the notebook via Python, Spark, SQL, or R. The first step is adding the Data Source to a notebook. The Data Source name is then used in a code snippet to establish the connection. Below we use the Zepl_DS Data Source as an example.","title":"Exploring Data from a Data Source"},{"location":"guide/datasource/datasource/#python-example","text":"%python import pandas as pd # establish connection to Snowflake conn = z.getDatasource('Zepl_DS') # execute query res = conn.execute('SELECT * FROM ITEM LIMIT 1000') # convert datasource into pandas dataframe df = pd.DataFrame(res) df.columns = [col[0] for col in conn.description] # print dataframe as table z.show(df)","title":"Python Example"},{"location":"guide/datasource/datasource/#spark-example","text":"%spark // establish connection to Snowflake and read query result as spark dataframe val df = z.getDatasource(\"Zepl_DS\") .asInstanceOf[org.apache.spark.sql.DataFrameReader] .option(\"query\", \"SELECT * FROM ITEM LIMIT 100\") .load() // print dataframe as table z.show(df)","title":"Spark Example"},{"location":"guide/datasource/datasource/#r-example","text":"%r # conn is an DBIConnection object returned by dbConnect() from DPLYR library conn <- z.getDatasource(\"Zepl_DS\") # use dbGetQuery() from DPLYR library to run query apps <- dbGetQuery(conn$con, \"select * from trip_master limit 200\") # z.show() works for the results returned from dbGetQuery() z.show(apps)","title":"R example"},{"location":"guide/datasource/datasource/#sql-example","text":"%datasource.Zepl_DS SELECT * FROM ITEM LIMIT 1000","title":"SQL Example"},{"location":"guide/datasource/datasource_integration/","text":"Data Source Integrations Zepl currently integrates with Snowflake, S3, MySQL, PostgreSQL, Cassandra, and SAP HANA - we are constantly adding more, so let us know if something you were hoping to use is not yet available! All Data Source integrations follow the same flow for configuring - you name your Data Source and add a helpful description, you add JDBC connection string information, and you add a set of user-specific personal credentials to allow you to securely establish a connection. Mandatory fields vary slightly for each Data Source integration. Snowflake Username & Password If your Snowflake instance is configured to log in with a Snowflake username and password, use this Data Source configuration option. Warehouse, Database, and Schema are an optional field to allow a Data Source to be defined a single time for many use cases, but if they are not provided, they must be declared in the SQL statements you send to snowflake. Role is also an optional field, and can be provided by users to refine their access to objects in Snowflake as defined by the Snowflake account permissions set by your administrator. SSO Authentication If your Snowflake instance is configured to log in with an external authentication provider, use this Data Source configuration option. Your Snowflake administrator will need to go through a few steps to extend authentication from Snowflake to Zepl before you will be able to establish a connection. You can follow the steps here. Warehouse, Database, and Schema are an optional field to allow a Data Source to be defined a single time for many use cases, but if they are not provided, they must be declared in the SQL statements you send to snowflake. Once the Snowflake account admin has properly integrated your snowflake instance with your snowflake security, you should be able to use whatever SSO provider your organization has elected to use to connect to your Snowflake instance. Role is also an optional credential field, and can be provided by users to refine their access to objects in Snowflake as defined by the Snowflake account permissions set by your administrator. Schema is an optional field but is required for R. Configure SSO Authentication Step 1. Create an OAuth integration To create an OAuth integration, execute the following CREATE SECURITY INTEGRATION command in Snowflake with an account that has sufficient privilege. Only account administrators (users with the ACCOUNTADMIN role) or a role with the global CREATE INTEGRATION privilege can execute this SQL command CREATE SECURITY INTEGRATION ZEPL_SSO_INTEGRATION TYPE = OAUTH ENABLED = TRUE OAUTH_CLIENT = CUSTOM OAUTH_CLIENT_TYPE = 'CONFIDENTIAL' OAUTH_REDIRECT_URI = 'https://app.zepl.com/callbacks/snowflake-datasource-sso' OAUTH_ISSUE_REFRESH_TOKENS = TRUE OAUTH_REFRESH_TOKEN_VALIDITY = 7776000 For more information on customizing the configurations for an OAuth integration, please see OAuth Custom Client Parameters\u200b . Please note that OAuth integration should be specified as following: OAUTH_CLIENT_TYPE must be set to CONFIDENTIAL . It allows the client (Zepl) to store a secret. OAUTH_REDIRECT_URI must be set to https://app.zepl.com/callbacks/snowflake-datasource-sso . After a user is authenticated, the web browser is redirected to this URI. OAUTH_ISSUE_REFRESH_TOKENS must be set to TRUE . It allows the client (Zepl) to exchange a refresh token for an access token when the current access token has expired. OAUTH_REFRESH_TOKEN_VALIDITY specifies how long an authentication should be valid (in seconds). The value is default to 7776000 (90 days). After the specified period passed after the authentication, you should re-authenticate in the detail page of the data source to continue the use of the data source. Step 2. Create a Snowflake data source After integrating OAuth, you can create a Snowflake data source without sharing or storing your login credentials on Zepl. You can also authenticate through an external, SAML 2.0-compliant identity provider (IdP) by enabling federated authentication. To configure your Snowflake account to use federated authentication, see Configuring Snowflake to Use Federated Authentication . Execute the Snowflake function SELECT SYSTEM$SHOW_OAUTH_CLIENT_SECRETS('ZEPL_SSO_INTEGRATION') in Snowflake to retrieve the client ID and secret for the integration you\u2019ve created in the previous step and note the client ID and secret for the later step. Note that the integration name is case-sensitive and must be uppercase and enclosed in single quotes. In the Zepl application, go to the Data Sources page and choose Snowflake in the Add Data Sources section. Enter the unique name for the data source in Name field and account name provided by Snowflake in Account field. In My Credentials section, choose SSO. Click the Connect to Snowflake button. It will open the pop-up with the following title: Log in to Snowflake to continue to ZEPL_SSO_INTEGRATION. If it does not open the pop-up, please make sure that your browser is not blocking the pop-up. It will show the error message if the provided account name is invalid or does not match with the client ID. In the pop-up, enter your Snowflake credentials and choose the Log In button. You can also sign in via the IdP if federated authentication is enabled in the provided Snowflake account. The pop-up will be closed and you will see the message that you are successfully authenticated. Click the Save button. If the provided client secret is invalid or does not match with the client ID, you will see the error message and the data source will not be saved. Limitation Expiration of the authentication: The integration uses the refresh token to authenticate with Snowflake. The refresh tokens generated by Snowflake have a limited lifetime up to 90 days due to security reasons. The expiration of the refresh token can be configured by specifying the OAUTH_REFRESH_TOKEN_VALIDITY parameter. Every (up to) 90 days, the user who created the Snowflake data source with SSO should go to the detail page of the Snowflake data source and re-authenticate with a Snowflake account by clicking the Connect to Snowflake button to continue the use of the data source. The OAuth integration (client ID and secret) does not have an expiration and a user does not have to re-create the OAuth integration. JDBC interpreter is not supported. Test connection is not supported. Amazon S3 MySQL PostgreSQL Cassandra SAP Hana","title":"Data Source Integrations"},{"location":"guide/datasource/datasource_integration/#data-source-integrations","text":"Zepl currently integrates with Snowflake, S3, MySQL, PostgreSQL, Cassandra, and SAP HANA - we are constantly adding more, so let us know if something you were hoping to use is not yet available! All Data Source integrations follow the same flow for configuring - you name your Data Source and add a helpful description, you add JDBC connection string information, and you add a set of user-specific personal credentials to allow you to securely establish a connection. Mandatory fields vary slightly for each Data Source integration.","title":"Data Source Integrations"},{"location":"guide/datasource/datasource_integration/#snowflake","text":"","title":"Snowflake"},{"location":"guide/datasource/datasource_integration/#username-password","text":"If your Snowflake instance is configured to log in with a Snowflake username and password, use this Data Source configuration option. Warehouse, Database, and Schema are an optional field to allow a Data Source to be defined a single time for many use cases, but if they are not provided, they must be declared in the SQL statements you send to snowflake. Role is also an optional field, and can be provided by users to refine their access to objects in Snowflake as defined by the Snowflake account permissions set by your administrator.","title":"Username &amp; Password"},{"location":"guide/datasource/datasource_integration/#sso-authentication","text":"If your Snowflake instance is configured to log in with an external authentication provider, use this Data Source configuration option. Your Snowflake administrator will need to go through a few steps to extend authentication from Snowflake to Zepl before you will be able to establish a connection. You can follow the steps here. Warehouse, Database, and Schema are an optional field to allow a Data Source to be defined a single time for many use cases, but if they are not provided, they must be declared in the SQL statements you send to snowflake. Once the Snowflake account admin has properly integrated your snowflake instance with your snowflake security, you should be able to use whatever SSO provider your organization has elected to use to connect to your Snowflake instance. Role is also an optional credential field, and can be provided by users to refine their access to objects in Snowflake as defined by the Snowflake account permissions set by your administrator. Schema is an optional field but is required for R.","title":"SSO Authentication"},{"location":"guide/datasource/datasource_integration/#configure-sso-authentication","text":"Step 1. Create an OAuth integration To create an OAuth integration, execute the following CREATE SECURITY INTEGRATION command in Snowflake with an account that has sufficient privilege. Only account administrators (users with the ACCOUNTADMIN role) or a role with the global CREATE INTEGRATION privilege can execute this SQL command CREATE SECURITY INTEGRATION ZEPL_SSO_INTEGRATION TYPE = OAUTH ENABLED = TRUE OAUTH_CLIENT = CUSTOM OAUTH_CLIENT_TYPE = 'CONFIDENTIAL' OAUTH_REDIRECT_URI = 'https://app.zepl.com/callbacks/snowflake-datasource-sso' OAUTH_ISSUE_REFRESH_TOKENS = TRUE OAUTH_REFRESH_TOKEN_VALIDITY = 7776000 For more information on customizing the configurations for an OAuth integration, please see OAuth Custom Client Parameters\u200b . Please note that OAuth integration should be specified as following: OAUTH_CLIENT_TYPE must be set to CONFIDENTIAL . It allows the client (Zepl) to store a secret. OAUTH_REDIRECT_URI must be set to https://app.zepl.com/callbacks/snowflake-datasource-sso . After a user is authenticated, the web browser is redirected to this URI. OAUTH_ISSUE_REFRESH_TOKENS must be set to TRUE . It allows the client (Zepl) to exchange a refresh token for an access token when the current access token has expired. OAUTH_REFRESH_TOKEN_VALIDITY specifies how long an authentication should be valid (in seconds). The value is default to 7776000 (90 days). After the specified period passed after the authentication, you should re-authenticate in the detail page of the data source to continue the use of the data source. Step 2. Create a Snowflake data source After integrating OAuth, you can create a Snowflake data source without sharing or storing your login credentials on Zepl. You can also authenticate through an external, SAML 2.0-compliant identity provider (IdP) by enabling federated authentication. To configure your Snowflake account to use federated authentication, see Configuring Snowflake to Use Federated Authentication . Execute the Snowflake function SELECT SYSTEM$SHOW_OAUTH_CLIENT_SECRETS('ZEPL_SSO_INTEGRATION') in Snowflake to retrieve the client ID and secret for the integration you\u2019ve created in the previous step and note the client ID and secret for the later step. Note that the integration name is case-sensitive and must be uppercase and enclosed in single quotes. In the Zepl application, go to the Data Sources page and choose Snowflake in the Add Data Sources section. Enter the unique name for the data source in Name field and account name provided by Snowflake in Account field. In My Credentials section, choose SSO. Click the Connect to Snowflake button. It will open the pop-up with the following title: Log in to Snowflake to continue to ZEPL_SSO_INTEGRATION. If it does not open the pop-up, please make sure that your browser is not blocking the pop-up. It will show the error message if the provided account name is invalid or does not match with the client ID. In the pop-up, enter your Snowflake credentials and choose the Log In button. You can also sign in via the IdP if federated authentication is enabled in the provided Snowflake account. The pop-up will be closed and you will see the message that you are successfully authenticated. Click the Save button. If the provided client secret is invalid or does not match with the client ID, you will see the error message and the data source will not be saved. Limitation Expiration of the authentication: The integration uses the refresh token to authenticate with Snowflake. The refresh tokens generated by Snowflake have a limited lifetime up to 90 days due to security reasons. The expiration of the refresh token can be configured by specifying the OAUTH_REFRESH_TOKEN_VALIDITY parameter. Every (up to) 90 days, the user who created the Snowflake data source with SSO should go to the detail page of the Snowflake data source and re-authenticate with a Snowflake account by clicking the Connect to Snowflake button to continue the use of the data source. The OAuth integration (client ID and secret) does not have an expiration and a user does not have to re-create the OAuth integration. JDBC interpreter is not supported. Test connection is not supported.","title":"Configure SSO Authentication"},{"location":"guide/datasource/datasource_integration/#amazon-s3","text":"","title":"Amazon S3"},{"location":"guide/datasource/datasource_integration/#mysql","text":"","title":"MySQL"},{"location":"guide/datasource/datasource_integration/#postgresql","text":"","title":"PostgreSQL"},{"location":"guide/datasource/datasource_integration/#cassandra","text":"","title":"Cassandra"},{"location":"guide/datasource/datasource_integration/#sap-hana","text":"","title":"SAP Hana"},{"location":"guide/datasource/datasource_objects/","text":"Data Source Objects and Libraries This article will cover all of the data types that are returned by Zepl\u2019s secure data source objects. In order to build a standard method for accessing any data source, Zepl has built data source connectors to support one or all of the four programming languages used by data scientists; Python, R, Scala, and SQL. Zepl has created a simple, secure, and standard method for all users to access their data. Zepl\u2019s data sources allow each user to securely store their user credentials through our encrypted keychain. Each programming language supports different access methods to each of these data sources. We will walk through what objects are returned upon connecting to each data source. This will give you the best insight to take advantage of our features and troubleshoot any errors that may arise. Snowflake Amazon S3 Google BigQuery MySQL PostgreSQL Apache Cassandra SAP HANA Alibaba MaxCompute # Snowflake ### Configure * Account (required) * Warehouse (optional - can be specified at the beginning of the session) * Database (optional - can be specified in each query) * Schema (optional - can be specified in each query) >Note: If your Warehouse is not specified, the user will need to run the \"USE WAREHOUSE \" command in the language of their choice. I.e. if you are connecting through python, select warehouse in this way: > >`cur = z.getDatasource(\" \")` >`cur.execute(\"USE WAREHOUSE \")` ### Read from Snowflake [Open Examples in Zepl](https://app.zepl.com/viewer/notebooks/bm90ZTovL3pzaGFpbnNreUB6ZXBsLmNvbS81MDA5Y2QyNTY4NmI0MmE3ODBlMGU5MzhmYjRhOGViYy9ub3RlLmpzb24){:target=\"_blank\"} | Language | Code | Object Returned | Library | |----------|------|-----------------|---------| | Python |`cur = z.getDatasource(\" \")` `conn = z.getDatasource(\" _con\")` | Snowflake Cursor Object Snowflake Connection Object | Snowflake-connector-python - v2.0.3 | | R / SparkR | `conn < - z.getDatasource(\" \")` | [DBIConnection object:](https://www.rdocumentation.org/packages/DBI/versions/0.5-1/topics/DBIConnection-class){:target=\"_blank\"} Zepl function uses the `dbConnect()` function from the DPLYR library | | | Scala | ```val df = z.getDatasource(\" \").asInstanceOf[org.apache.spark.sql.DataFrameReader]``` | Spark SQL Dataframe: [org.apache.spark.sql.DataFrameReader](https://spark.apache.org/docs/2.3.1/api/java/org/apache/spark/sql/DataFrameReader.html){:target=\"_blank\"} | | SQL | `%datasource. ` | JDBC connection to Snowflake | | >Note: Spark (Scala and Pyspark) require 'STAGE' permissions on the Snowflake database or else this exception may be thrown: `net.snowflake.client.jdbc.SnowflakeSQLException: SQL execution error: Creating stage on shared database 'SNOWFLAKE_SAMPLE_DATA' is not allowed` ### Write data to Snowflake [Open Examples in Zepl](https://app.zepl.com/viewer/notebooks/bm90ZTovL3pzaGFpbnNreUB6ZXBsLmNvbS83ZGQzZWYwOTYxMmY0MzQ3YmRmMDA5Nzc5MDk2MzY2OS9ub3RlLmpzb24){:target=\"_blank\"} # Amazon S3 ### Configure * Bucket Name (required) ### Read data from Amazon S3 [Open Examples in Zepl](https://app.zepl.com/viewer/notebooks/bm90ZTovL3pzaGFpbnNreUB6ZXBsLmNvbS8yYWE1M2I1MjAzMzE0NGM4OWQ0NGYwMjNmNTRmOTAwMC9ub3RlLmpzb24){:target=\"_blank\"} | Language | Code | Object Returned | Library | |---------|------|-----------------|---------| |Python | `s3_bucket = z.getDatasource(\" \")`| [Boto3 Bucket Object](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#bucket){:target=\"_blank\"} | Boto3 - v1.10.9 ### Write data to Amazon S3 [Open Examples in Zepl](https://app.zepl.com/OQ4NBK79S/notebooks/9fb05a846a394f8788a22f65796fb413){:target=\"_blank\"} # Google BigQuery ### Configure * Google Cloud Project ID (required) ### Read data from BigQuery [Open Examples in Zepl](https://app.zepl.com/viewer/notebooks/bm90ZTovL3pzaGFpbnNreUB6ZXBsLmNvbS9iNTk4MjBjMjg0YmQ0NDkzYjdkZmI1NjU5ZDY4NTNkYi9ub3RlLmpzb24){:target=\"_blank\"} | Language | Code | Object Returned | Library | |----------|------|-----------------|---------| | Python | `client = z.getDatasource(\" \")` | [BigQuery Client Object](https://googleapis.dev/python/bigquery/latest/reference.html#client){:target=\"_blank\"}| google-cloud-bigquery - v1.21.0 | ### Write data to BigQuery >Example coming soon! # MySQL ### Configure * Host (required) * Port (required) * Database (optional) ### Read data from MySQL [Open Examples in Zepl](https://app.zepl.com/viewer/notebooks/bm90ZTovL3pzaGFpbnNreUB6ZXBsLmNvbS83YzY2MjkwMzIyMjY0OTk4OWM5YTgxNDA4YmQzOWRiYi9ub3RlLmpzb24){:target=\"_blank\"} | Language | Code | Object Returned | Library | |----------|------|-----------------|---------| | Python | `conn = z.getDatasource(\" \")` | [mysql.connector.connect](https://dev.mysql.com/doc/connector-python/en/connector-python-api-mysqlconnection-connect.html){:target=\"_blank\"}|mysql-connector-python - v8.0.18 | | SQL | `%datasource. `| | | ### Write data to MySQL >Example coming soon! # PostgreSQL ### Configure * Host (required) * Port (required) * Database (optional) ### Read data from PostgreSQL [Open Examples in Zepl](https://app.zepl.com/viewer/notebooks/bm90ZTovL3pzaGFpbnNreUB6ZXBsLmNvbS8yMDc4ZWZmNjE0MDU0MzJlOTEyMWYwNjcxNjBlZjg4Ni9ub3RlLmpzb24){:target=\"_blank\"} | Language | Code | Object Returned | Library | |----------|------|-----------------|---------| | Python | `conn = z.getDatasource(\" \")` | [psycopg2.connect](https://www.psycopg.org/docs/connection.html){:target=\"_blank\"} | psycopg2-binary - v2.8.4 | | SQL | `%datasource. `| | | ### Write data to PostgreSQL >Example coming soon! # Apache Cassandra ### Configure * Host (required) * Port (required) * Keyspace (optional) ### Read data from Apache Cassandra [Open Examples in Zepl](https://app.zepl.com/viewer/notebooks/bm90ZTovL3pzaGFpbnNreUB6ZXBsLmNvbS9kMmJjMWY5ZDlhZWI0NzIxYjgwZDEyZDQ2OGFiODVmNC9ub3RlLmpzb24){:target=\"_blank\"} | Language | Code | Object Returned | Library | |----------|------|-----------------|---------| | Python | `session = z.getDatasource(\" \")` | [cluster.connect.session](https://docs.datastax.com/en/developer/python-driver/3.24/api/cassandra/cluster/#cassandra.cluster.Session){:target=\"_blank\"}| cassandra-driver - v3.20.0 | ### Write data to Apache Cassandra >Example coming soon! # SAP HANA ### Configure * Host (required) * Port (required) * Database (optional) ### Read data from SAP HANA >Example coming soon! | Language | Code | Object Returned | Library | |----------|------|-----------------|---------| | Python | `conn = z.getDatasource(\" \")` | [dbapi.connect](https://help.sap.com/viewer/f1b440ded6144a54ada97ff95dac7adf/2.5/en-US/3b5ebe388c1040ec83617c9e511ecda5.html){:target=\"_blank\"} | hdbcli - v2.4.167 | | Scala | ```val df = z.getDatasource(\" \").asInstanceOf[org.apache.spark.sql.DataFrameReader]``` | Spark SQL Dataframe: [org.apache.spark.sql.DataFrameReader](https://spark.apache.org/docs/2.3.1/api/java/org/apache/spark/sql/DataFrameReader.html){:target=\"_blank\"}| ### Write data to SAP HANA >Example coming soon! # Alibaba MaxCompute ### Configure * Endpoint (required) * Region (required) * Project (required) ### Read data from Alibaba MaxCompute >Example coming soon! | Language | Code | Object Returned | Library | |----------|------|-----------------|---------| | Python | `odps = z.getDatasource(\" \")` | [ODPS Object](https://pyodps.readthedocs.io/en/latest/index.html)| pyodps - v0.8.4 | | Scala | ```val df = z.getDatasource(\" \").asInstanceOf[org.apache.spark.sql.DataFrameReader]``` | Spark SQL Dataframe: [org.apache.spark.sql.DataFrameReader](https://spark.apache.org/docs/2.3.1/api/java/org/apache/spark/sql/DataFrameReader.html) | odps-jdbc-3.1.0.jar | ### Read data to Alibaba MaxCompute >Example coming soon!","title":"Using Zepl Data Sources"},{"location":"guide/datasource/datasource_objects/#data-source-objects-and-libraries","text":"This article will cover all of the data types that are returned by Zepl\u2019s secure data source objects. In order to build a standard method for accessing any data source, Zepl has built data source connectors to support one or all of the four programming languages used by data scientists; Python, R, Scala, and SQL. Zepl has created a simple, secure, and standard method for all users to access their data. Zepl\u2019s data sources allow each user to securely store their user credentials through our encrypted keychain. Each programming language supports different access methods to each of these data sources. We will walk through what objects are returned upon connecting to each data source. This will give you the best insight to take advantage of our features and troubleshoot any errors that may arise. Snowflake Amazon S3 Google BigQuery MySQL PostgreSQL Apache Cassandra SAP HANA Alibaba MaxCompute # Snowflake ### Configure * Account (required) * Warehouse (optional - can be specified at the beginning of the session) * Database (optional - can be specified in each query) * Schema (optional - can be specified in each query) >Note: If your Warehouse is not specified, the user will need to run the \"USE WAREHOUSE \" command in the language of their choice. I.e. if you are connecting through python, select warehouse in this way: > >`cur = z.getDatasource(\" \")` >`cur.execute(\"USE WAREHOUSE \")` ### Read from Snowflake [Open Examples in Zepl](https://app.zepl.com/viewer/notebooks/bm90ZTovL3pzaGFpbnNreUB6ZXBsLmNvbS81MDA5Y2QyNTY4NmI0MmE3ODBlMGU5MzhmYjRhOGViYy9ub3RlLmpzb24){:target=\"_blank\"} | Language | Code | Object Returned | Library | |----------|------|-----------------|---------| | Python |`cur = z.getDatasource(\" \")` `conn = z.getDatasource(\" _con\")` | Snowflake Cursor Object Snowflake Connection Object | Snowflake-connector-python - v2.0.3 | | R / SparkR | `conn < - z.getDatasource(\" \")` | [DBIConnection object:](https://www.rdocumentation.org/packages/DBI/versions/0.5-1/topics/DBIConnection-class){:target=\"_blank\"} Zepl function uses the `dbConnect()` function from the DPLYR library | | | Scala | ```val df = z.getDatasource(\" \").asInstanceOf[org.apache.spark.sql.DataFrameReader]``` | Spark SQL Dataframe: [org.apache.spark.sql.DataFrameReader](https://spark.apache.org/docs/2.3.1/api/java/org/apache/spark/sql/DataFrameReader.html){:target=\"_blank\"} | | SQL | `%datasource. ` | JDBC connection to Snowflake | | >Note: Spark (Scala and Pyspark) require 'STAGE' permissions on the Snowflake database or else this exception may be thrown: `net.snowflake.client.jdbc.SnowflakeSQLException: SQL execution error: Creating stage on shared database 'SNOWFLAKE_SAMPLE_DATA' is not allowed` ### Write data to Snowflake [Open Examples in Zepl](https://app.zepl.com/viewer/notebooks/bm90ZTovL3pzaGFpbnNreUB6ZXBsLmNvbS83ZGQzZWYwOTYxMmY0MzQ3YmRmMDA5Nzc5MDk2MzY2OS9ub3RlLmpzb24){:target=\"_blank\"} # Amazon S3 ### Configure * Bucket Name (required) ### Read data from Amazon S3 [Open Examples in Zepl](https://app.zepl.com/viewer/notebooks/bm90ZTovL3pzaGFpbnNreUB6ZXBsLmNvbS8yYWE1M2I1MjAzMzE0NGM4OWQ0NGYwMjNmNTRmOTAwMC9ub3RlLmpzb24){:target=\"_blank\"} | Language | Code | Object Returned | Library | |---------|------|-----------------|---------| |Python | `s3_bucket = z.getDatasource(\" \")`| [Boto3 Bucket Object](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#bucket){:target=\"_blank\"} | Boto3 - v1.10.9 ### Write data to Amazon S3 [Open Examples in Zepl](https://app.zepl.com/OQ4NBK79S/notebooks/9fb05a846a394f8788a22f65796fb413){:target=\"_blank\"} # Google BigQuery ### Configure * Google Cloud Project ID (required) ### Read data from BigQuery [Open Examples in Zepl](https://app.zepl.com/viewer/notebooks/bm90ZTovL3pzaGFpbnNreUB6ZXBsLmNvbS9iNTk4MjBjMjg0YmQ0NDkzYjdkZmI1NjU5ZDY4NTNkYi9ub3RlLmpzb24){:target=\"_blank\"} | Language | Code | Object Returned | Library | |----------|------|-----------------|---------| | Python | `client = z.getDatasource(\" \")` | [BigQuery Client Object](https://googleapis.dev/python/bigquery/latest/reference.html#client){:target=\"_blank\"}| google-cloud-bigquery - v1.21.0 | ### Write data to BigQuery >Example coming soon! # MySQL ### Configure * Host (required) * Port (required) * Database (optional) ### Read data from MySQL [Open Examples in Zepl](https://app.zepl.com/viewer/notebooks/bm90ZTovL3pzaGFpbnNreUB6ZXBsLmNvbS83YzY2MjkwMzIyMjY0OTk4OWM5YTgxNDA4YmQzOWRiYi9ub3RlLmpzb24){:target=\"_blank\"} | Language | Code | Object Returned | Library | |----------|------|-----------------|---------| | Python | `conn = z.getDatasource(\" \")` | [mysql.connector.connect](https://dev.mysql.com/doc/connector-python/en/connector-python-api-mysqlconnection-connect.html){:target=\"_blank\"}|mysql-connector-python - v8.0.18 | | SQL | `%datasource. `| | | ### Write data to MySQL >Example coming soon! # PostgreSQL ### Configure * Host (required) * Port (required) * Database (optional) ### Read data from PostgreSQL [Open Examples in Zepl](https://app.zepl.com/viewer/notebooks/bm90ZTovL3pzaGFpbnNreUB6ZXBsLmNvbS8yMDc4ZWZmNjE0MDU0MzJlOTEyMWYwNjcxNjBlZjg4Ni9ub3RlLmpzb24){:target=\"_blank\"} | Language | Code | Object Returned | Library | |----------|------|-----------------|---------| | Python | `conn = z.getDatasource(\" \")` | [psycopg2.connect](https://www.psycopg.org/docs/connection.html){:target=\"_blank\"} | psycopg2-binary - v2.8.4 | | SQL | `%datasource. `| | | ### Write data to PostgreSQL >Example coming soon! # Apache Cassandra ### Configure * Host (required) * Port (required) * Keyspace (optional) ### Read data from Apache Cassandra [Open Examples in Zepl](https://app.zepl.com/viewer/notebooks/bm90ZTovL3pzaGFpbnNreUB6ZXBsLmNvbS9kMmJjMWY5ZDlhZWI0NzIxYjgwZDEyZDQ2OGFiODVmNC9ub3RlLmpzb24){:target=\"_blank\"} | Language | Code | Object Returned | Library | |----------|------|-----------------|---------| | Python | `session = z.getDatasource(\" \")` | [cluster.connect.session](https://docs.datastax.com/en/developer/python-driver/3.24/api/cassandra/cluster/#cassandra.cluster.Session){:target=\"_blank\"}| cassandra-driver - v3.20.0 | ### Write data to Apache Cassandra >Example coming soon! # SAP HANA ### Configure * Host (required) * Port (required) * Database (optional) ### Read data from SAP HANA >Example coming soon! | Language | Code | Object Returned | Library | |----------|------|-----------------|---------| | Python | `conn = z.getDatasource(\" \")` | [dbapi.connect](https://help.sap.com/viewer/f1b440ded6144a54ada97ff95dac7adf/2.5/en-US/3b5ebe388c1040ec83617c9e511ecda5.html){:target=\"_blank\"} | hdbcli - v2.4.167 | | Scala | ```val df = z.getDatasource(\" \").asInstanceOf[org.apache.spark.sql.DataFrameReader]``` | Spark SQL Dataframe: [org.apache.spark.sql.DataFrameReader](https://spark.apache.org/docs/2.3.1/api/java/org/apache/spark/sql/DataFrameReader.html){:target=\"_blank\"}| ### Write data to SAP HANA >Example coming soon! # Alibaba MaxCompute ### Configure * Endpoint (required) * Region (required) * Project (required) ### Read data from Alibaba MaxCompute >Example coming soon! | Language | Code | Object Returned | Library | |----------|------|-----------------|---------| | Python | `odps = z.getDatasource(\" \")` | [ODPS Object](https://pyodps.readthedocs.io/en/latest/index.html)| pyodps - v0.8.4 | | Scala | ```val df = z.getDatasource(\" \").asInstanceOf[org.apache.spark.sql.DataFrameReader]``` | Spark SQL Dataframe: [org.apache.spark.sql.DataFrameReader](https://spark.apache.org/docs/2.3.1/api/java/org/apache/spark/sql/DataFrameReader.html) | odps-jdbc-3.1.0.jar | ### Read data to Alibaba MaxCompute >Example coming soon!","title":"Data Source Objects and Libraries"},{"location":"guide/enterprise/custom_image_saas/","text":"Custom Images Creating a reproducible environment for data science is a common challenge when working as a team. It is common practice to put several install commands at the beginning of a notebook or script to help ensure that others will be able to recreate your analysis. Unfortunately, this installation step can take a long time when many different libraries and packages are required. This is where Zepl Custom Images can help you and your team save significant time. Every time you run a notebook, you are spinning up a container which contains an \"image\", which is a prebuilt environment with all the libraries and settings your team needs for reproducible data science. Custom Images lets you create images that contain as many or as few libraries as your team needs. Contact support@zepl.com to get access and learn more! Creating a New Custom Image Custom Images can easily be found under Resources. To start creating a new custom image, simply click \"Create new\". If you have another Custom Image which you\u2019d like to modify as a template for your new image, you can also \"Clone\" that template under the actions for that Custom Image. Each Custom Image should have a unique, descriptive name that makes it easy to find later. As a first step of creating a Custom Image, give your image a name and a description of what you plan to use it for. After naming your image, you are ready to start configuring your Custom Image with interpreters, libraries, packages, and system dependencies. The syntax for doing this is documented below. To build your custom image, click the \"Create\" button at the bottom. Building the image will take a few minutes, and takes longer when more libraries are installed to the image. Sometimes builds can fail if the wrong syntax is used, or there is a library version mismatch issue. If that occurs, you can download a log in the image actions which describes where the build failed. If the image builds successfully, congrats! You can now attach it to notebooks. If you\u2019d like the image you created to be the standard for your entire organization, you can set it as the default image in the image actions. You can also edit or delete an image under these image actions.\u2003 Adding Python Interpreter & Libraries To add a Python interpreter to a Custom Image, simply click \"+ Python\" in the image creation screen. Zepl supports Python 3.8 as the default Python interpreter and can be referenced by the %python alias, To add libraries to your custom image, you can use the following supported to install libraries using pip: Libraryname libraryname==version In addition to any libraries that you include in your custom image list, Zepl installs multiple Python libraries to the image necessary for Zepl to function well. If you try to install a different version of these libraries, it is possible you may run into a compatibility issue. You can always see what versions of libraries you have installed in your image by running a pip list command in the notebook. The following is the syntax for doing so: %python !pip list As of July 2020, the list of libraries we install for Python in a custom image are the following: grpcio==1.24.3 ipykernel==5.1.3 ipython==7.13.0 jupyter-client==5.3.4 protobuf==3.10.0 py4j==0.10.8.1 boto3==1.10.9 cassandra-driver==3.22.0 google-cloud-bigquery==1.21.0 mysql-connector-python==8.0.18 snowflake-connector-python==2.2.7 hdbcli==2.4.167 psycopg2-binary==2.8.4 pyodps==0.8.4 Adding R Interpreter & Libraries To add an R interpreter to a Custom Image, simply click \"+ R\" in the interpreter creation screen. Zepl supports R 3.6 as the default R interpreter for custom image and can be referenced with the %r alias. To add libraries from a CRAN server to your custom image, you simply list out the libraries you hope to install on seperate lines. This installs packages similarly to using install.package(\"LibraryName\") in the notebook Libraryname1 Libraryname2 In addition, you can install libraries using the devtools. The following syntaxes are supported: devtools::install_github() from github devtools::install_bitbucket() from bitbucket devtools::install_url() from an arbitrary url devtools::install_version() installs a specified version from cran In addition to any libraries that you include in your custom image list, R comes with many libraries already installed, and Zepl installs multiple R libraries to the image necessary for Zepl to function well. If you try to install a different version of these libraries, it is possible you may run into a compatibility issue. You can always see what versions of libraries you have installed in your image by running a installed.packages() command. As of July 2020, the list of libraries we install for R in a custom image are the following: devtools DatabaseConnector dplyr knitr rJava RJDBC snowflakedb/dplyr-snowflakedb Adding Spark Interpreter & Libraries To add a Spark interpreter to a Custom Image, simply click \"+ Spark\" in the interpreter creation screen. Zepl supports Spark 2.3.2 as the default Spark interpreter for custom images, which can be refrenced with the %spark alias. To add libraries to Spark, you can do so by adding maven dependencies on separate lines. Please note that we only support compile and exclude_group commands in the syntax today. compile \"commons-io:commons-io:2.6\" compile(\"org.apache.hadoop:hadoop-aws:2.8.3\") { exclude group \u2018javax.servlet\u2019, module: \u2018servlet-api\u2019} In addition to any libraries that you include in your custom image list, Spark comes with many libraries already installed, and Zepl installs multiple Spark libraries to the image necessary for Zepl to function well. If you try to install a different version of these libraries, it is possible you may run into a compatibility issue. You can always see what versions of libraries you have installed in your image by running the following code in the notebook: %pyspark !ls /usr/zepl/interpreter/lib/ As of July 2020, the list of libraries we install for Spark in a custom image are the following: interpreterDatasource \"mysql:mysql-connector-java:8.0.18\" interpreterDatasource \"org.postgresql:postgresql:42.2.8\" interpreterDatasource \"net.snowflake:snowflake-jdbc:3.12.5\" interpreterDatasource \"com.sap.cloud.db.jdbc:ngdbc:2.4.63\" interpreterDatasource \"com.aliyun.odps:odps-jdbc:3.1.0\" interpreterDatasource \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.11:0.15.1-beta\" interpreterDatasource \"com.aliyun.odps:odps-jdbc:3.1.0\" interpreterDatasource(\"com.datastax.spark:spark-cassandra-connector_2.11:2.0.11\") interpreterDatasource(\"net.snowflake:spark-snowflake_2.11:2.7.1-spark_2.2\") interpreterDatasource(\"org.apache.spark:spark-hive_2.11:2.2.1\") Adding System Dependencies You can add environmental variables and packages to your image with this feature. This can be helpful because certain libraries sometimes require underlying infrastructure to be installed to or environment variables set in the image for certain libraries to work properly. You can set an environment variable with very simple syntax: VAR1=123 VAR2=456 You can install packages from github directories with maven syntax. Custom Image Management Custom Images can easily be managed and analyzed from the Custom Images management console. For any existing image that you have created, you can see what it is called, who created the image, if the image successfully created, when it was created, and what notebooks are associated with the image. For any Custom Image that you are curious in learning more about, you can click on the row to learn more about how that custom image was constructed. Under the actions menu for custom images, you can perform the following operations: Clone: This lets you use an existing image as a template to modify while creating a new image Edit: This lets the creator of an image modify the configuration of the image Delete: This lets the creator of an image delete the image Set as Default: This sets an image as the default image for all users in an organization Using a Custom Image In A Notebook When you are creating a new notebook, you must select the Custom Image you want to use to execute code from that notebook. In the notebook, we\u2019ve designed a toolbar on the right hand side which helps you see what interpreters and libraries are available for you to use in your custom image. You can change this selection at any time for a given notebook by editing the notebook settings. To do so, your container must be shut down.","title":"SaaS Image Builder"},{"location":"guide/enterprise/custom_image_saas/#custom-images","text":"Creating a reproducible environment for data science is a common challenge when working as a team. It is common practice to put several install commands at the beginning of a notebook or script to help ensure that others will be able to recreate your analysis. Unfortunately, this installation step can take a long time when many different libraries and packages are required. This is where Zepl Custom Images can help you and your team save significant time. Every time you run a notebook, you are spinning up a container which contains an \"image\", which is a prebuilt environment with all the libraries and settings your team needs for reproducible data science. Custom Images lets you create images that contain as many or as few libraries as your team needs. Contact support@zepl.com to get access and learn more!","title":"Custom Images"},{"location":"guide/enterprise/custom_image_saas/#creating-a-new-custom-image","text":"Custom Images can easily be found under Resources. To start creating a new custom image, simply click \"Create new\". If you have another Custom Image which you\u2019d like to modify as a template for your new image, you can also \"Clone\" that template under the actions for that Custom Image. Each Custom Image should have a unique, descriptive name that makes it easy to find later. As a first step of creating a Custom Image, give your image a name and a description of what you plan to use it for. After naming your image, you are ready to start configuring your Custom Image with interpreters, libraries, packages, and system dependencies. The syntax for doing this is documented below. To build your custom image, click the \"Create\" button at the bottom. Building the image will take a few minutes, and takes longer when more libraries are installed to the image. Sometimes builds can fail if the wrong syntax is used, or there is a library version mismatch issue. If that occurs, you can download a log in the image actions which describes where the build failed. If the image builds successfully, congrats! You can now attach it to notebooks. If you\u2019d like the image you created to be the standard for your entire organization, you can set it as the default image in the image actions. You can also edit or delete an image under these image actions.","title":"Creating a New Custom Image"},{"location":"guide/enterprise/custom_image_saas/#adding-python-interpreter-libraries","text":"To add a Python interpreter to a Custom Image, simply click \"+ Python\" in the image creation screen. Zepl supports Python 3.8 as the default Python interpreter and can be referenced by the %python alias, To add libraries to your custom image, you can use the following supported to install libraries using pip: Libraryname libraryname==version In addition to any libraries that you include in your custom image list, Zepl installs multiple Python libraries to the image necessary for Zepl to function well. If you try to install a different version of these libraries, it is possible you may run into a compatibility issue. You can always see what versions of libraries you have installed in your image by running a pip list command in the notebook. The following is the syntax for doing so: %python !pip list As of July 2020, the list of libraries we install for Python in a custom image are the following: grpcio==1.24.3 ipykernel==5.1.3 ipython==7.13.0 jupyter-client==5.3.4 protobuf==3.10.0 py4j==0.10.8.1 boto3==1.10.9 cassandra-driver==3.22.0 google-cloud-bigquery==1.21.0 mysql-connector-python==8.0.18 snowflake-connector-python==2.2.7 hdbcli==2.4.167 psycopg2-binary==2.8.4 pyodps==0.8.4","title":"Adding Python Interpreter &amp; Libraries"},{"location":"guide/enterprise/custom_image_saas/#adding-r-interpreter-libraries","text":"To add an R interpreter to a Custom Image, simply click \"+ R\" in the interpreter creation screen. Zepl supports R 3.6 as the default R interpreter for custom image and can be referenced with the %r alias. To add libraries from a CRAN server to your custom image, you simply list out the libraries you hope to install on seperate lines. This installs packages similarly to using install.package(\"LibraryName\") in the notebook Libraryname1 Libraryname2 In addition, you can install libraries using the devtools. The following syntaxes are supported: devtools::install_github() from github devtools::install_bitbucket() from bitbucket devtools::install_url() from an arbitrary url devtools::install_version() installs a specified version from cran In addition to any libraries that you include in your custom image list, R comes with many libraries already installed, and Zepl installs multiple R libraries to the image necessary for Zepl to function well. If you try to install a different version of these libraries, it is possible you may run into a compatibility issue. You can always see what versions of libraries you have installed in your image by running a installed.packages() command. As of July 2020, the list of libraries we install for R in a custom image are the following: devtools DatabaseConnector dplyr knitr rJava RJDBC snowflakedb/dplyr-snowflakedb","title":"Adding R Interpreter &amp; Libraries"},{"location":"guide/enterprise/custom_image_saas/#adding-spark-interpreter-libraries","text":"To add a Spark interpreter to a Custom Image, simply click \"+ Spark\" in the interpreter creation screen. Zepl supports Spark 2.3.2 as the default Spark interpreter for custom images, which can be refrenced with the %spark alias. To add libraries to Spark, you can do so by adding maven dependencies on separate lines. Please note that we only support compile and exclude_group commands in the syntax today. compile \"commons-io:commons-io:2.6\" compile(\"org.apache.hadoop:hadoop-aws:2.8.3\") { exclude group \u2018javax.servlet\u2019, module: \u2018servlet-api\u2019} In addition to any libraries that you include in your custom image list, Spark comes with many libraries already installed, and Zepl installs multiple Spark libraries to the image necessary for Zepl to function well. If you try to install a different version of these libraries, it is possible you may run into a compatibility issue. You can always see what versions of libraries you have installed in your image by running the following code in the notebook: %pyspark !ls /usr/zepl/interpreter/lib/ As of July 2020, the list of libraries we install for Spark in a custom image are the following: interpreterDatasource \"mysql:mysql-connector-java:8.0.18\" interpreterDatasource \"org.postgresql:postgresql:42.2.8\" interpreterDatasource \"net.snowflake:snowflake-jdbc:3.12.5\" interpreterDatasource \"com.sap.cloud.db.jdbc:ngdbc:2.4.63\" interpreterDatasource \"com.aliyun.odps:odps-jdbc:3.1.0\" interpreterDatasource \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.11:0.15.1-beta\" interpreterDatasource \"com.aliyun.odps:odps-jdbc:3.1.0\" interpreterDatasource(\"com.datastax.spark:spark-cassandra-connector_2.11:2.0.11\") interpreterDatasource(\"net.snowflake:spark-snowflake_2.11:2.7.1-spark_2.2\") interpreterDatasource(\"org.apache.spark:spark-hive_2.11:2.2.1\")","title":"Adding Spark Interpreter &amp; Libraries"},{"location":"guide/enterprise/custom_image_saas/#adding-system-dependencies","text":"You can add environmental variables and packages to your image with this feature. This can be helpful because certain libraries sometimes require underlying infrastructure to be installed to or environment variables set in the image for certain libraries to work properly. You can set an environment variable with very simple syntax: VAR1=123 VAR2=456 You can install packages from github directories with maven syntax.","title":"Adding System Dependencies"},{"location":"guide/enterprise/custom_image_saas/#custom-image-management","text":"Custom Images can easily be managed and analyzed from the Custom Images management console. For any existing image that you have created, you can see what it is called, who created the image, if the image successfully created, when it was created, and what notebooks are associated with the image. For any Custom Image that you are curious in learning more about, you can click on the row to learn more about how that custom image was constructed. Under the actions menu for custom images, you can perform the following operations: Clone: This lets you use an existing image as a template to modify while creating a new image Edit: This lets the creator of an image modify the configuration of the image Delete: This lets the creator of an image delete the image Set as Default: This sets an image as the default image for all users in an organization","title":"Custom Image Management"},{"location":"guide/enterprise/custom_image_saas/#using-a-custom-image-in-a-notebook","text":"When you are creating a new notebook, you must select the Custom Image you want to use to execute code from that notebook. In the notebook, we\u2019ve designed a toolbar on the right hand side which helps you see what interpreters and libraries are available for you to use in your custom image. You can change this selection at any time for a given notebook by editing the notebook settings. To do so, your container must be shut down.","title":"Using a Custom Image In A Notebook"},{"location":"guide/enterprise/custom_image_support/","text":"Custom Image Support Zepl notebooks run in a container on the cloud. The particular environment that is used to run code on this container is stored as an image file, which ensures any analysis created in Zepl is reproducible on another container. This image file contains preinstalled libraries which are relevant to you and your team. With custom image support, you are able to create an image for you and your organization to use with exactly the toolkit you need. You can pick the versions of programming languages / interpreters that are right for you and your team, define the versions libraries that come preloaded for those interpreters, and set any system dependencies that you hope to refer to while programming in the notebook. SaaS Deployment - Custom Images You don't need to be a DevOps professional to build an image. With our Custom Image editor, simply choose the interpreters that are important to you and your team and list out the libraries you hope to install. Within 30 minutes, you and your team will have a new image to try out when spinning up a container. You can learn how to use custom images here. VPC Deployment - zcr Utility Zepl provides a CLI tool called zcr to build custom interpreter images that you can use with notebooks in your enterprise deployment. You can learn how to use zcr here.","title":"Custom Image Support"},{"location":"guide/enterprise/custom_image_support/#custom-image-support","text":"Zepl notebooks run in a container on the cloud. The particular environment that is used to run code on this container is stored as an image file, which ensures any analysis created in Zepl is reproducible on another container. This image file contains preinstalled libraries which are relevant to you and your team. With custom image support, you are able to create an image for you and your organization to use with exactly the toolkit you need. You can pick the versions of programming languages / interpreters that are right for you and your team, define the versions libraries that come preloaded for those interpreters, and set any system dependencies that you hope to refer to while programming in the notebook.","title":"Custom Image Support"},{"location":"guide/enterprise/custom_image_support/#saas-deployment-custom-images","text":"You don't need to be a DevOps professional to build an image. With our Custom Image editor, simply choose the interpreters that are important to you and your team and list out the libraries you hope to install. Within 30 minutes, you and your team will have a new image to try out when spinning up a container. You can learn how to use custom images here.","title":"SaaS Deployment - Custom Images"},{"location":"guide/enterprise/custom_image_support/#vpc-deployment-zcr-utility","text":"Zepl provides a CLI tool called zcr to build custom interpreter images that you can use with notebooks in your enterprise deployment. You can learn how to use zcr here.","title":"VPC Deployment - zcr Utility"},{"location":"guide/enterprise/custom_image_zcr/","text":"Requirements Currently the zcr supports macOS and 64-bit Linux. Please contact us if you need support for additional operating systems. Name Version Description Docker CE 18.06 + Install Docker Community Edition AWS CLI 1.15.31 + Install AWS CLI You'll also need AWS resources to push the built images to AWS ECR : create the AWS key pair and export it to the terminal bash export IAM=1ambda AWS_ACCESS_KEY_ID={VALUE} AWS_SECRET_ACCESS_KEY={VALUE} the AWS keys should have AWS ECR push permissions create the AWS ECR in the same region as the Zepl deployment # you can create an ECR repository using the AWS cli as well # https://docs.aws.amazon.com/cli/latest/reference/ecr/create-repository.html aws create-repository --repository-name {NAME} --region {REGION} Installation zcr can be installed/updated via the commands below: curl https://s3-us-west-2.amazonaws.com/io.zepl.asset.public/zcr/dist/install.sh | bash - # note that you may need to run the sudo version below, or alternatively chown /usr/local curl https://s3-us-west-2.amazonaws.com/io.zepl.asset.public/zcr/dist/install.sh | sudo bash - zcr will be installed in /usr/local/bin by default and you should now be able to run it in the terminal as follows: $zcr help zcr [command] Available Commands: build Build Zepl interpreter docker image from the files generated by `template` command create Create Zepl interpreter docker image based on the given definition file (.yaml) help Help about any command push Push Zepl interpreter docker image from the files generated by `build` command register Register the built Zepl interpreter docker image into the service database template Template Zepl interpreter Dockerfile based on the given definition file (.yaml) version Print the version information Flags: -d, --dry-run (optional) If specified, will not push and print JSON output. -h, --help help for zcr Use \"zcr [command] --help\" for more information about a command. Registration zcr requires a definition file which defines custom packages for interpreters such as R libraries and Pip packages. With the zcr create command users are able to build/push/register custom interpreter images. For an explanation of the definition file, please refer to the definition section below . # make sure that you exported `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` as env variables # replace `{PATH}` with the file path to your definition file $ export TOKEN=$(aws ecr get-login --no-include-email --region us-west-2 | cut -d\" \" -f6) $ zcr create --definition {PATH} --password ${TOKEN} # or you can pass the ECR auth token directly $ zcr create --definition ./spark2.definition.yaml \\ --password $(aws ecr get-login --no-include-email --region us-west-2 | cut -d\" \" -f6) The command will take a few minutes to run depending on your network speed. If the process is successful you will see the following messages: Successfully built c5c7c63c2bff Successfully tagged 8XXXXXXXX.dkr.ecr.us-west-2.amazonaws.com/test-interpreter-image:v1 2018-09-03T16:29:41.968+0900 INFO Successfully built image 8XXXXXXXX.dkr.ecr.us-west-2.amazonaws.com/test-interpreter-image:v1 2018-09-03T16:33:55.688+0900 INFO Successfully registered test-interpreter-image:v1 into https://XXXX.zepl.com/api/... Please follow the steps below to use the registered interpreter image: Create a new resource image (or select an existing one) from the Image Type area at the bottom of the resource definition page Select the registered image type (the name will be image.name:image.tag ) Create or update the resource Select the resource in your notebook settings Definition File Spec Custom interpreter definition files are written in YAML format. The following fields must be set for each user: meta.service_domain : service domain including protocol (e.g. https://... ) image.repository : docker image repository (e.g. 0000000000.dkr.ecr.us-west-2.amazonaws.com ) image.name : docker image name which should match the ECR repository name (e.g. custom-image ) image.tag : docker image tag (e.g. v1 ) image.description : description for docker image tag which will be displayed in the Zepl UI Registered image names appear as image.name:image.tag in the UI. WARNING: If the image.repository:image.name:image:tag image already exists it will be overwritten when pushing a new version. If you'd like this to appear as a separate image please modify the image.tag component of the concatenated image name. Below is an example definition file with system package and interpreter definition field sections settable by the user. Please refer to the comments for help: meta: service_domain: \"https://{DOMAIN}\" # Service Domain image: type: \"docker\" repository: \"\" # Image Repository e.g `0000000000.dkr.ecr.us-west-2.amazonaws.com` name: \"custom-image\" # Image Name (= ECR repository name) tag: \"v1\" # Image Tag description: \"Test Zepl Interpreter\" # Image Description system: type: \"debian\" packages: - name: \"git\" install: \"apt-get install -y git\" verify: \"git version\" - name: \"unzip\" interpreter: # currently only supports Python versions 2.7.x and 3.x python: version: \"2\" # remove `python.pip_mirror` if you don't need it pip_mirror: index_url: \"http://ftp.daumkakao.com/pypi/simple\" trusted_host: \"ftp.daumkakao.com\" packages: - name: \"matplotlib\" version: \"2.2.2\" - name: \"numpy\" version: \"1.14.3\" - name: \"pandas\" version: \"0.22.0\" # currently only supports spark versions `2.1.2`, `2.2.1` and `2.3.0` spark: version: \"2.3.0\" dependency: packages: # currently only supports hadoop version `2.8.3` - name: \"hadoop\" type: \"hadoop\" version: \"2.8.3\" # currently only supports R verisons `3.3`, `3.4` and `3.5` r: version: \"3.5\" # remove `r.cran_mirror` if you don't need it cran_mirror: \"https://ftp.harukasan.org/CRAN\" packages: - name: \"knitr\" # `knitr` is required for Zepl's %spark.r - name: \"ggplot2\" - name: \"devtools::install_github('rasmusab/bayesian_first_aid')\" type: \"devtools\" jdbc: repository: # define additional/private maven repositories (`mavenLocal` and `mavenCentral` are used by default) maven: - url: \"http://redshift-maven-repository.s3-website-us-east-1.amazonaws.com/release\" packages: # download athena JDBC jar from URL directly - name: \"athena\" version: \"com.amazonaws.athena.jdbc:AthenaJDBC41:2.0.5\" type: \"maven:url\" url: \"https://s3.amazonaws.com/athena-downloads/drivers/JDBC/SimbaAthenaJDBC_2.0.5/AthenaJDBC41_2.0.5.jar\" # download mysql JDBC jar from maven repositories - name: \"mysql\" version: \"mysql:mysql-connector-java:5.1.46\" - name: \"postgresql\" version: \"org.postgresql:postgresql:42.2.2\" - name: \"redshift\" version: \"com.amazon.redshift:redshift-jdbc41:1.2.15.1025\" Trouble Shooting zcr 's create command is a combination of the following commands, each of which can execute separately: template : generates templates based on the given definition file build : builds docker images from the Dockerfile generated by the template command push : pushes the built docker image into the docker repository (ECR) register : registers the pushed docker image into the Zepl database For required parameters please run zcr help (e.g zcr template --help ). Verifying Installed Interpreters To test whether libraries/packages are installed correctly in your custom image, you can use the following commands in notebook paragraphs for the respective interpreters. Note: make sure the custom image is attached to the notebook. Testing Spark and Hadoop Versions %spark println(sc.version) %python !ls -al /usr/ZEPL | grep hadoop Testing R Version and Libraries %python !R --version %spark.r library(ggplot2) Testing Python Version and Packages %python import sys print(sys.version) import pandas Testing Installed JDBC Jars %python !ls -al /usr/ZEPL/interpreter/lib | grep mysql !ls -al /usr/ZEPL/interpreter/lib | grep athena","title":"VPC zcr Utility"},{"location":"guide/enterprise/custom_image_zcr/#requirements","text":"Currently the zcr supports macOS and 64-bit Linux. Please contact us if you need support for additional operating systems. Name Version Description Docker CE 18.06 + Install Docker Community Edition AWS CLI 1.15.31 + Install AWS CLI You'll also need AWS resources to push the built images to AWS ECR : create the AWS key pair and export it to the terminal bash export IAM=1ambda AWS_ACCESS_KEY_ID={VALUE} AWS_SECRET_ACCESS_KEY={VALUE} the AWS keys should have AWS ECR push permissions create the AWS ECR in the same region as the Zepl deployment # you can create an ECR repository using the AWS cli as well # https://docs.aws.amazon.com/cli/latest/reference/ecr/create-repository.html aws create-repository --repository-name {NAME} --region {REGION}","title":"Requirements"},{"location":"guide/enterprise/custom_image_zcr/#installation","text":"zcr can be installed/updated via the commands below: curl https://s3-us-west-2.amazonaws.com/io.zepl.asset.public/zcr/dist/install.sh | bash - # note that you may need to run the sudo version below, or alternatively chown /usr/local curl https://s3-us-west-2.amazonaws.com/io.zepl.asset.public/zcr/dist/install.sh | sudo bash - zcr will be installed in /usr/local/bin by default and you should now be able to run it in the terminal as follows: $zcr help zcr [command] Available Commands: build Build Zepl interpreter docker image from the files generated by `template` command create Create Zepl interpreter docker image based on the given definition file (.yaml) help Help about any command push Push Zepl interpreter docker image from the files generated by `build` command register Register the built Zepl interpreter docker image into the service database template Template Zepl interpreter Dockerfile based on the given definition file (.yaml) version Print the version information Flags: -d, --dry-run (optional) If specified, will not push and print JSON output. -h, --help help for zcr Use \"zcr [command] --help\" for more information about a command.","title":"Installation"},{"location":"guide/enterprise/custom_image_zcr/#registration","text":"zcr requires a definition file which defines custom packages for interpreters such as R libraries and Pip packages. With the zcr create command users are able to build/push/register custom interpreter images. For an explanation of the definition file, please refer to the definition section below . # make sure that you exported `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` as env variables # replace `{PATH}` with the file path to your definition file $ export TOKEN=$(aws ecr get-login --no-include-email --region us-west-2 | cut -d\" \" -f6) $ zcr create --definition {PATH} --password ${TOKEN} # or you can pass the ECR auth token directly $ zcr create --definition ./spark2.definition.yaml \\ --password $(aws ecr get-login --no-include-email --region us-west-2 | cut -d\" \" -f6) The command will take a few minutes to run depending on your network speed. If the process is successful you will see the following messages: Successfully built c5c7c63c2bff Successfully tagged 8XXXXXXXX.dkr.ecr.us-west-2.amazonaws.com/test-interpreter-image:v1 2018-09-03T16:29:41.968+0900 INFO Successfully built image 8XXXXXXXX.dkr.ecr.us-west-2.amazonaws.com/test-interpreter-image:v1 2018-09-03T16:33:55.688+0900 INFO Successfully registered test-interpreter-image:v1 into https://XXXX.zepl.com/api/... Please follow the steps below to use the registered interpreter image: Create a new resource image (or select an existing one) from the Image Type area at the bottom of the resource definition page Select the registered image type (the name will be image.name:image.tag ) Create or update the resource Select the resource in your notebook settings","title":"Registration"},{"location":"guide/enterprise/custom_image_zcr/#definition-file-spec","text":"Custom interpreter definition files are written in YAML format. The following fields must be set for each user: meta.service_domain : service domain including protocol (e.g. https://... ) image.repository : docker image repository (e.g. 0000000000.dkr.ecr.us-west-2.amazonaws.com ) image.name : docker image name which should match the ECR repository name (e.g. custom-image ) image.tag : docker image tag (e.g. v1 ) image.description : description for docker image tag which will be displayed in the Zepl UI Registered image names appear as image.name:image.tag in the UI. WARNING: If the image.repository:image.name:image:tag image already exists it will be overwritten when pushing a new version. If you'd like this to appear as a separate image please modify the image.tag component of the concatenated image name. Below is an example definition file with system package and interpreter definition field sections settable by the user. Please refer to the comments for help: meta: service_domain: \"https://{DOMAIN}\" # Service Domain image: type: \"docker\" repository: \"\" # Image Repository e.g `0000000000.dkr.ecr.us-west-2.amazonaws.com` name: \"custom-image\" # Image Name (= ECR repository name) tag: \"v1\" # Image Tag description: \"Test Zepl Interpreter\" # Image Description system: type: \"debian\" packages: - name: \"git\" install: \"apt-get install -y git\" verify: \"git version\" - name: \"unzip\" interpreter: # currently only supports Python versions 2.7.x and 3.x python: version: \"2\" # remove `python.pip_mirror` if you don't need it pip_mirror: index_url: \"http://ftp.daumkakao.com/pypi/simple\" trusted_host: \"ftp.daumkakao.com\" packages: - name: \"matplotlib\" version: \"2.2.2\" - name: \"numpy\" version: \"1.14.3\" - name: \"pandas\" version: \"0.22.0\" # currently only supports spark versions `2.1.2`, `2.2.1` and `2.3.0` spark: version: \"2.3.0\" dependency: packages: # currently only supports hadoop version `2.8.3` - name: \"hadoop\" type: \"hadoop\" version: \"2.8.3\" # currently only supports R verisons `3.3`, `3.4` and `3.5` r: version: \"3.5\" # remove `r.cran_mirror` if you don't need it cran_mirror: \"https://ftp.harukasan.org/CRAN\" packages: - name: \"knitr\" # `knitr` is required for Zepl's %spark.r - name: \"ggplot2\" - name: \"devtools::install_github('rasmusab/bayesian_first_aid')\" type: \"devtools\" jdbc: repository: # define additional/private maven repositories (`mavenLocal` and `mavenCentral` are used by default) maven: - url: \"http://redshift-maven-repository.s3-website-us-east-1.amazonaws.com/release\" packages: # download athena JDBC jar from URL directly - name: \"athena\" version: \"com.amazonaws.athena.jdbc:AthenaJDBC41:2.0.5\" type: \"maven:url\" url: \"https://s3.amazonaws.com/athena-downloads/drivers/JDBC/SimbaAthenaJDBC_2.0.5/AthenaJDBC41_2.0.5.jar\" # download mysql JDBC jar from maven repositories - name: \"mysql\" version: \"mysql:mysql-connector-java:5.1.46\" - name: \"postgresql\" version: \"org.postgresql:postgresql:42.2.2\" - name: \"redshift\" version: \"com.amazon.redshift:redshift-jdbc41:1.2.15.1025\"","title":"Definition File Spec"},{"location":"guide/enterprise/custom_image_zcr/#trouble-shooting","text":"zcr 's create command is a combination of the following commands, each of which can execute separately: template : generates templates based on the given definition file build : builds docker images from the Dockerfile generated by the template command push : pushes the built docker image into the docker repository (ECR) register : registers the pushed docker image into the Zepl database For required parameters please run zcr help (e.g zcr template --help ).","title":"Trouble Shooting"},{"location":"guide/enterprise/custom_image_zcr/#verifying-installed-interpreters","text":"To test whether libraries/packages are installed correctly in your custom image, you can use the following commands in notebook paragraphs for the respective interpreters. Note: make sure the custom image is attached to the notebook.","title":"Verifying Installed Interpreters"},{"location":"guide/enterprise/custom_image_zcr/#testing-spark-and-hadoop-versions","text":"%spark println(sc.version) %python !ls -al /usr/ZEPL | grep hadoop","title":"Testing Spark and Hadoop Versions"},{"location":"guide/enterprise/custom_image_zcr/#testing-r-version-and-libraries","text":"%python !R --version %spark.r library(ggplot2)","title":"Testing R Version and Libraries"},{"location":"guide/enterprise/custom_image_zcr/#testing-python-version-and-packages","text":"%python import sys print(sys.version) import pandas","title":"Testing Python Version and Packages"},{"location":"guide/enterprise/custom_image_zcr/#testing-installed-jdbc-jars","text":"%python !ls -al /usr/ZEPL/interpreter/lib | grep mysql !ls -al /usr/ZEPL/interpreter/lib | grep athena","title":"Testing Installed JDBC Jars"},{"location":"guide/enterprise/emr_integration/","text":"Integration with Amazon EMR Amazon EMR is a managed cluster platform that simplifies running big data frameworks such as Apache Hadoop and Apache Spark on AWS to process and analyze vast amounts of data. Important: This article applies to the enterprise plan (with AWS Cloud) only. Please contact us for more information. There are two options when connecting Zepl to AWS EMR clusters. Note: In both cases, the EMR cluster will have to reside on the same VPC as Zepl. Option 1: Existing AWS EMR Clusters Zepl can connect to existing EMR clusters that your team has created through the AWS console. There are two requirements: the EMR cluster and the Zepl deployment must be on the same VPC the EMR cluster must have a public, resolvable domain name To connect a Zepl notebook to an existing EMR cluster: Go to the Resources page on Zepl and click on the Clusters item On the Clusters page, click on New Cluster Select the Connect to an externally managed EMR cluster and click Next Give the cluster a name and add the Master public DNS of the EMR cluster in the respective fields Once it's connected you can go to any notebook and select the cluster you just created in the Notebook Settings window Note: Currently Zepl only supports EMR Release v5.14.0 (more will be added in the future) Option 2. Create a new EMR Cluster Zepl also enables you to create a new EMR cluster through the Zepl interface. Note: It is assumed that in the process of the Zepl deployment, the Zepl user IAM role has the credentials to create EMR clusters. As with the above, go to the Resources page and click on the Clusters menu On the Clusters page click on Create new Cluster Select the Launch new Zepl managed EMR cluster and click Next Give the cluster a name, give it an idle terminate value (shuts down the cluster after the time specified), give it any additional configurations, select the Hardware configuration from the dropdown and click Create Again, once created, go to any notebook and select the cluster you just created. Note: The speed at which the new EMR cluster is created is dependent on AWS. This often take about 5 minutes. Managing Clusters All clusters can be managed from the Clusters console. From here you can disconnect, shutdown, clone and control access to these clusters from your organization members.","title":"Amazon EMR Integration"},{"location":"guide/enterprise/emr_integration/#integration-with-amazon-emr","text":"Amazon EMR is a managed cluster platform that simplifies running big data frameworks such as Apache Hadoop and Apache Spark on AWS to process and analyze vast amounts of data. Important: This article applies to the enterprise plan (with AWS Cloud) only. Please contact us for more information. There are two options when connecting Zepl to AWS EMR clusters. Note: In both cases, the EMR cluster will have to reside on the same VPC as Zepl.","title":"Integration with Amazon EMR"},{"location":"guide/enterprise/emr_integration/#option-1-existing-aws-emr-clusters","text":"Zepl can connect to existing EMR clusters that your team has created through the AWS console. There are two requirements: the EMR cluster and the Zepl deployment must be on the same VPC the EMR cluster must have a public, resolvable domain name To connect a Zepl notebook to an existing EMR cluster: Go to the Resources page on Zepl and click on the Clusters item On the Clusters page, click on New Cluster Select the Connect to an externally managed EMR cluster and click Next Give the cluster a name and add the Master public DNS of the EMR cluster in the respective fields Once it's connected you can go to any notebook and select the cluster you just created in the Notebook Settings window Note: Currently Zepl only supports EMR Release v5.14.0 (more will be added in the future)","title":"Option 1: Existing AWS EMR Clusters"},{"location":"guide/enterprise/emr_integration/#option-2-create-a-new-emr-cluster","text":"Zepl also enables you to create a new EMR cluster through the Zepl interface. Note: It is assumed that in the process of the Zepl deployment, the Zepl user IAM role has the credentials to create EMR clusters. As with the above, go to the Resources page and click on the Clusters menu On the Clusters page click on Create new Cluster Select the Launch new Zepl managed EMR cluster and click Next Give the cluster a name, give it an idle terminate value (shuts down the cluster after the time specified), give it any additional configurations, select the Hardware configuration from the dropdown and click Create Again, once created, go to any notebook and select the cluster you just created. Note: The speed at which the new EMR cluster is created is dependent on AWS. This often take about 5 minutes.","title":"Option 2. Create a new EMR Cluster"},{"location":"guide/enterprise/emr_integration/#managing-clusters","text":"All clusters can be managed from the Clusters console. From here you can disconnect, shutdown, clone and control access to these clusters from your organization members.","title":"Managing Clusters"},{"location":"guide/enterprise/logging_console/","text":"The Log Console in Zepl Zepl provides a Log Console for a given notebook that the user can open to view the interpreter logs. The output stream in the Log Console will be specific to the given notebook and its interpreters. How to Use the Log Console Zepl provides the Log Console for Python, Spark and JDBC interpreters. To begin using it simply run the paragraph you're working on. It's located at the bottom of the notebook and can be opened by clicking on the Console tab. If you shutdown the notebook the Log Console will be closed automatically. Note: You will only see log output in the Log Console if the notebook is running. About Log Console Contents The interpreter logs are based on the Apache Zeppelin Interpreter log with the following output: timestamp: ISO8601 log level: INFO , ERROR , WARN thread: the name of the thread that generated the logging event message: the application supplied message associated with the logging event The following is some sample Spark Interpreter log output in the Log Console : ... [2018-09-04 15:46:46,483Z] INFO (pool-2-thread-4) | Job 13121f1eca... finished by scheduler interpreter [2018-09-04 15:46:45,070Z] INFO (event-loop) | Created broadcast 0 [2018-09-04 15:46:45,074Z] INFO (event-loop) | Submitting 4 missing tasks from ShuffleMapStage 0 [2018-09-04 15:46:45,076Z] INFO (event-loop) | Adding task set 0.0 with 4 tasks [2018-09-04 15:46:45,128Z] INFO (event-loop-3) | Starting task 0.0 in stage 0.0 (TID 0, 124008 bytes) [2018-09-04 15:46:45,134Z] INFO (event-loop-3) | Starting task 1.0 in stage 0.0 (TID 1, 123915 bytes) ... [2018-09-04 15:46:45,156Z] INFO (worker for task 0) | Running task 0.0 in stage 0.0 (TID 0) [2018-09-04 15:46:45,156Z] INFO (worker for task 3) | Running task 3.0 in stage 0.0 (TID 3) ... [2018-09-04 15:46:45,164Z] INFO (worker for task 2) | Fetching spark [2018-09-04 15:46:46,131Z] INFO (worker for task 2) | Finished task 2.0 in stage 0.0 (TID 2). 1968 bytes ...","title":"Log Console"},{"location":"guide/enterprise/logging_console/#the-log-console-in-zepl","text":"Zepl provides a Log Console for a given notebook that the user can open to view the interpreter logs. The output stream in the Log Console will be specific to the given notebook and its interpreters.","title":"The Log Console in Zepl"},{"location":"guide/enterprise/logging_console/#how-to-use-the-log-console","text":"Zepl provides the Log Console for Python, Spark and JDBC interpreters. To begin using it simply run the paragraph you're working on. It's located at the bottom of the notebook and can be opened by clicking on the Console tab. If you shutdown the notebook the Log Console will be closed automatically. Note: You will only see log output in the Log Console if the notebook is running.","title":"How to Use the Log Console"},{"location":"guide/enterprise/logging_console/#about-log-console-contents","text":"The interpreter logs are based on the Apache Zeppelin Interpreter log with the following output: timestamp: ISO8601 log level: INFO , ERROR , WARN thread: the name of the thread that generated the logging event message: the application supplied message associated with the logging event The following is some sample Spark Interpreter log output in the Log Console : ... [2018-09-04 15:46:46,483Z] INFO (pool-2-thread-4) | Job 13121f1eca... finished by scheduler interpreter [2018-09-04 15:46:45,070Z] INFO (event-loop) | Created broadcast 0 [2018-09-04 15:46:45,074Z] INFO (event-loop) | Submitting 4 missing tasks from ShuffleMapStage 0 [2018-09-04 15:46:45,076Z] INFO (event-loop) | Adding task set 0.0 with 4 tasks [2018-09-04 15:46:45,128Z] INFO (event-loop-3) | Starting task 0.0 in stage 0.0 (TID 0, 124008 bytes) [2018-09-04 15:46:45,134Z] INFO (event-loop-3) | Starting task 1.0 in stage 0.0 (TID 1, 123915 bytes) ... [2018-09-04 15:46:45,156Z] INFO (worker for task 0) | Running task 0.0 in stage 0.0 (TID 0) [2018-09-04 15:46:45,156Z] INFO (worker for task 3) | Running task 3.0 in stage 0.0 (TID 3) ... [2018-09-04 15:46:45,164Z] INFO (worker for task 2) | Fetching spark [2018-09-04 15:46:46,131Z] INFO (worker for task 2) | Finished task 2.0 in stage 0.0 (TID 2). 1968 bytes ...","title":"About Log Console Contents"},{"location":"guide/enterprise/sagemaker/","text":"Integration with Amazon Sagemaker Amazon Sagemaker is a fully managed service for handling machine learning workflows. It enables users to build, train, and deploy ML models quickly. But before you can deploy your ML model, it must first be built, tuned, and iterated. Notebooks, and more specifically Zepl notebooks, are perfectly suited for these tasks. Now you can use Zepl to connect directly to Sagemaker in your VPC by simply selecting the Sagemaker resource available in Zepl. Simply create a new notebook and select Sagemaker in the resource drop down: You can also switch an existing notebook's resource to Sagemaker by clicking the Settings link in the top right of the notebook. That's it! Nothing else to install. You can then do the following in the notebook: %python from sagemaker.session import Session from sagemaker import KMeans import boto3 import pickle, gzip, numpy, urllib.request import matplotlib.pyplot as plt AWS_ACCESS_KEY_ID=\"[your_AWS_KEY_ID]\" AWS_SECRET_ACCESS_KEY=\"[your_AWS_SECRET_KEY]\" REGION_NAME = \"[region_of_your_VPC]\" def get_boto3_session_with_credentials(): return boto3.Session(aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY,region_name=REGION_NAME) role = \"[your_ARN_ROLE]\" session = Session(get_boto3_session_with_credentials()) bucket = session.default_bucket() # your code goes below As long as you have your AWS credentials setup correctly your model would be deployed to the Sagemaker service in your VPC. In addition, the Zepl Sagemaker resource image is already pre-loaded with the following Python libraries allowing you and your team to leverage Sagemaker for all of your machine learning needs: boto3 matplotlib numpy pandas pandasql Pillow scipy scikit-learn tensorflow bkzep statsmodels seaborn plotly bokeh pydot keras nltk gensim scrapy requests sagemaker You can also create custom images for your organization using our Custom Image Support feature. And as always, if you have any questions please don't hesitate to contact us .","title":"Sagemaker Integration"},{"location":"guide/enterprise/sagemaker/#integration-with-amazon-sagemaker","text":"Amazon Sagemaker is a fully managed service for handling machine learning workflows. It enables users to build, train, and deploy ML models quickly. But before you can deploy your ML model, it must first be built, tuned, and iterated. Notebooks, and more specifically Zepl notebooks, are perfectly suited for these tasks. Now you can use Zepl to connect directly to Sagemaker in your VPC by simply selecting the Sagemaker resource available in Zepl. Simply create a new notebook and select Sagemaker in the resource drop down: You can also switch an existing notebook's resource to Sagemaker by clicking the Settings link in the top right of the notebook. That's it! Nothing else to install. You can then do the following in the notebook: %python from sagemaker.session import Session from sagemaker import KMeans import boto3 import pickle, gzip, numpy, urllib.request import matplotlib.pyplot as plt AWS_ACCESS_KEY_ID=\"[your_AWS_KEY_ID]\" AWS_SECRET_ACCESS_KEY=\"[your_AWS_SECRET_KEY]\" REGION_NAME = \"[region_of_your_VPC]\" def get_boto3_session_with_credentials(): return boto3.Session(aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY,region_name=REGION_NAME) role = \"[your_ARN_ROLE]\" session = Session(get_boto3_session_with_credentials()) bucket = session.default_bucket() # your code goes below As long as you have your AWS credentials setup correctly your model would be deployed to the Sagemaker service in your VPC. In addition, the Zepl Sagemaker resource image is already pre-loaded with the following Python libraries allowing you and your team to leverage Sagemaker for all of your machine learning needs: boto3 matplotlib numpy pandas pandasql Pillow scipy scikit-learn tensorflow bkzep statsmodels seaborn plotly bokeh pydot keras nltk gensim scrapy requests sagemaker You can also create custom images for your organization using our Custom Image Support feature. And as always, if you have any questions please don't hesitate to contact us .","title":"Integration with Amazon Sagemaker"},{"location":"guide/interpreter/angular/","text":"Angular Interpreter Please note that this feature is in beta and is only available for selected customers. The AngularJS interpreter is the port of the Frontend Angular API in Apache Zeppelin . Using the AngularJS interpreter you can render AngularJS 1.7 templates on Zepl paragraphs and leverage the robust two-way binding system. Similar to Apache Zeppelin we expose a simple AngularJS z object on the front-end side to expose the same capabilities. This z object is accessible in the Angular isolated scope for each paragraph. You don't need to create new resources or interpreters to start using the Angular interpreter. All you need to do is add the magic keyword %angular to the first line of your paragraph. API Actions API Initiate binding z.angularBind(var, initialValue, paragraphId) Update value z.angularBind(var, newValue, paragraphId) Destroy binding z.angularUnbind(var, paragraphId) Execute Paragraph z.runParagraph(paragraphId) Basic example In paragraph A (you can find each paragraph's id in its context menu): %angular <form class=\"form-inline\"> <div class=\"form-group\"> <label for=\"superheroId\">Super Hero: </label> <input type=\"text\" class=\"form-control\" id=\"superheroId\" placeholder=\"Superhero name ...\" ng-model=\"superhero\"></input> </div> <p>We will bind {{superhero}} to next paragraph</p> <button type=\"submit\" class=\"btn btn-primary\" ng-click=\"z.angularBind('superhero',superhero,'20181127-043625_2107755481')\"> Bind</button> <button type=\"submit\" class=\"btn btn-primary\" ng-click=\"z.runParagraph('20181127-043625_210775548')\"> Run {{superhero}}</button> <button type=\"submit\" class=\"btn btn-primary\" ng-click=\"z.angularUnbind('superhero','20181127-043625_210775548')\"> UnBind {{superhero}}</button> </form> In paragraph 20181127-043625_210775548 : %angular <div class=\"card\" style=\"width: 18rem;\"> <img class=\"card-img-top\" src=\"//cdn.zepl.com/032d0183959db90c080f31a2dbae0b7e.gif\" alt=\"Card image cap\"> <div class=\"card-body\"> <h5 class=\"card-title\">SuperHero</h5> <p class=\"card-text\">{{superhero}}</p> </div> </div> Angular API We support Angular's ng-directives and ng-filters . You can combine these to build interactive applications on the front-end. Example The following example uses ng-init , ng-model and ng-repeat directives and ng-filter(filter:searchText) to make a filterable table. %angular <div ng-init=\"friends = [{name:'John', phone:'555-1276'}, {name:'Mary', phone:'800-BIG-MARY'}, {name:'Mike', phone:'555-4321'}, {name:'Adam', phone:'555-5678'}, {name:'Julie', phone:'555-8765'}, {name:'Juliette', phone:'555-5678'}]\"></div> <label>Search: <input ng-model=\"searchText\"></label> <table id=\"searchTextResults\" class=\"table\"> <thead> <tr> <th>Name</th> <th>Phone</th> </tr> </thead> <tbody> <tr ng-repeat=\"friend in friends | filter:searchText\"> <td>{{friend.name}}</td> <td>{{friend.phone}}</td> </tr> </tbody> </table>","title":"Angular"},{"location":"guide/interpreter/angular/#angular-interpreter","text":"Please note that this feature is in beta and is only available for selected customers. The AngularJS interpreter is the port of the Frontend Angular API in Apache Zeppelin . Using the AngularJS interpreter you can render AngularJS 1.7 templates on Zepl paragraphs and leverage the robust two-way binding system. Similar to Apache Zeppelin we expose a simple AngularJS z object on the front-end side to expose the same capabilities. This z object is accessible in the Angular isolated scope for each paragraph. You don't need to create new resources or interpreters to start using the Angular interpreter. All you need to do is add the magic keyword %angular to the first line of your paragraph.","title":"Angular Interpreter"},{"location":"guide/interpreter/angular/#api","text":"Actions API Initiate binding z.angularBind(var, initialValue, paragraphId) Update value z.angularBind(var, newValue, paragraphId) Destroy binding z.angularUnbind(var, paragraphId) Execute Paragraph z.runParagraph(paragraphId)","title":"API"},{"location":"guide/interpreter/angular/#basic-example","text":"In paragraph A (you can find each paragraph's id in its context menu): %angular <form class=\"form-inline\"> <div class=\"form-group\"> <label for=\"superheroId\">Super Hero: </label> <input type=\"text\" class=\"form-control\" id=\"superheroId\" placeholder=\"Superhero name ...\" ng-model=\"superhero\"></input> </div> <p>We will bind {{superhero}} to next paragraph</p> <button type=\"submit\" class=\"btn btn-primary\" ng-click=\"z.angularBind('superhero',superhero,'20181127-043625_2107755481')\"> Bind</button> <button type=\"submit\" class=\"btn btn-primary\" ng-click=\"z.runParagraph('20181127-043625_210775548')\"> Run {{superhero}}</button> <button type=\"submit\" class=\"btn btn-primary\" ng-click=\"z.angularUnbind('superhero','20181127-043625_210775548')\"> UnBind {{superhero}}</button> </form> In paragraph 20181127-043625_210775548 : %angular <div class=\"card\" style=\"width: 18rem;\"> <img class=\"card-img-top\" src=\"//cdn.zepl.com/032d0183959db90c080f31a2dbae0b7e.gif\" alt=\"Card image cap\"> <div class=\"card-body\"> <h5 class=\"card-title\">SuperHero</h5> <p class=\"card-text\">{{superhero}}</p> </div> </div>","title":"Basic example"},{"location":"guide/interpreter/angular/#angular-api","text":"We support Angular's ng-directives and ng-filters . You can combine these to build interactive applications on the front-end.","title":"Angular API"},{"location":"guide/interpreter/angular/#example","text":"The following example uses ng-init , ng-model and ng-repeat directives and ng-filter(filter:searchText) to make a filterable table. %angular <div ng-init=\"friends = [{name:'John', phone:'555-1276'}, {name:'Mary', phone:'800-BIG-MARY'}, {name:'Mike', phone:'555-4321'}, {name:'Adam', phone:'555-5678'}, {name:'Julie', phone:'555-8765'}, {name:'Juliette', phone:'555-5678'}]\"></div> <label>Search: <input ng-model=\"searchText\"></label> <table id=\"searchTextResults\" class=\"table\"> <thead> <tr> <th>Name</th> <th>Phone</th> </tr> </thead> <tbody> <tr ng-repeat=\"friend in friends | filter:searchText\"> <td>{{friend.name}}</td> <td>{{friend.phone}}</td> </tr> </tbody> </table>","title":"Example"},{"location":"guide/interpreter/config/","text":"Inline Interpreter Configuration %[Interpreter name].config is a special interpreter that allows for interpreter properties and environment variables to be configured in each notebook. This features brings additional flexibility enabling users to do, among other things: share a notebook with a custom interpreter configuration or specific environment variable that the paragraphs require already set fine-grained control of interpreter configuration at the notebook level Usage %[Interpreter name to configure].config [Key 2] = [Value 1] # comment [Key 2] = [Value 2] ... Note that inline configuration must run before any other paragraph so it will usually be the first paragraph in the notebook. Key Keys can not include empty spaces and all caps ( [A-Z_0-9]+ ) is treated as an environment variable, otherwise it's considered an interpreter property. e.g.: spark.app.name , zeppelin.python - interpreter property SPARK_HOME , CUSTOM_VAR - environment variable Value Values are separated by spaces or tabs from keys. Value can include any character except for a newline. For example, all of the following are valid: 100 , true , /dir/path , spark app name If a value needs to include a new line (multi-line), wrap the value with \"\"\" (triple double quotes). Examples %spark.config spark.app.name = ZEPL # override default spark app name AWS_ACCESS_KEY_ID = .... # AWS access key environment variable AWS_SECRET_ACCESS_KEY = .... # AWS secret key environment variable MY_VAR = \"\"\"multiple line value\"\"\" %spark val data = sc.read.text(\"s3a://...\") Available Configurations 1. %python.config key value description zeppelin.python python or python3 Python command to use. python for 2.x, python3 for 3.x. You can also configure the python version by creating a conda environment . zeppelin.python.maxResult Number (e.g. 10000) Max number of dataframe rows to display. 2. %spark.config key value description zeppelin.spark.useHiveContext true or false Use HiveContext instead of SQLContext if it is true (default). zeppelin.spark.maxResult Number (e.g. 10000) Max number of Spark SQL results to display. zeppelin.dep.additionalRemoteRepository A list of 'id,remote-repository-URL,is-snapshot;'. e.g. spark-packages,http://dl.bintray.com/spark-packages/maven,false; Additional maven repository for spark dependency interpreter %spark.dep . zeppelin.pyspark.python python or python3 Python command to use in pyspark. python for 2.x, python3 for 3.x. 3. %jdbc.config key value description default.url JDBC url. e.g. jdbc:postgresql://my.host.com:5432 JDBC URL to connect default.user String user name default.password String password default.driver Driver class name. e.g. org.postgresql.Driver JDBC driver class name common.max_count Number (e.g. 10000) Maximum number of rows to return","title":"Config"},{"location":"guide/interpreter/config/#inline-interpreter-configuration","text":"%[Interpreter name].config is a special interpreter that allows for interpreter properties and environment variables to be configured in each notebook. This features brings additional flexibility enabling users to do, among other things: share a notebook with a custom interpreter configuration or specific environment variable that the paragraphs require already set fine-grained control of interpreter configuration at the notebook level","title":"Inline Interpreter Configuration"},{"location":"guide/interpreter/config/#usage","text":"%[Interpreter name to configure].config [Key 2] = [Value 1] # comment [Key 2] = [Value 2] ... Note that inline configuration must run before any other paragraph so it will usually be the first paragraph in the notebook.","title":"Usage"},{"location":"guide/interpreter/config/#key","text":"Keys can not include empty spaces and all caps ( [A-Z_0-9]+ ) is treated as an environment variable, otherwise it's considered an interpreter property. e.g.: spark.app.name , zeppelin.python - interpreter property SPARK_HOME , CUSTOM_VAR - environment variable","title":"Key"},{"location":"guide/interpreter/config/#value","text":"Values are separated by spaces or tabs from keys. Value can include any character except for a newline. For example, all of the following are valid: 100 , true , /dir/path , spark app name If a value needs to include a new line (multi-line), wrap the value with \"\"\" (triple double quotes).","title":"Value"},{"location":"guide/interpreter/config/#examples","text":"%spark.config spark.app.name = ZEPL # override default spark app name AWS_ACCESS_KEY_ID = .... # AWS access key environment variable AWS_SECRET_ACCESS_KEY = .... # AWS secret key environment variable MY_VAR = \"\"\"multiple line value\"\"\" %spark val data = sc.read.text(\"s3a://...\")","title":"Examples"},{"location":"guide/interpreter/config/#available-configurations","text":"1. %python.config key value description zeppelin.python python or python3 Python command to use. python for 2.x, python3 for 3.x. You can also configure the python version by creating a conda environment . zeppelin.python.maxResult Number (e.g. 10000) Max number of dataframe rows to display. 2. %spark.config key value description zeppelin.spark.useHiveContext true or false Use HiveContext instead of SQLContext if it is true (default). zeppelin.spark.maxResult Number (e.g. 10000) Max number of Spark SQL results to display. zeppelin.dep.additionalRemoteRepository A list of 'id,remote-repository-URL,is-snapshot;'. e.g. spark-packages,http://dl.bintray.com/spark-packages/maven,false; Additional maven repository for spark dependency interpreter %spark.dep . zeppelin.pyspark.python python or python3 Python command to use in pyspark. python for 2.x, python3 for 3.x. 3. %jdbc.config key value description default.url JDBC url. e.g. jdbc:postgresql://my.host.com:5432 JDBC URL to connect default.user String user name default.password String password default.driver Driver class name. e.g. org.postgresql.Driver JDBC driver class name common.max_count Number (e.g. 10000) Maximum number of rows to return","title":"Available Configurations"},{"location":"guide/interpreter/jdbc/","text":"JDBC Interpreter Zepl supports a JDBC interpreter with drivers for popular databases. Drivers for the following databases are currently supported by Zepl: Mysql Oracle Postgresql Redshift For additional database connections, please contact support@zepl.com . Before connecting to your database, it's important to check the following: the database is currently up and running the database is accessible from the public internet you have the proper credentials to access the database Creating a New JDBC Interpreter First you'll need to create an interpreter to provide the database connection information as follows: Click the Create interpreter button from the Interpreter settings page Select jdbc from the Interpreter type group Provide a name for your interpreter (note that this name will be used in the notebook to call this JDBC interpreter) Provide the JDBC connection URL, username and password Select a JDBC driver from the dropdown menu Use the Test connection button to test the connection. Using Your JDBC Database Interpreter Once you have created the JDBC interpreter, you can use it in your notebook by providing the %[Interpreter name] directive. For example, if you have created your JDBC interpreter with name \"psql\", you can use %psql in the notebook as follows: %psql SELECT * from my_table Secure Database Connections Whitelist IP Addresses Zepl connects to your database using the IP addresses below so you'll need to whitelist them in your firewall: 34.214.146.198 35.164.138.115 35.164.59.56 (NEW) 35.166.141.251 (NEW) 44.236.139.173 (NEW) 52.24.205.101 52.27.170.152 (NEW) SSH Tunneling for Private Networks To connect to a database in a private network, create an SSH tunnel using the following steps: - Create a New JDBC Interpreter Go to the Interpreters page and click Create interpreter Select JDBC from the Interpreter type Enable SSH tunneling in the JDBC driver area - Setup the Public Key Download our public key using the Download button and whitelist the IP addresses listed above through your firewall. Create a user account for Zepl sh ubuntu@user:~$ sudo useradd zepl Since Zepl authenticates via public key, there's no need to set a password. Authorize the key by opening up default.ssh.public_key (the downloaded file) and pasting its contents into a new line in /home/zepl/.ssh/authorized_keys . Make sure the authorized_keys file has 600 permission. In most cases the SSH Port will be 22 by default. Check the Port variable in /etc/ssh/sshd_config to see which port is used for SSH. sh ubuntu@user:~$ cat /etc/ssh/sshd_config | grep Port Port 22 Now you can use your newly created JDBC interpreter with your notebooks to query and process data.","title":"JDBC"},{"location":"guide/interpreter/jdbc/#jdbc-interpreter","text":"Zepl supports a JDBC interpreter with drivers for popular databases. Drivers for the following databases are currently supported by Zepl: Mysql Oracle Postgresql Redshift For additional database connections, please contact support@zepl.com . Before connecting to your database, it's important to check the following: the database is currently up and running the database is accessible from the public internet you have the proper credentials to access the database","title":"JDBC Interpreter"},{"location":"guide/interpreter/jdbc/#creating-a-new-jdbc-interpreter","text":"First you'll need to create an interpreter to provide the database connection information as follows: Click the Create interpreter button from the Interpreter settings page Select jdbc from the Interpreter type group Provide a name for your interpreter (note that this name will be used in the notebook to call this JDBC interpreter) Provide the JDBC connection URL, username and password Select a JDBC driver from the dropdown menu Use the Test connection button to test the connection.","title":"Creating a New JDBC Interpreter"},{"location":"guide/interpreter/jdbc/#using-your-jdbc-database-interpreter","text":"Once you have created the JDBC interpreter, you can use it in your notebook by providing the %[Interpreter name] directive. For example, if you have created your JDBC interpreter with name \"psql\", you can use %psql in the notebook as follows: %psql SELECT * from my_table","title":"Using Your JDBC Database Interpreter"},{"location":"guide/interpreter/jdbc/#secure-database-connections","text":"","title":"Secure Database Connections"},{"location":"guide/interpreter/jdbc/#whitelist-ip-addresses","text":"Zepl connects to your database using the IP addresses below so you'll need to whitelist them in your firewall: 34.214.146.198 35.164.138.115 35.164.59.56 (NEW) 35.166.141.251 (NEW) 44.236.139.173 (NEW) 52.24.205.101 52.27.170.152 (NEW)","title":"Whitelist IP Addresses"},{"location":"guide/interpreter/jdbc/#ssh-tunneling-for-private-networks","text":"To connect to a database in a private network, create an SSH tunnel using the following steps:","title":"SSH Tunneling for Private Networks"},{"location":"guide/interpreter/jdbc/#-create-a-new-jdbc-interpreter","text":"Go to the Interpreters page and click Create interpreter Select JDBC from the Interpreter type Enable SSH tunneling in the JDBC driver area","title":"- Create a New JDBC Interpreter"},{"location":"guide/interpreter/jdbc/#-setup-the-public-key","text":"Download our public key using the Download button and whitelist the IP addresses listed above through your firewall. Create a user account for Zepl sh ubuntu@user:~$ sudo useradd zepl Since Zepl authenticates via public key, there's no need to set a password. Authorize the key by opening up default.ssh.public_key (the downloaded file) and pasting its contents into a new line in /home/zepl/.ssh/authorized_keys . Make sure the authorized_keys file has 600 permission. In most cases the SSH Port will be 22 by default. Check the Port variable in /etc/ssh/sshd_config to see which port is used for SSH. sh ubuntu@user:~$ cat /etc/ssh/sshd_config | grep Port Port 22 Now you can use your newly created JDBC interpreter with your notebooks to query and process data.","title":"- Setup the Public Key"},{"location":"guide/interpreter/python/","text":"Python Interpreter Python is a general purpose programming language which is becoming increasingly popular among data analysts and data scientists due to the strength of its core libraries, simple data structures and growing wealth of packages and modules catering to data science, ML and AI. Zepl provides an online Python interpreter with the following popular libraries pre-installed: numpy : a scientific computing library designed to efficiently manipulate large multi-dimensional arrays scipy : a library that provides a large number of operations for scientific and engineering computing pandas : an easy-to-use data structuring tool for data analysis pandasql : a data manipulation tool using SQL syntax built on top of pandas Dataframes matplotlib : a 2D plotting library Pillow : Python Image Library supporting image manipulation of many different image file formats plotly : interactive, D3 and WebGL charts bokeh : an interactive visualization library that targets modern web browsers for presentation seaborn : a Python data visualization library based on matplotlib, providing a high-level interface for drawing attractive and informative statistical graphics boto3 : AWS SDK for Python scikit-learn : a machine learning toolkit for data mining and analysis tensorflow : a machine intelligent library that helps developer to design, build, and train deep learning models easily keras : a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano theano : a library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. sagemaker : a fully managed machine learning service which allows data scientists and developers to quickly and easily build and train machine learning models, and then directly deploy them into a production-ready hosted environment statsmodels : a Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration nltk : a platform for building Python programs to work with human language data scrapy : a collaborative framework for extracting the data you need from websites Pip Install additional libraries using pip . Usage %python !pip install [packages] Example: %python !pip install -q keras import keras The pip command installs packages in the current environment. If another environment is activated, it will install packages in the particular environment activated. Check the next section to create and switch multiple python environments. Managing Python Environments If you'd like to install additional environments you can use the conda interpreter bundled with the Python interpreter. Conda is open an source package and environment management system which enables easy package installation and environment switching. Usages Get conda information %python.conda info List your environments %python.conda env list Create an environment %python.conda create --name [ENV_NAME] # or if you want to specify python version (e.g 3.6) %python.conda create --name [ENV_NAME] python=3.6 Activate an environment %python.conda activate [ENV_NAME] Deactivate an environment %python.conda deactivate List installed packages %python.conda list Install packages (see previous section to install packages via pip ) %python.conda install [PACKAGE1 PACKAGE2 ...] # or if you want to specify an environment name %python.conda install -n [ENV_NAME] [PACKAGE1 PACKAGE2 ...] Uninstall packages %python.conda uninstall [PACKAGE1 PACKAGE2 ...] Help %python.conda help IPython Support You can use IPython commands with the default Python interpreter in Zepl as shown below: %python # !ls Run a shell command. # Install additional packages !pip install keras # object? Details about the object help? # Time execution of a Python statement or expression %timeit range(1000)","title":"Python"},{"location":"guide/interpreter/python/#python-interpreter","text":"Python is a general purpose programming language which is becoming increasingly popular among data analysts and data scientists due to the strength of its core libraries, simple data structures and growing wealth of packages and modules catering to data science, ML and AI. Zepl provides an online Python interpreter with the following popular libraries pre-installed: numpy : a scientific computing library designed to efficiently manipulate large multi-dimensional arrays scipy : a library that provides a large number of operations for scientific and engineering computing pandas : an easy-to-use data structuring tool for data analysis pandasql : a data manipulation tool using SQL syntax built on top of pandas Dataframes matplotlib : a 2D plotting library Pillow : Python Image Library supporting image manipulation of many different image file formats plotly : interactive, D3 and WebGL charts bokeh : an interactive visualization library that targets modern web browsers for presentation seaborn : a Python data visualization library based on matplotlib, providing a high-level interface for drawing attractive and informative statistical graphics boto3 : AWS SDK for Python scikit-learn : a machine learning toolkit for data mining and analysis tensorflow : a machine intelligent library that helps developer to design, build, and train deep learning models easily keras : a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano theano : a library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. sagemaker : a fully managed machine learning service which allows data scientists and developers to quickly and easily build and train machine learning models, and then directly deploy them into a production-ready hosted environment statsmodels : a Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration nltk : a platform for building Python programs to work with human language data scrapy : a collaborative framework for extracting the data you need from websites","title":"Python Interpreter"},{"location":"guide/interpreter/python/#pip","text":"Install additional libraries using pip . Usage %python !pip install [packages] Example: %python !pip install -q keras import keras The pip command installs packages in the current environment. If another environment is activated, it will install packages in the particular environment activated. Check the next section to create and switch multiple python environments.","title":"Pip"},{"location":"guide/interpreter/python/#managing-python-environments","text":"If you'd like to install additional environments you can use the conda interpreter bundled with the Python interpreter. Conda is open an source package and environment management system which enables easy package installation and environment switching.","title":"Managing Python Environments"},{"location":"guide/interpreter/python/#usages","text":"Get conda information %python.conda info List your environments %python.conda env list Create an environment %python.conda create --name [ENV_NAME] # or if you want to specify python version (e.g 3.6) %python.conda create --name [ENV_NAME] python=3.6 Activate an environment %python.conda activate [ENV_NAME] Deactivate an environment %python.conda deactivate List installed packages %python.conda list Install packages (see previous section to install packages via pip ) %python.conda install [PACKAGE1 PACKAGE2 ...] # or if you want to specify an environment name %python.conda install -n [ENV_NAME] [PACKAGE1 PACKAGE2 ...] Uninstall packages %python.conda uninstall [PACKAGE1 PACKAGE2 ...] Help %python.conda help","title":"Usages"},{"location":"guide/interpreter/python/#ipython-support","text":"You can use IPython commands with the default Python interpreter in Zepl as shown below: %python # !ls Run a shell command. # Install additional packages !pip install keras # object? Details about the object help? # Time execution of a Python statement or expression %timeit range(1000)","title":"IPython Support"},{"location":"guide/interpreter/spark/","text":"Apache Spark Interpreter Apache Spark is an open source processing engine built around speed, ease of use and sophisticated analytics. Zepl provides several interpreters for Apache Spark as follows: %spark - provides a Scala environment %spark.pyspark - provides a Python environment %spark.sql - provides a SparkSQL environment %spark.dep - loads dependency libraries into a Spark environment %spark.r - provides an R environment A single Spark context is shared among %spark , %spark.pyspark , %spark.sql and %spark.r sessions. Zepl currently runs Apache Spark v2.3.0 on a single node (non-distributed) per notebook container. Loading Data from AWS S3 To create a dataset from AWS S3 it is recommended to use the s3a connector. First, you need to configure your access and secret keys as follows: %spark sc.hadoopConfiguration.set(\"fs.s3a.access.key\", \"[YOUR_ACCESS_KEY]\") sc.hadoopConfiguration.set(\"fs.s3a.secret.key\", \"[YOUR_SECRET_KEY]\") Then your Spark context will be able to create a dataset from S3 like so: %spark val data = spark.read.text(\"s3a://apache-zeppelin/tutorial/bank/bank.csv\") Alternatively you can load data using the s3n connector. In this case your access and secret keys can be configured in the following way: %spark sc.hadoopConfiguration.set(\"fs.s3n.awsAccessKeyId\", \"[YOUR_ACCESS_KEY]\") sc.hadoopConfiguration.set(\"fs.s3n.awsSecretAccessKey\", \"[YOUR_SECRET_KEY]\") And your data accessed as follows: %spark val data = sc.textFile(\"s3n://....\") You can reference the data loading example notebook to try it yourself. Loading Dependencies When your code requires external libraries %spark.dep helps load them from a maven repository. The %spark.dep interpreter leverages the Scala environment. You can write scala expressions to call dependency load APIs. Note that %spark.dep should be the first interpreter run in the notebook before %spark , %spark.pyspark or %spark.sql . Otherwise, %spark.dep will print error messages and you'll need to shutdown and restart the container for the notebook again. Check the Spark dependency loading example notebook for details. Usages %spark.dep z.reset() // clean up previously added artifact and repository // add maven repository z.addRepo(\"RepoName\").url(\"RepoURL\") // add maven snapshot repository z.addRepo(\"RepoName\").url(\"RepoURL\").snapshot() // add credentials for private maven repository z.addRepo(\"RepoName\").url(\"RepoURL\").username(\"username\").password(\"password\") // add artifact from filesystem z.load(\"/path/to.jar\") // add artifact from maven repository, with no dependency z.load(\"groupId:artifactId:version\").excludeAll() // add artifact recursively z.load(\"groupId:artifactId:version\") // add artifact recursively except comma separated GroupID:ArtifactId list z.load(\"groupId:artifactId:version\").exclude(\"groupId:artifactId,groupId:artifactId, ...\") // exclude with pattern z.load(\"groupId:artifactId:version\").exclude(*) z.load(\"groupId:artifactId:version\").exclude(\"groupId:artifactId:*\") z.load(\"groupId:artifactId:version\").exclude(\"groupId:*\") // local() skips adding artifact to spark clusters (skipping sc.addJar()) z.load(\"groupId:artifactId:version\").local() SparkR Installed libraries - ggplot2 , googleVis , knitr , data.table , devtools %spark.r library(googleVis) bubble <- gvisBubbleChart(Fruits, idvar=\"Fruit\", xvar=\"Sales\", yvar=\"Expenses\", colorvar=\"Year\", sizevar=\"Profit\", options=list( hAxis='{minValue:75, maxValue:125}')) print(bubble, tag = 'chart') %spark.r plot(iris, col = heat.colors(3)) %spark.r library(ggplot2) pres_rating <- data.frame( rating = as.numeric(presidents), year = as.numeric(floor(time(presidents))), quarter = as.numeric(cycle(presidents)) ) p <- ggplot(pres_rating, aes(x=year, y=quarter, fill=rating))","title":"Apache Spark"},{"location":"guide/interpreter/spark/#apache-spark-interpreter","text":"Apache Spark is an open source processing engine built around speed, ease of use and sophisticated analytics. Zepl provides several interpreters for Apache Spark as follows: %spark - provides a Scala environment %spark.pyspark - provides a Python environment %spark.sql - provides a SparkSQL environment %spark.dep - loads dependency libraries into a Spark environment %spark.r - provides an R environment A single Spark context is shared among %spark , %spark.pyspark , %spark.sql and %spark.r sessions. Zepl currently runs Apache Spark v2.3.0 on a single node (non-distributed) per notebook container.","title":"Apache Spark Interpreter"},{"location":"guide/interpreter/spark/#loading-data-from-aws-s3","text":"To create a dataset from AWS S3 it is recommended to use the s3a connector. First, you need to configure your access and secret keys as follows: %spark sc.hadoopConfiguration.set(\"fs.s3a.access.key\", \"[YOUR_ACCESS_KEY]\") sc.hadoopConfiguration.set(\"fs.s3a.secret.key\", \"[YOUR_SECRET_KEY]\") Then your Spark context will be able to create a dataset from S3 like so: %spark val data = spark.read.text(\"s3a://apache-zeppelin/tutorial/bank/bank.csv\") Alternatively you can load data using the s3n connector. In this case your access and secret keys can be configured in the following way: %spark sc.hadoopConfiguration.set(\"fs.s3n.awsAccessKeyId\", \"[YOUR_ACCESS_KEY]\") sc.hadoopConfiguration.set(\"fs.s3n.awsSecretAccessKey\", \"[YOUR_SECRET_KEY]\") And your data accessed as follows: %spark val data = sc.textFile(\"s3n://....\") You can reference the data loading example notebook to try it yourself.","title":"Loading Data from AWS S3"},{"location":"guide/interpreter/spark/#loading-dependencies","text":"When your code requires external libraries %spark.dep helps load them from a maven repository. The %spark.dep interpreter leverages the Scala environment. You can write scala expressions to call dependency load APIs. Note that %spark.dep should be the first interpreter run in the notebook before %spark , %spark.pyspark or %spark.sql . Otherwise, %spark.dep will print error messages and you'll need to shutdown and restart the container for the notebook again. Check the Spark dependency loading example notebook for details.","title":"Loading Dependencies"},{"location":"guide/interpreter/spark/#usages","text":"%spark.dep z.reset() // clean up previously added artifact and repository // add maven repository z.addRepo(\"RepoName\").url(\"RepoURL\") // add maven snapshot repository z.addRepo(\"RepoName\").url(\"RepoURL\").snapshot() // add credentials for private maven repository z.addRepo(\"RepoName\").url(\"RepoURL\").username(\"username\").password(\"password\") // add artifact from filesystem z.load(\"/path/to.jar\") // add artifact from maven repository, with no dependency z.load(\"groupId:artifactId:version\").excludeAll() // add artifact recursively z.load(\"groupId:artifactId:version\") // add artifact recursively except comma separated GroupID:ArtifactId list z.load(\"groupId:artifactId:version\").exclude(\"groupId:artifactId,groupId:artifactId, ...\") // exclude with pattern z.load(\"groupId:artifactId:version\").exclude(*) z.load(\"groupId:artifactId:version\").exclude(\"groupId:artifactId:*\") z.load(\"groupId:artifactId:version\").exclude(\"groupId:*\") // local() skips adding artifact to spark clusters (skipping sc.addJar()) z.load(\"groupId:artifactId:version\").local()","title":"Usages"},{"location":"guide/interpreter/spark/#sparkr","text":"Installed libraries - ggplot2 , googleVis , knitr , data.table , devtools %spark.r library(googleVis) bubble <- gvisBubbleChart(Fruits, idvar=\"Fruit\", xvar=\"Sales\", yvar=\"Expenses\", colorvar=\"Year\", sizevar=\"Profit\", options=list( hAxis='{minValue:75, maxValue:125}')) print(bubble, tag = 'chart') %spark.r plot(iris, col = heat.colors(3)) %spark.r library(ggplot2) pres_rating <- data.frame( rating = as.numeric(presidents), year = as.numeric(floor(time(presidents))), quarter = as.numeric(cycle(presidents)) ) p <- ggplot(pres_rating, aes(x=year, y=quarter, fill=rating))","title":"SparkR"},{"location":"guide/partnerships/snowflake_partner_connect/","text":"Snowflake Partner Connect Zepl is proud to participate in Snowflake Partner Connect. This program allows Snowflake users to quickly create a Zepl trial & connect Zepl to your Snowflake account. Here\u2019s how you can get started using Zepl with Partner Connect: Step 1 Log in to your Snowflake account as an Account Admin. Step 2 Select the Partner Connect tile from the top right. Step 3 Click the Zepl tile, followed by Connect. Snowflake will auto-create an extra-small data warehouse ( PC_ZEPL_WH ) and a database ( PC_ZEPL_DATABASE ) for you to pipe data into for use in Zepl. Step 4 Once the account has been created, a pop-up will appear confirming that the Snowflake <> Zepl connection is ready to be activated. Click Activate to continue with the process. Step 5 You can now choose between creating a new Zepl account / organization or connecting your Snowflake environment to an existing Zepl organization. To create your account, enter your email and click Next. Step 6 After answering a few questions to create a new account account, you will begin a 30 day free trial with $400 worth of Zepl credit to run analytics. Step 7 Log in to Zepl. Upon landing in the product, you will be shown how to run your first notebook via a guided tour.","title":"Snowflake Partner Connect"},{"location":"guide/partnerships/snowflake_partner_connect/#snowflake-partner-connect","text":"Zepl is proud to participate in Snowflake Partner Connect. This program allows Snowflake users to quickly create a Zepl trial & connect Zepl to your Snowflake account. Here\u2019s how you can get started using Zepl with Partner Connect: Step 1 Log in to your Snowflake account as an Account Admin. Step 2 Select the Partner Connect tile from the top right. Step 3 Click the Zepl tile, followed by Connect. Snowflake will auto-create an extra-small data warehouse ( PC_ZEPL_WH ) and a database ( PC_ZEPL_DATABASE ) for you to pipe data into for use in Zepl. Step 4 Once the account has been created, a pop-up will appear confirming that the Snowflake <> Zepl connection is ready to be activated. Click Activate to continue with the process. Step 5 You can now choose between creating a new Zepl account / organization or connecting your Snowflake environment to an existing Zepl organization. To create your account, enter your email and click Next. Step 6 After answering a few questions to create a new account account, you will begin a 30 day free trial with $400 worth of Zepl credit to run analytics. Step 7 Log in to Zepl. Upon landing in the product, you will be shown how to run your first notebook via a guided tour.","title":"Snowflake Partner Connect"}]}